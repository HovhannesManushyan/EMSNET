{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-27 12:10:05.152675: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-27 12:10:05.424140: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-27 12:10:05.424157: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-27 12:10:06.386120: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-27 12:10:06.386181: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-27 12:10:06.386189: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tenseal as ts\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "data = np.random.rand(10000).reshape((-1, 1))\n",
    "y = data < 0.5\n",
    "y = y.reshape((-1, 1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "data_test = np.random.rand(1000).reshape((-1, 1))\n",
    "y_test = data_test < 0.5\n",
    "y_test = y_test.reshape((-1, 1))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-27 12:10:07.840497: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-01-27 12:10:07.840736: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-01-27 12:10:07.840762: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (elina-G3-3500): /proc/driver/nvidia/version does not exist\n",
      "2023-01-27 12:10:07.841321: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "sample_model = tf.keras.Sequential()\n",
    "sample_model.add(tf.keras.layers.Input((None, 1)))\n",
    "sample_model.add(tf.keras.layers.Dense(16, activation=\"sigmoid\"))\n",
    "sample_model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "sample_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6096 - accuracy: 0.9145 - val_loss: 0.5136 - val_accuracy: 0.9780\n",
      "Epoch 2/4\n",
      "1000/1000 [==============================] - 1s 788us/step - loss: 0.4081 - accuracy: 0.9843 - val_loss: 0.3039 - val_accuracy: 0.9990\n",
      "Epoch 3/4\n",
      "1000/1000 [==============================] - 1s 784us/step - loss: 0.2492 - accuracy: 0.9898 - val_loss: 0.1937 - val_accuracy: 0.9870\n",
      "Epoch 4/4\n",
      "1000/1000 [==============================] - 1s 763us/step - loss: 0.1716 - accuracy: 0.9936 - val_loss: 0.1407 - val_accuracy: 0.9890\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x7f6a547e93c0>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_model.fit(data, y, validation_data=(data_test, y_test), epochs=4, batch_size=10)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 0.6828716993331909 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6895090937614441 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.706517219543457 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.7069393992424011 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.7078315615653992 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.6881425976753235 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6703822016716003 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6885309815406799 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.698070228099823 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6977242827415466 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6975101232528687 Accuracy 0.5\n",
      "Epoch 0 Loss 0.7175335884094238 Accuracy 0.30000001192092896\n",
      "Epoch 0 Loss 0.6970621943473816 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6973863840103149 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6970797777175903 Accuracy 0.5\n",
      "Epoch 0 Loss 0.7155755162239075 Accuracy 0.30000001192092896\n",
      "Epoch 0 Loss 0.6885615587234497 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6786136627197266 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6967819929122925 Accuracy 0.5\n",
      "Epoch 0 Loss 0.7304867506027222 Accuracy 0.10000000149011612\n",
      "Epoch 0 Loss 0.7125793695449829 Accuracy 0.30000001192092896\n",
      "Epoch 0 Loss 0.6805899739265442 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6952229142189026 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6955611109733582 Accuracy 0.5\n",
      "Epoch 0 Loss 0.7019895911216736 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.6820252537727356 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.7073196768760681 Accuracy 0.30000001192092896\n",
      "Epoch 0 Loss 0.7071756720542908 Accuracy 0.30000001192092896\n",
      "Epoch 0 Loss 0.689553439617157 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6947528123855591 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6897217631340027 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6941195130348206 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6943966150283813 Accuracy 0.5\n",
      "Epoch 0 Loss 0.7069022059440613 Accuracy 0.20000000298023224\n",
      "Epoch 0 Loss 0.6900535821914673 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6900213360786438 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6972739696502686 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.7000396251678467 Accuracy 0.30000001192092896\n",
      "Epoch 0 Loss 0.6907628178596497 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6960304379463196 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.6868366003036499 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6911553144454956 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6932329535484314 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6911115050315857 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6929743885993958 Accuracy 0.5\n",
      "Epoch 0 Loss 0.688643753528595 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6885281801223755 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6904106736183167 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6981927156448364 Accuracy 0.30000001192092896\n",
      "Epoch 0 Loss 0.6956855058670044 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.6869995594024658 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.683671772480011 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6858361959457397 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6816326379776001 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.7010703682899475 Accuracy 0.30000001192092896\n",
      "Epoch 0 Loss 0.6881359815597534 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6929076313972473 Accuracy 0.5\n",
      "Epoch 0 Loss 0.682326078414917 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6925309300422668 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6806526780128479 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6924177408218384 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6925663352012634 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6923003792762756 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6998417377471924 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.6776617765426636 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.7077840566635132 Accuracy 0.30000001192092896\n",
      "Epoch 0 Loss 0.6691912412643433 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.7004062533378601 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.6839063167572021 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6919594407081604 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6752455830574036 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.7187614440917969 Accuracy 0.20000000298023224\n",
      "Epoch 0 Loss 0.7180136442184448 Accuracy 0.20000000298023224\n",
      "Epoch 0 Loss 0.6917586326599121 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6999632120132446 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.6752527952194214 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.7071623206138611 Accuracy 0.30000001192092896\n",
      "Epoch 0 Loss 0.6840792894363403 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6843122243881226 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6915157437324524 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6764539480209351 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6753103137016296 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6833696961402893 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6840936541557312 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6995463371276855 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.6586502194404602 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6827192306518555 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6742061376571655 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6916133165359497 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6914485692977905 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6633560657501221 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.681663453578949 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.659135103225708 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.7129026651382446 Accuracy 0.30000001192092896\n",
      "Epoch 0 Loss 0.7019960880279541 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.6681991815567017 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.7036556005477905 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.6910165548324585 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6794148683547974 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.7035906910896301 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.6786710023880005 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6786394715309143 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6915168166160583 Accuracy 0.5\n",
      "Epoch 0 Loss 0.7162898778915405 Accuracy 0.30000001192092896\n",
      "Epoch 0 Loss 0.7155116200447083 Accuracy 0.30000001192092896\n",
      "Epoch 0 Loss 0.7408192157745361 Accuracy 0.10000000149011612\n",
      "Epoch 0 Loss 0.702536404132843 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.6676799654960632 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.679702639579773 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.7016491293907166 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.7007580995559692 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.6791985034942627 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6688016057014465 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.7079586982727051 Accuracy 0.30000001192092896\n",
      "Epoch 0 Loss 0.6784875988960266 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6886057257652283 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6712484359741211 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6984978318214417 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.6619333028793335 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6882530450820923 Accuracy 0.5\n",
      "Epoch 0 Loss 0.733756959438324 Accuracy 0.0\n",
      "Epoch 0 Loss 0.6895809173583984 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6889090538024902 Accuracy 0.5\n",
      "Epoch 0 Loss 0.680144190788269 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6653644442558289 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.7020159363746643 Accuracy 0.30000001192092896\n",
      "Epoch 0 Loss 0.6970164179801941 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.6943433880805969 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.6870759129524231 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6729175448417664 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6864340305328369 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6790634393692017 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6818140745162964 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6939042806625366 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.6820566058158875 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.7003728151321411 Accuracy 0.30000001192092896\n",
      "Epoch 0 Loss 0.68394935131073 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6924947500228882 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.6861092448234558 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6702536344528198 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6864054203033447 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6901097297668457 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.6803525686264038 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6804637908935547 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6780811548233032 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.679862380027771 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6750586032867432 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6801183223724365 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6917808651924133 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.6869341135025024 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6851312518119812 Accuracy 0.5\n",
      "Epoch 0 Loss 0.687139630317688 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6854273080825806 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6799977421760559 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6703047752380371 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6971414685249329 Accuracy 0.30000001192092896\n",
      "Epoch 0 Loss 0.6779246926307678 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6850975155830383 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6742874383926392 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6850560307502747 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6726512312889099 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6986921429634094 Accuracy 0.30000001192092896\n",
      "Epoch 0 Loss 0.6814156174659729 Accuracy 0.5\n",
      "Epoch 0 Loss 0.670301079750061 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6891326904296875 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6633606553077698 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.674618124961853 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6765602827072144 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6937620043754578 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.6925224661827087 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.6760246157646179 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6677700877189636 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6928428411483765 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.6952164769172668 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.6694091558456421 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6750005483627319 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.676581084728241 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6647726893424988 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6833526492118835 Accuracy 0.5\n",
      "Epoch 0 Loss 0.702861487865448 Accuracy 0.30000001192092896\n",
      "Epoch 0 Loss 0.6832042932510376 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6598402261734009 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6712192893028259 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6757780909538269 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6850837469100952 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6942391395568848 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.6912473440170288 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.6615368723869324 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.686445951461792 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6987199187278748 Accuracy 0.30000001192092896\n",
      "Epoch 0 Loss 0.6855651140213013 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6959778070449829 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.6920387148857117 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.7025831937789917 Accuracy 0.30000001192092896\n",
      "Epoch 0 Loss 0.6839011311531067 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6909956336021423 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.6988644003868103 Accuracy 0.30000001192092896\n",
      "Epoch 0 Loss 0.6674009561538696 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.690360426902771 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.6955066919326782 Accuracy 0.30000001192092896\n",
      "Epoch 0 Loss 0.658393383026123 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.679291844367981 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6914511919021606 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.6693223118782043 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.663958728313446 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.666998028755188 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6752216815948486 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6820282340049744 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6716840267181396 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6598621606826782 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.7186500430107117 Accuracy 0.0\n",
      "Epoch 0 Loss 0.676959753036499 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6724851727485657 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6530983448028564 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.7016216516494751 Accuracy 0.20000000298023224\n",
      "Epoch 0 Loss 0.6604335904121399 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6799848079681396 Accuracy 0.5\n",
      "Epoch 0 Loss 0.7017958760261536 Accuracy 0.20000000298023224\n",
      "Epoch 0 Loss 0.6854686141014099 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.6822323799133301 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6775782108306885 Accuracy 0.5\n",
      "Epoch 0 Loss 0.665449857711792 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.690184473991394 Accuracy 0.30000001192092896\n",
      "Epoch 0 Loss 0.6801604628562927 Accuracy 0.5\n",
      "Epoch 0 Loss 0.672583281993866 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6911393404006958 Accuracy 0.30000001192092896\n",
      "Epoch 0 Loss 0.6697322130203247 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6949630379676819 Accuracy 0.30000001192092896\n",
      "Epoch 0 Loss 0.6689807176589966 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6794554591178894 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6712208390235901 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6731473207473755 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6766335368156433 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6926644444465637 Accuracy 0.30000001192092896\n",
      "Epoch 0 Loss 0.6921912431716919 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.6634363532066345 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6830588579177856 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6743212938308716 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6652454137802124 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6801062226295471 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6838970184326172 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6676238179206848 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6797107458114624 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6720288395881653 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6779669523239136 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6834477186203003 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6746309995651245 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6699403524398804 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6710718870162964 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6661726236343384 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6801601648330688 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6829822063446045 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6675816774368286 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6697302460670471 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6684167385101318 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6711427569389343 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6623881459236145 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6717618107795715 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6735502481460571 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6685193181037903 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6922391653060913 Accuracy 0.20000000298023224\n",
      "Epoch 0 Loss 0.671546459197998 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6827720999717712 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.6686949729919434 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6654869318008423 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6746443510055542 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6611939072608948 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6574382781982422 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6667059063911438 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6883323192596436 Accuracy 0.30000001192092896\n",
      "Epoch 0 Loss 0.6764082312583923 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6819981932640076 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.668121337890625 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6772063374519348 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6879442930221558 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.6574643850326538 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6678675413131714 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6716566681861877 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6814450025558472 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.664357602596283 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6780053377151489 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6634531021118164 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6825527548789978 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.6732756495475769 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6747277975082397 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6478109955787659 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6665850877761841 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6764845252037048 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6915051341056824 Accuracy 0.30000001192092896\n",
      "Epoch 0 Loss 0.6554386019706726 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6643719673156738 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6707901954650879 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6436546444892883 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6444907188415527 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6471672058105469 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6674216389656067 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6747260093688965 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6685609817504883 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6740584969520569 Accuracy 0.5\n",
      "Epoch 0 Loss 0.692089319229126 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.6715959310531616 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6791477799415588 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6846425533294678 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.6890519857406616 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.6831793785095215 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.659505307674408 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.676439106464386 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6370252370834351 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6872822642326355 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.6746436953544617 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6744641661643982 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6638970375061035 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6868194341659546 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.6752243638038635 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6671591997146606 Accuracy 0.5\n",
      "Epoch 0 Loss 0.7035084962844849 Accuracy 0.20000000298023224\n",
      "Epoch 0 Loss 0.6877014636993408 Accuracy 0.30000001192092896\n",
      "Epoch 0 Loss 0.6688796281814575 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6724818348884583 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6625913381576538 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.646080493927002 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6658603549003601 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6730685234069824 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.6835953593254089 Accuracy 0.30000001192092896\n",
      "Epoch 0 Loss 0.6912018656730652 Accuracy 0.30000001192092896\n",
      "Epoch 0 Loss 0.6440623998641968 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6732682585716248 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6636356115341187 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6819798350334167 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6746228933334351 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6799970865249634 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6674359440803528 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.66669762134552 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6889491081237793 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6540396213531494 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6722812652587891 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6714336276054382 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6697978377342224 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6736382246017456 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6724061369895935 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6708877682685852 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6663111448287964 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6629050374031067 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6707688570022583 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6664136052131653 Accuracy 1.0\n",
      "Epoch 0 Loss 0.671725332736969 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6676830053329468 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6768943071365356 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6644654870033264 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6616584062576294 Accuracy 1.0\n",
      "Epoch 0 Loss 0.661992609500885 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6627174615859985 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6591096520423889 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6736947298049927 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6697167158126831 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6711090803146362 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6629117727279663 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6549078226089478 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6608526706695557 Accuracy 1.0\n",
      "Epoch 0 Loss 0.670233428478241 Accuracy 1.0\n",
      "Epoch 0 Loss 0.658230185508728 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6612502336502075 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6635783910751343 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6613267064094543 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6620321273803711 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6735570430755615 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6567233800888062 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6708556413650513 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.670113742351532 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6663476228713989 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6791858077049255 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6567623615264893 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6651304960250854 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6719143986701965 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6675777435302734 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6691288352012634 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6576476097106934 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6787671446800232 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6607834100723267 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6648644208908081 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6615800261497498 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6650383472442627 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6696576476097107 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6704815626144409 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6757980585098267 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.668869137763977 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6661556959152222 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6584185361862183 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6579669117927551 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6723352670669556 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6683327555656433 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.661723256111145 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6659891605377197 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6546460390090942 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6601718664169312 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6628125905990601 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6552777290344238 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6667482256889343 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.667173445224762 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6608899831771851 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6634947061538696 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6608073711395264 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.660732090473175 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6589848399162292 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6508678197860718 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6608087420463562 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6514222621917725 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6649695634841919 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6674395799636841 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6492687463760376 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6556381583213806 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6584798097610474 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6693992018699646 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.671181321144104 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6789841651916504 Accuracy 0.5\n",
      "Epoch 0 Loss 0.6603151559829712 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6720199584960938 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6528847217559814 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6669572591781616 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6546082496643066 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6691873669624329 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6573531627655029 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6483904123306274 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6787906885147095 Accuracy 0.4000000059604645\n",
      "Epoch 0 Loss 0.6782479286193848 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6542290449142456 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6688925623893738 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6684918999671936 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6639548540115356 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6574279069900513 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6534760594367981 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6697648763656616 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6640383005142212 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6608127355575562 Accuracy 1.0\n",
      "Epoch 0 Loss 0.663051187992096 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6678396463394165 Accuracy 1.0\n",
      "Epoch 0 Loss 0.652484118938446 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6553806662559509 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6634258031845093 Accuracy 1.0\n",
      "Epoch 0 Loss 0.655219554901123 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6692095994949341 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6658086776733398 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.665768027305603 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6548066139221191 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6524984240531921 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6627607941627502 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6660986542701721 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.645977258682251 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6574802398681641 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6653608083724976 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6602197885513306 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6544979810714722 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6579621434211731 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6470718383789062 Accuracy 1.0\n",
      "Epoch 0 Loss 0.674340009689331 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6505563259124756 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6355616450309753 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6693204641342163 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6538191437721252 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.655683159828186 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6541715860366821 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6338428258895874 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6518664956092834 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6632441878318787 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6626014113426208 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6703544855117798 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6493104100227356 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6614648699760437 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.65135657787323 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6465449333190918 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6612530946731567 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6542111039161682 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6532004475593567 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6639235019683838 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6692218780517578 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6346357464790344 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6354563236236572 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6510476469993591 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6496160626411438 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.641316294670105 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6543757319450378 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6553606390953064 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6606603860855103 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6495527029037476 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6662136912345886 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6627200245857239 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6512206792831421 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6544158458709717 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6430319547653198 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6627217531204224 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6556078195571899 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6594899892807007 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6585229635238647 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6347686052322388 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.655269980430603 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6700454950332642 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6595758199691772 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.650789201259613 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6607488989830017 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6318101286888123 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6559892892837524 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6569118499755859 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6407485008239746 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6404058337211609 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6549137830734253 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6605460047721863 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6413007378578186 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6616791486740112 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6536377668380737 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.648476779460907 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6719462275505066 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6568840146064758 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6637089848518372 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6411179900169373 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6551855206489563 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6395172476768494 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6602888107299805 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6356273889541626 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6516204476356506 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6648635864257812 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6355417370796204 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6492608785629272 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6462341547012329 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6510404348373413 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6515815854072571 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6326512098312378 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6284404397010803 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6428871750831604 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6499950885772705 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6602752804756165 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6341807246208191 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6528241038322449 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6548105478286743 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6599540114402771 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6479955911636353 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6541024446487427 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6461316347122192 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6527994275093079 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6506593227386475 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6609066724777222 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6445270776748657 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6550668478012085 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6394151449203491 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6401222348213196 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6552642583847046 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6623467206954956 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6402174830436707 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6410471200942993 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6364009976387024 Accuracy 1.0\n",
      "Epoch 0 Loss 0.641197144985199 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6480388045310974 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6480421423912048 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6518875956535339 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6409457921981812 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6432842016220093 Accuracy 1.0\n",
      "Epoch 0 Loss 0.643856406211853 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6439569592475891 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6285123229026794 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6454370021820068 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6541047692298889 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6295050382614136 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6375391483306885 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6501359939575195 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6365809440612793 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6498844623565674 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6464471817016602 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6521231532096863 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6388667225837708 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6343604326248169 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6390649080276489 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6447865962982178 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.627159833908081 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6433463096618652 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6491076350212097 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6523523926734924 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6337907910346985 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6414331197738647 Accuracy 1.0\n",
      "Epoch 0 Loss 0.650492787361145 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6459356546401978 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6299989819526672 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6459898948669434 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6601865887641907 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6352357864379883 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6267803311347961 Accuracy 1.0\n",
      "Epoch 0 Loss 0.653777003288269 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6557061076164246 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6392229795455933 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6454190611839294 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6349077820777893 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6275240182876587 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6499791145324707 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6450314521789551 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6475018858909607 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6545823216438293 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6307219862937927 Accuracy 1.0\n",
      "Epoch 0 Loss 0.644908607006073 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6454237699508667 Accuracy 1.0\n",
      "Epoch 0 Loss 0.644698441028595 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6421715021133423 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6368392705917358 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6203181743621826 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6319234371185303 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6353565454483032 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6312088966369629 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6439821124076843 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6445281505584717 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6420053243637085 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6497465968132019 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6303977370262146 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6516165137290955 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6585151553153992 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6257595419883728 Accuracy 1.0\n",
      "Epoch 0 Loss 0.622677206993103 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6256929636001587 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6154573559761047 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6244343519210815 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6315575838088989 Accuracy 1.0\n",
      "Epoch 0 Loss 0.653761088848114 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6363281011581421 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6453979015350342 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6249531507492065 Accuracy 1.0\n",
      "Epoch 0 Loss 0.626975953578949 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.642970860004425 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6659003496170044 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.635428249835968 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.622449517250061 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6436566114425659 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6146324872970581 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6414799094200134 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6415031552314758 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6420508623123169 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6511101722717285 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6454542875289917 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6466296315193176 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6363353729248047 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6342276334762573 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6491656303405762 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6278782486915588 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6293094158172607 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6133216619491577 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6452499628067017 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6505768895149231 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6264897584915161 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.634067714214325 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6458969116210938 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6294784545898438 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6548276543617249 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6428217887878418 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6263488531112671 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6323748230934143 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.649031400680542 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.642187237739563 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6378676891326904 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6414772272109985 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6411453485488892 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6257712841033936 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6194077134132385 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6390894055366516 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.628637433052063 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6227189898490906 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6426600217819214 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6219069361686707 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6246243119239807 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6402602195739746 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6161364316940308 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6306232810020447 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6147698163986206 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6107387542724609 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6322565078735352 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6397305727005005 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.63567715883255 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6191003918647766 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6493363380432129 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6582797765731812 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6375797986984253 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6336463689804077 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6240944862365723 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6260063648223877 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6291040778160095 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6224378943443298 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6430124044418335 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6274257302284241 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6409627199172974 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6251078248023987 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6187335848808289 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6444774866104126 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6394266486167908 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6093524694442749 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6148681640625 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6298964023590088 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.611772894859314 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6092702150344849 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6357504725456238 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6214907765388489 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6358681917190552 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6196749806404114 Accuracy 1.0\n",
      "Epoch 0 Loss 0.639782726764679 Accuracy 1.0\n",
      "Epoch 0 Loss 0.604812502861023 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6387956738471985 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6290185451507568 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6280542612075806 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6386128664016724 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6252166628837585 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6351910829544067 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6072409152984619 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6170013546943665 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6417940855026245 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.633776843547821 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6110345721244812 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6272851228713989 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6249982118606567 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6189516186714172 Accuracy 1.0\n",
      "Epoch 0 Loss 0.61871737241745 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6152120232582092 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6163843870162964 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6016414165496826 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6230486631393433 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6124565005302429 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6384992003440857 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6385578513145447 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6091703176498413 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6051324605941772 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6111736297607422 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6256526708602905 Accuracy 1.0\n",
      "Epoch 0 Loss 0.632866621017456 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6408922076225281 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6390003561973572 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6325064301490784 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6114081144332886 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6145209670066833 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6133056282997131 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6309711933135986 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6205017566680908 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6275347471237183 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6261295080184937 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6116542816162109 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6390317678451538 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5976899862289429 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6214091777801514 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6140447854995728 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6039032936096191 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6219330430030823 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6112350821495056 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6323263049125671 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6184539794921875 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6188138723373413 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6112398505210876 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6311268210411072 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6142483949661255 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6431552171707153 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6087859272956848 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6070833206176758 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6324750781059265 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6208046674728394 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6129786372184753 Accuracy 1.0\n",
      "Epoch 0 Loss 0.602137565612793 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.5981721878051758 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6083385944366455 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6256625056266785 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6185864210128784 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6165351867675781 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6332539319992065 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6206759214401245 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6010235548019409 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6141936779022217 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6256282925605774 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6020902991294861 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6348137855529785 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6191247701644897 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6073340177536011 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6237245202064514 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6110049486160278 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6248642206192017 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6085335612297058 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6091910600662231 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5912231802940369 Accuracy 1.0\n",
      "Epoch 0 Loss 0.591072142124176 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6071062088012695 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5825715661048889 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6329743266105652 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6011852622032166 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6347652077674866 Accuracy 1.0\n",
      "Epoch 0 Loss 0.631348729133606 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6205306053161621 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5933923125267029 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6180232763290405 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6068049073219299 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6125388741493225 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6255537271499634 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6292833089828491 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6078671216964722 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5957069993019104 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6053342223167419 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6272090673446655 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6161394715309143 Accuracy 1.0\n",
      "Epoch 0 Loss 0.620658814907074 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6084681153297424 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6290254592895508 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6396397352218628 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.598547637462616 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6478909254074097 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6268430352210999 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6047360897064209 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5927271842956543 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6435171961784363 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.5979973077774048 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.596759021282196 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6034046411514282 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.5978512763977051 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6084094047546387 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5848999619483948 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6247235536575317 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6124979257583618 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5878626108169556 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6005595922470093 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6119138598442078 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6302275657653809 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.641005277633667 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.5999853014945984 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.5835564136505127 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5992655754089355 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6258193254470825 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.6189942359924316 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6192549467086792 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6242327690124512 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6141180992126465 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5943065881729126 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6165479421615601 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6382663249969482 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.5727341771125793 Accuracy 1.0\n",
      "Epoch 0 Loss 0.604711651802063 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6017335057258606 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6142874360084534 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5931916236877441 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6053504347801208 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.5919778347015381 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6074918508529663 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6025040745735168 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6364010572433472 Accuracy 0.6000000238418579\n",
      "Epoch 0 Loss 0.6159394383430481 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.5951062440872192 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6041253209114075 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6114165186882019 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6149784922599792 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6385756134986877 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.5889084935188293 Accuracy 1.0\n",
      "Epoch 0 Loss 0.576742947101593 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6249104738235474 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.617867112159729 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.5950068831443787 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6125839948654175 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5858746767044067 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6002746224403381 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6089953184127808 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6018101572990417 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6037126183509827 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5939532518386841 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6361082196235657 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6019293069839478 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.5978962779045105 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5987590551376343 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6109539270401001 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6165212392807007 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.5859969258308411 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5797943472862244 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6250184774398804 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6203463077545166 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.5704475045204163 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6083219647407532 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.5887531042098999 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.612151563167572 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6231389045715332 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.564816951751709 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6126397848129272 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5863120555877686 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6142935752868652 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5727745890617371 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6018339991569519 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6124441623687744 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.5767027139663696 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6062451601028442 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.5909063816070557 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.5736306309700012 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5930033922195435 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5977597236633301 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6066528558731079 Accuracy 1.0\n",
      "Epoch 0 Loss 0.585263192653656 Accuracy 1.0\n",
      "Epoch 0 Loss 0.561667799949646 Accuracy 1.0\n",
      "Epoch 0 Loss 0.574206531047821 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6174348592758179 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.5981820821762085 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.5685850381851196 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5701176524162292 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5835402011871338 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.598924458026886 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6273235082626343 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.5830680727958679 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6084217429161072 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6213361620903015 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.5974339246749878 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6064680814743042 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6333398818969727 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.5991454124450684 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5877707004547119 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5462684631347656 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6074222326278687 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5995976328849792 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6000332832336426 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5813837051391602 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5923217535018921 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5836594700813293 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6008392572402954 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.5915652513504028 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5907884836196899 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5905096530914307 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.5995420813560486 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.5703689455986023 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5673431158065796 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5824066996574402 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.5810824632644653 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5981425046920776 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.5853468775749207 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6050008535385132 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6190968751907349 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.585983157157898 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6078234910964966 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6009770035743713 Accuracy 1.0\n",
      "Epoch 0 Loss 0.574134349822998 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5940703749656677 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5833927392959595 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.5693145394325256 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5626038908958435 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5828913450241089 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6042441725730896 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5995684862136841 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5902116894721985 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5771253108978271 Accuracy 1.0\n",
      "Epoch 0 Loss 0.53944993019104 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5797184109687805 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6321107149124146 Accuracy 0.699999988079071\n",
      "Epoch 0 Loss 0.5709429979324341 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6049754023551941 Accuracy 1.0\n",
      "Epoch 0 Loss 0.612822413444519 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.5899298787117004 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.5619717836380005 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.594840407371521 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5693491697311401 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5984035730361938 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.553837239742279 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5621666312217712 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6026724576950073 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5821275115013123 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.5837175250053406 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.5788215398788452 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.6093200445175171 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5829677581787109 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5933063626289368 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5622930526733398 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.5874419212341309 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.5550289750099182 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6190780401229858 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6111057996749878 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6367977857589722 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.5828143954277039 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5619369149208069 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6126346588134766 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.5757914185523987 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6087589859962463 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.5771519541740417 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5831568837165833 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.5831345319747925 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.560371994972229 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.5976711511611938 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.5375043749809265 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6127137541770935 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.5596960186958313 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.6076772212982178 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.5765992403030396 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.5783178806304932 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6033103466033936 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6184612512588501 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.5845257043838501 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.5644729733467102 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5942007899284363 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.5983266830444336 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.5862494111061096 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5853579044342041 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5780523419380188 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.5525217056274414 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6117494106292725 Accuracy 0.800000011920929\n",
      "Epoch 0 Loss 0.5880832672119141 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5269501805305481 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5804736614227295 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.5717883706092834 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5867699980735779 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5540634989738464 Accuracy 1.0\n",
      "Epoch 0 Loss 0.6060327291488647 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5473236441612244 Accuracy 1.0\n",
      "Epoch 0 Loss 0.5817375779151917 Accuracy 0.8999999761581421\n",
      "Epoch 0 Loss 0.5664540529251099 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5641568899154663 Accuracy 1.0\n",
      "Epoch 1 Loss 0.565234899520874 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5973635911941528 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5984858274459839 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5582550764083862 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5813025236129761 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5714529752731323 Accuracy 1.0\n",
      "Epoch 1 Loss 0.564287543296814 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5581041574478149 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5534032583236694 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5685826539993286 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5665327906608582 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5863546133041382 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5574896335601807 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5636208057403564 Accuracy 1.0\n",
      "Epoch 1 Loss 0.563262939453125 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5312358736991882 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5918161273002625 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5496325492858887 Accuracy 1.0\n",
      "Epoch 1 Loss 0.562198281288147 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5603073835372925 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5610942840576172 Accuracy 1.0\n",
      "Epoch 1 Loss 0.6028541922569275 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5692774653434753 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5739327669143677 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.6077010035514832 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5851770639419556 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5459964871406555 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5433951616287231 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5635477304458618 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5570200681686401 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5989232659339905 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5582764148712158 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5797299146652222 Accuracy 1.0\n",
      "Epoch 1 Loss 0.580769419670105 Accuracy 1.0\n",
      "Epoch 1 Loss 0.6212679743766785 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5518130660057068 Accuracy 1.0\n",
      "Epoch 1 Loss 0.558533787727356 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5897012948989868 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5299558043479919 Accuracy 1.0\n",
      "Epoch 1 Loss 0.588329017162323 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5629123449325562 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5732197761535645 Accuracy 1.0\n",
      "Epoch 1 Loss 0.573489785194397 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5701368451118469 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5207686424255371 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5631966590881348 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5605211853981018 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5347055196762085 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5953927040100098 Accuracy 1.0\n",
      "Epoch 1 Loss 0.580263614654541 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5805641412734985 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5488384962081909 Accuracy 1.0\n",
      "Epoch 1 Loss 0.588718056678772 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5778299570083618 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5792475938796997 Accuracy 1.0\n",
      "Epoch 1 Loss 0.6229373812675476 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5893927812576294 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5652386546134949 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5597066283226013 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5535218119621277 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5640408396720886 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5392458438873291 Accuracy 1.0\n",
      "Epoch 1 Loss 0.564182698726654 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5705110430717468 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5558719038963318 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5749517679214478 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5613893270492554 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5442808866500854 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5349262952804565 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5547670125961304 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5844155550003052 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5368326306343079 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5374231934547424 Accuracy 1.0\n",
      "Epoch 1 Loss 0.537280261516571 Accuracy 1.0\n",
      "Epoch 1 Loss 0.544282078742981 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5355702042579651 Accuracy 1.0\n",
      "Epoch 1 Loss 0.574280858039856 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5803075432777405 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5736628770828247 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5535918474197388 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5091023445129395 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5469727516174316 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5850311517715454 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5957894921302795 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5114597678184509 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5530911684036255 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5558911561965942 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5840142369270325 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5762373805046082 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5720924735069275 Accuracy 1.0\n",
      "Epoch 1 Loss 0.573885977268219 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5011807084083557 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5726715326309204 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5498260259628296 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5353089570999146 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5915743112564087 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5490339398384094 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5562648773193359 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5720055103302002 Accuracy 0.800000011920929\n",
      "Epoch 1 Loss 0.5419744253158569 Accuracy 0.800000011920929\n",
      "Epoch 1 Loss 0.546786367893219 Accuracy 0.800000011920929\n",
      "Epoch 1 Loss 0.5692015886306763 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5742524862289429 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5554919242858887 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.6073821783065796 Accuracy 0.800000011920929\n",
      "Epoch 1 Loss 0.5650849342346191 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5495763421058655 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5657630562782288 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5838617086410522 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5713828802108765 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5406027436256409 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5199993848800659 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5410150289535522 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5147610902786255 Accuracy 1.0\n",
      "Epoch 1 Loss 0.541129469871521 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5531412959098816 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5760775208473206 Accuracy 1.0\n",
      "Epoch 1 Loss 0.530218780040741 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5420018434524536 Accuracy 1.0\n",
      "Epoch 1 Loss 0.6003133654594421 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5783440470695496 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5677372217178345 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5488867163658142 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5562595129013062 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5287408828735352 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5923160314559937 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.537291407585144 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5416617393493652 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5276744365692139 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5358939170837402 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5162402987480164 Accuracy 1.0\n",
      "Epoch 1 Loss 0.57253497838974 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5598152875900269 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5765967965126038 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5718004107475281 Accuracy 1.0\n",
      "Epoch 1 Loss 0.49605804681777954 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5479329824447632 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5428647994995117 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5621867179870605 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5505993962287903 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5217657685279846 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5407071113586426 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5461868047714233 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5052004456520081 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.538634181022644 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5535879731178284 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5479165315628052 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5471130609512329 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5683994889259338 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5374504923820496 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5702105164527893 Accuracy 1.0\n",
      "Epoch 1 Loss 0.544773280620575 Accuracy 1.0\n",
      "Epoch 1 Loss 0.560465931892395 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5059354901313782 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5248935222625732 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5315176248550415 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5427867770195007 Accuracy 1.0\n",
      "Epoch 1 Loss 0.47791457176208496 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5430589318275452 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5627185106277466 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5452194213867188 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.49132204055786133 Accuracy 1.0\n",
      "Epoch 1 Loss 0.534061849117279 Accuracy 1.0\n",
      "Epoch 1 Loss 0.6128275990486145 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5415897369384766 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4991922378540039 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5358811020851135 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5643231868743896 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.544804036617279 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5384715795516968 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5368677973747253 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5446807742118835 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5764651298522949 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5721948146820068 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5301036238670349 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5529736876487732 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5178838968276978 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5281455516815186 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5379336476325989 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5245445966720581 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4668692648410797 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4908069670200348 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5598179697990417 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5523668527603149 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5397118926048279 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5057857632637024 Accuracy 1.0\n",
      "Epoch 1 Loss 0.506513237953186 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5752349495887756 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.4730379581451416 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5649575591087341 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5738944411277771 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5244065523147583 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5405943393707275 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5503000617027283 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5309958457946777 Accuracy 1.0\n",
      "Epoch 1 Loss 0.530689537525177 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5468420386314392 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5430067777633667 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5231731534004211 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4945001006126404 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5849116444587708 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5768951177597046 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5347287058830261 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5400825142860413 Accuracy 1.0\n",
      "Epoch 1 Loss 0.510047197341919 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5386167168617249 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5426246523857117 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4980277121067047 Accuracy 1.0\n",
      "Epoch 1 Loss 0.524745762348175 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5604153871536255 Accuracy 1.0\n",
      "Epoch 1 Loss 0.568130373954773 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5178577899932861 Accuracy 1.0\n",
      "Epoch 1 Loss 0.46122464537620544 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5211088061332703 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.46716204285621643 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5248796939849854 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5201041102409363 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.505422830581665 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5498577952384949 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5064482688903809 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5221239328384399 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4944119453430176 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5355130434036255 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5221652984619141 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5148093700408936 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4935525953769684 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5662126541137695 Accuracy 1.0\n",
      "Epoch 1 Loss 0.48135942220687866 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5348579287528992 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5028730630874634 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5828281044960022 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5076063275337219 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5643056631088257 Accuracy 1.0\n",
      "Epoch 1 Loss 0.514189600944519 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4709172248840332 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5811855792999268 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.48814716935157776 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5239261388778687 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5467584729194641 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5475281476974487 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4649065434932709 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5498543977737427 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5079111456871033 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5288113355636597 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5045145750045776 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5376964211463928 Accuracy 1.0\n",
      "Epoch 1 Loss 0.531525731086731 Accuracy 1.0\n",
      "Epoch 1 Loss 0.46682268381118774 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5047174096107483 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5125367045402527 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5424350500106812 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4356159567832947 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5392063856124878 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4825761914253235 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5137702226638794 Accuracy 1.0\n",
      "Epoch 1 Loss 0.514724850654602 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5206397771835327 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4944690763950348 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5009247660636902 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5119757652282715 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5262647271156311 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5234800577163696 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5060679912567139 Accuracy 1.0\n",
      "Epoch 1 Loss 0.47738200426101685 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5117470026016235 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4965735971927643 Accuracy 1.0\n",
      "Epoch 1 Loss 0.46702784299850464 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5591605305671692 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5090246796607971 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5196781754493713 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5099188089370728 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5105388760566711 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5301109552383423 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5603710412979126 Accuracy 1.0\n",
      "Epoch 1 Loss 0.488166481256485 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5179158449172974 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.4870428144931793 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5048452615737915 Accuracy 1.0\n",
      "Epoch 1 Loss 0.559071958065033 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5431168675422668 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.4826292395591736 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5113746523857117 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5727748274803162 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5147372484207153 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4933405816555023 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5180928707122803 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5279995203018188 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5104668736457825 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4984685778617859 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5030426979064941 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.4780072569847107 Accuracy 1.0\n",
      "Epoch 1 Loss 0.48007717728614807 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4959016442298889 Accuracy 1.0\n",
      "Epoch 1 Loss 0.44614964723587036 Accuracy 1.0\n",
      "Epoch 1 Loss 0.45357218384742737 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5132488012313843 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5540192723274231 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5046735405921936 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5644474029541016 Accuracy 0.800000011920929\n",
      "Epoch 1 Loss 0.48795753717422485 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5491417646408081 Accuracy 0.800000011920929\n",
      "Epoch 1 Loss 0.4977502226829529 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5361194610595703 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4954169690608978 Accuracy 1.0\n",
      "Epoch 1 Loss 0.49153032898902893 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5274122953414917 Accuracy 1.0\n",
      "Epoch 1 Loss 0.482899010181427 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5302581191062927 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5139241814613342 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5154864192008972 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5221002101898193 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5321956872940063 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5245092511177063 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4630494713783264 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4971589148044586 Accuracy 1.0\n",
      "Epoch 1 Loss 0.46771422028541565 Accuracy 1.0\n",
      "Epoch 1 Loss 0.481486976146698 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5070703625679016 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5009121298789978 Accuracy 1.0\n",
      "Epoch 1 Loss 0.44583144783973694 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4626862406730652 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4558737874031067 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4743821620941162 Accuracy 1.0\n",
      "Epoch 1 Loss 0.47973424196243286 Accuracy 1.0\n",
      "Epoch 1 Loss 0.47001156210899353 Accuracy 1.0\n",
      "Epoch 1 Loss 0.46963876485824585 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5065951943397522 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5406684875488281 Accuracy 1.0\n",
      "Epoch 1 Loss 0.48983973264694214 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5342165231704712 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5247496366500854 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5177487134933472 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4994780123233795 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4589060842990875 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5266819596290588 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5188137888908386 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5100713968276978 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5107206106185913 Accuracy 1.0\n",
      "Epoch 1 Loss 0.47798746824264526 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5378965735435486 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.466566264629364 Accuracy 1.0\n",
      "Epoch 1 Loss 0.48029327392578125 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5506317019462585 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5213918685913086 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5106005072593689 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4999710023403168 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5371025204658508 Accuracy 1.0\n",
      "Epoch 1 Loss 0.47731366753578186 Accuracy 1.0\n",
      "Epoch 1 Loss 0.47067636251449585 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4915889799594879 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4793813228607178 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4545741081237793 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5078414082527161 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5022339224815369 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5156463980674744 Accuracy 1.0\n",
      "Epoch 1 Loss 0.46789059042930603 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.4352846145629883 Accuracy 1.0\n",
      "Epoch 1 Loss 0.47580471634864807 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5362110137939453 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.4387560784816742 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4791845679283142 Accuracy 1.0\n",
      "Epoch 1 Loss 0.48617225885391235 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5044445991516113 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.462075412273407 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5333434343338013 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4686921238899231 Accuracy 1.0\n",
      "Epoch 1 Loss 0.47463458776474 Accuracy 1.0\n",
      "Epoch 1 Loss 0.502012312412262 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5365426540374756 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5678927302360535 Accuracy 1.0\n",
      "Epoch 1 Loss 0.47407063841819763 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4962364137172699 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5392700433731079 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.4674034118652344 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5001789927482605 Accuracy 1.0\n",
      "Epoch 1 Loss 0.45303893089294434 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5679494142532349 Accuracy 1.0\n",
      "Epoch 1 Loss 0.48124462366104126 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4548324644565582 Accuracy 1.0\n",
      "Epoch 1 Loss 0.506994903087616 Accuracy 1.0\n",
      "Epoch 1 Loss 0.48110175132751465 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4692607820034027 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5054548978805542 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5367792248725891 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5142110586166382 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5003523230552673 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4569811224937439 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4435548782348633 Accuracy 1.0\n",
      "Epoch 1 Loss 0.535741925239563 Accuracy 0.800000011920929\n",
      "Epoch 1 Loss 0.5130980014801025 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.48420119285583496 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.506937563419342 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4500122666358948 Accuracy 1.0\n",
      "Epoch 1 Loss 0.45912593603134155 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5017868876457214 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.44732147455215454 Accuracy 1.0\n",
      "Epoch 1 Loss 0.524321973323822 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.4918449819087982 Accuracy 1.0\n",
      "Epoch 1 Loss 0.489316463470459 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5103071928024292 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4400387406349182 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.48045477271080017 Accuracy 0.800000011920929\n",
      "Epoch 1 Loss 0.4825325608253479 Accuracy 1.0\n",
      "Epoch 1 Loss 0.41779452562332153 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5112589597702026 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4551132321357727 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4897667467594147 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5030627250671387 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4340580403804779 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.43290066719055176 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4534887671470642 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5165637731552124 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5464583039283752 Accuracy 0.800000011920929\n",
      "Epoch 1 Loss 0.5784875154495239 Accuracy 0.800000011920929\n",
      "Epoch 1 Loss 0.48352116346359253 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5396804809570312 Accuracy 0.699999988079071\n",
      "Epoch 1 Loss 0.4334019124507904 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5219076871871948 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.4708467125892639 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5243692994117737 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.42685118317604065 Accuracy 1.0\n",
      "Epoch 1 Loss 0.44683438539505005 Accuracy 1.0\n",
      "Epoch 1 Loss 0.567654013633728 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5580118894577026 Accuracy 0.800000011920929\n",
      "Epoch 1 Loss 0.45035257935523987 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5332468748092651 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5357130765914917 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5050386786460876 Accuracy 1.0\n",
      "Epoch 1 Loss 0.47365251183509827 Accuracy 1.0\n",
      "Epoch 1 Loss 0.44397592544555664 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5358075499534607 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5069597959518433 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.4947168827056885 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.49916577339172363 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5375280976295471 Accuracy 1.0\n",
      "Epoch 1 Loss 0.44372257590293884 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4537414014339447 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5124133825302124 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.4590516984462738 Accuracy 1.0\n",
      "Epoch 1 Loss 0.528092086315155 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.5087252855300903 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5386708378791809 Accuracy 1.0\n",
      "Epoch 1 Loss 0.45353808999061584 Accuracy 1.0\n",
      "Epoch 1 Loss 0.46341389417648315 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4975007176399231 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5058093070983887 Accuracy 1.0\n",
      "Epoch 1 Loss 0.42705225944519043 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4807954430580139 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5050560235977173 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4651087820529938 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4920744001865387 Accuracy 1.0\n",
      "Epoch 1 Loss 0.49340564012527466 Accuracy 1.0\n",
      "Epoch 1 Loss 0.45258450508117676 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5106511116027832 Accuracy 1.0\n",
      "Epoch 1 Loss 0.45065587759017944 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4014490246772766 Accuracy 1.0\n",
      "Epoch 1 Loss 0.49869269132614136 Accuracy 1.0\n",
      "Epoch 1 Loss 0.44029292464256287 Accuracy 1.0\n",
      "Epoch 1 Loss 0.48274365067481995 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4613649845123291 Accuracy 1.0\n",
      "Epoch 1 Loss 0.41015106439590454 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4486912786960602 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4903221130371094 Accuracy 1.0\n",
      "Epoch 1 Loss 0.48810920119285583 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5479645133018494 Accuracy 1.0\n",
      "Epoch 1 Loss 0.47130221128463745 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.48451104760169983 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5023102760314941 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.4584856629371643 Accuracy 1.0\n",
      "Epoch 1 Loss 0.463735967874527 Accuracy 1.0\n",
      "Epoch 1 Loss 0.46093469858169556 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4878579080104828 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5121820569038391 Accuracy 1.0\n",
      "Epoch 1 Loss 0.49373239278793335 Accuracy 1.0\n",
      "Epoch 1 Loss 0.40052223205566406 Accuracy 1.0\n",
      "Epoch 1 Loss 0.41543832421302795 Accuracy 1.0\n",
      "Epoch 1 Loss 0.47431936860084534 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4620773196220398 Accuracy 1.0\n",
      "Epoch 1 Loss 0.44391098618507385 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4630846083164215 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5031965374946594 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4651104807853699 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4064715504646301 Accuracy 1.0\n",
      "Epoch 1 Loss 0.49208754301071167 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5097506642341614 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4828101098537445 Accuracy 1.0\n",
      "Epoch 1 Loss 0.40989571809768677 Accuracy 1.0\n",
      "Epoch 1 Loss 0.44339004158973694 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4863428473472595 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4371716380119324 Accuracy 1.0\n",
      "Epoch 1 Loss 0.46627306938171387 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5193506479263306 Accuracy 1.0\n",
      "Epoch 1 Loss 0.44834309816360474 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4427361488342285 Accuracy 1.0\n",
      "Epoch 1 Loss 0.49787425994873047 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5234571695327759 Accuracy 1.0\n",
      "Epoch 1 Loss 0.45199888944625854 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4731386601924896 Accuracy 1.0\n",
      "Epoch 1 Loss 0.42399293184280396 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4760427474975586 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4900668263435364 Accuracy 1.0\n",
      "Epoch 1 Loss 0.42274340987205505 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4221535623073578 Accuracy 1.0\n",
      "Epoch 1 Loss 0.44920435547828674 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5079122185707092 Accuracy 1.0\n",
      "Epoch 1 Loss 0.42979270219802856 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5163734555244446 Accuracy 1.0\n",
      "Epoch 1 Loss 0.49595150351524353 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4408404231071472 Accuracy 1.0\n",
      "Epoch 1 Loss 0.49401941895484924 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4455508291721344 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4780738949775696 Accuracy 1.0\n",
      "Epoch 1 Loss 0.41063523292541504 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5007911920547485 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4205394685268402 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4933686852455139 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4164111018180847 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4509539008140564 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5251615047454834 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3990616798400879 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4636495113372803 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4765704274177551 Accuracy 1.0\n",
      "Epoch 1 Loss 0.45686906576156616 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.4810849130153656 Accuracy 1.0\n",
      "Epoch 1 Loss 0.40045347809791565 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3638758063316345 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4252970814704895 Accuracy 1.0\n",
      "Epoch 1 Loss 0.463203102350235 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5000885128974915 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.36228951811790466 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4545120596885681 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4582931101322174 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4915730953216553 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4441990852355957 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4694632887840271 Accuracy 1.0\n",
      "Epoch 1 Loss 0.46158117055892944 Accuracy 1.0\n",
      "Epoch 1 Loss 0.46480482816696167 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4680333137512207 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5211483240127563 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.4387381076812744 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4873611032962799 Accuracy 0.800000011920929\n",
      "Epoch 1 Loss 0.40481019020080566 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4326383173465729 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.45974573493003845 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5033553838729858 Accuracy 0.800000011920929\n",
      "Epoch 1 Loss 0.40164613723754883 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4161888062953949 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4114801287651062 Accuracy 1.0\n",
      "Epoch 1 Loss 0.43925076723098755 Accuracy 1.0\n",
      "Epoch 1 Loss 0.45494237542152405 Accuracy 1.0\n",
      "Epoch 1 Loss 0.46035856008529663 Accuracy 1.0\n",
      "Epoch 1 Loss 0.47942835092544556 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4390029013156891 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.4379919469356537 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4395364224910736 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4396337568759918 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3681427538394928 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4498381018638611 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4875620901584625 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3785959482192993 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4137548804283142 Accuracy 1.0\n",
      "Epoch 1 Loss 0.47396713495254517 Accuracy 1.0\n",
      "Epoch 1 Loss 0.41480207443237305 Accuracy 1.0\n",
      "Epoch 1 Loss 0.46957331895828247 Accuracy 1.0\n",
      "Epoch 1 Loss 0.45656323432922363 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4816426634788513 Accuracy 1.0\n",
      "Epoch 1 Loss 0.42303118109703064 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.40689998865127563 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4261527955532074 Accuracy 1.0\n",
      "Epoch 1 Loss 0.45540913939476013 Accuracy 0.800000011920929\n",
      "Epoch 1 Loss 0.3786170482635498 Accuracy 1.0\n",
      "Epoch 1 Loss 0.44338417053222656 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4649318754673004 Accuracy 1.0\n",
      "Epoch 1 Loss 0.48845797777175903 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.4087677597999573 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4406709671020508 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4750460088253021 Accuracy 1.0\n",
      "Epoch 1 Loss 0.45267122983932495 Accuracy 1.0\n",
      "Epoch 1 Loss 0.39369621872901917 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4586482048034668 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5172237157821655 Accuracy 1.0\n",
      "Epoch 1 Loss 0.40860041975975037 Accuracy 1.0\n",
      "Epoch 1 Loss 0.37918367981910706 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5051048994064331 Accuracy 0.800000011920929\n",
      "Epoch 1 Loss 0.4973282814025879 Accuracy 1.0\n",
      "Epoch 1 Loss 0.433098703622818 Accuracy 1.0\n",
      "Epoch 1 Loss 0.45226946473121643 Accuracy 1.0\n",
      "Epoch 1 Loss 0.41373759508132935 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.3847592771053314 Accuracy 1.0\n",
      "Epoch 1 Loss 0.48373278975486755 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4589381217956543 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.46989989280700684 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5046719908714294 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3905499279499054 Accuracy 1.0\n",
      "Epoch 1 Loss 0.45292145013809204 Accuracy 1.0\n",
      "Epoch 1 Loss 0.45922690629959106 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4602507948875427 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.4518672823905945 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4259123206138611 Accuracy 1.0\n",
      "Epoch 1 Loss 0.35532626509666443 Accuracy 1.0\n",
      "Epoch 1 Loss 0.40167760848999023 Accuracy 1.0\n",
      "Epoch 1 Loss 0.41706305742263794 Accuracy 1.0\n",
      "Epoch 1 Loss 0.405609667301178 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4527434706687927 Accuracy 1.0\n",
      "Epoch 1 Loss 0.46247631311416626 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.4552536606788635 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4848710596561432 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.4073779582977295 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4913002848625183 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5241149663925171 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.3877606987953186 Accuracy 1.0\n",
      "Epoch 1 Loss 0.38368090987205505 Accuracy 1.0\n",
      "Epoch 1 Loss 0.40204325318336487 Accuracy 1.0\n",
      "Epoch 1 Loss 0.349424809217453 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3834789991378784 Accuracy 1.0\n",
      "Epoch 1 Loss 0.39317548274993896 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4910792410373688 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4228017330169678 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4703443646430969 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.39992842078208923 Accuracy 1.0\n",
      "Epoch 1 Loss 0.43551188707351685 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.43524232506752014 Accuracy 1.0\n",
      "Epoch 1 Loss 0.49778681993484497 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4256807267665863 Accuracy 1.0\n",
      "Epoch 1 Loss 0.41133612394332886 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4272189140319824 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3579537570476532 Accuracy 1.0\n",
      "Epoch 1 Loss 0.45470914244651794 Accuracy 0.800000011920929\n",
      "Epoch 1 Loss 0.4666503369808197 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4584018588066101 Accuracy 1.0\n",
      "Epoch 1 Loss 0.509629487991333 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.4587419927120209 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.4689716696739197 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4416782259941101 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4485481381416321 Accuracy 1.0\n",
      "Epoch 1 Loss 0.45113030076026917 Accuracy 1.0\n",
      "Epoch 1 Loss 0.41370901465415955 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3876681923866272 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3656371235847473 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4756713807582855 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4775789678096771 Accuracy 0.800000011920929\n",
      "Epoch 1 Loss 0.37954357266426086 Accuracy 1.0\n",
      "Epoch 1 Loss 0.39117899537086487 Accuracy 1.0\n",
      "Epoch 1 Loss 0.43719276785850525 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4064442217350006 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5171071887016296 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.44001975655555725 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.42103320360183716 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.4134337306022644 Accuracy 1.0\n",
      "Epoch 1 Loss 0.46231794357299805 Accuracy 0.800000011920929\n",
      "Epoch 1 Loss 0.4546416699886322 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.44663065671920776 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.4551759362220764 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.4516697824001312 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4044745862483978 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3857855200767517 Accuracy 1.0\n",
      "Epoch 1 Loss 0.45791640877723694 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4209059178829193 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.3966948390007019 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4640510082244873 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.39630767703056335 Accuracy 1.0\n",
      "Epoch 1 Loss 0.40542173385620117 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4506511092185974 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.3897447884082794 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4241538941860199 Accuracy 1.0\n",
      "Epoch 1 Loss 0.362582802772522 Accuracy 1.0\n",
      "Epoch 1 Loss 0.34761667251586914 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4136144518852234 Accuracy 1.0\n",
      "Epoch 1 Loss 0.46471714973449707 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.4509381353855133 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.4289456903934479 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4968086779117584 Accuracy 0.800000011920929\n",
      "Epoch 1 Loss 0.5034412741661072 Accuracy 0.800000011920929\n",
      "Epoch 1 Loss 0.4520919919013977 Accuracy 0.800000011920929\n",
      "Epoch 1 Loss 0.4033277928829193 Accuracy 1.0\n",
      "Epoch 1 Loss 0.40870770812034607 Accuracy 1.0\n",
      "Epoch 1 Loss 0.407367080450058 Accuracy 1.0\n",
      "Epoch 1 Loss 0.37844815850257874 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3835957944393158 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.4471386969089508 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4161621034145355 Accuracy 1.0\n",
      "Epoch 1 Loss 0.44024935364723206 Accuracy 0.800000011920929\n",
      "Epoch 1 Loss 0.4023003578186035 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3834958076477051 Accuracy 1.0\n",
      "Epoch 1 Loss 0.46759963035583496 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.44844841957092285 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.36236387491226196 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3640587329864502 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4038253724575043 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3850978910923004 Accuracy 1.0\n",
      "Epoch 1 Loss 0.34297436475753784 Accuracy 1.0\n",
      "Epoch 1 Loss 0.45226016640663147 Accuracy 1.0\n",
      "Epoch 1 Loss 0.38980066776275635 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4439408779144287 Accuracy 1.0\n",
      "Epoch 1 Loss 0.40011921525001526 Accuracy 1.0\n",
      "Epoch 1 Loss 0.45053061842918396 Accuracy 1.0\n",
      "Epoch 1 Loss 0.33370429277420044 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4660155177116394 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.41116219758987427 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4238011837005615 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4608474373817444 Accuracy 1.0\n",
      "Epoch 1 Loss 0.40838566422462463 Accuracy 1.0\n",
      "Epoch 1 Loss 0.44213443994522095 Accuracy 1.0\n",
      "Epoch 1 Loss 0.34401899576187134 Accuracy 1.0\n",
      "Epoch 1 Loss 0.37972015142440796 Accuracy 1.0\n",
      "Epoch 1 Loss 0.47359466552734375 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.4455109238624573 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.36875978112220764 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4159364104270935 Accuracy 1.0\n",
      "Epoch 1 Loss 0.41429805755615234 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3929400146007538 Accuracy 1.0\n",
      "Epoch 1 Loss 0.39467206597328186 Accuracy 1.0\n",
      "Epoch 1 Loss 0.38047531247138977 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3822317123413086 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3304576873779297 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4060652256011963 Accuracy 1.0\n",
      "Epoch 1 Loss 0.37512150406837463 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4756454527378082 Accuracy 1.0\n",
      "Epoch 1 Loss 0.47284379601478577 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.3781721591949463 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3411220610141754 Accuracy 1.0\n",
      "Epoch 1 Loss 0.369327187538147 Accuracy 1.0\n",
      "Epoch 1 Loss 0.40978294610977173 Accuracy 1.0\n",
      "Epoch 1 Loss 0.44817954301834106 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.4572840631008148 Accuracy 1.0\n",
      "Epoch 1 Loss 0.47075000405311584 Accuracy 1.0\n",
      "Epoch 1 Loss 0.43806400895118713 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4014333188533783 Accuracy 1.0\n",
      "Epoch 1 Loss 0.39123672246932983 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3755874037742615 Accuracy 1.0\n",
      "Epoch 1 Loss 0.42751774191856384 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.3829275965690613 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4299490451812744 Accuracy 1.0\n",
      "Epoch 1 Loss 0.42581114172935486 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3805766701698303 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.45825743675231934 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3678897023200989 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3992384076118469 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3891957402229309 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3360857367515564 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4256075322628021 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3892812430858612 Accuracy 1.0\n",
      "Epoch 1 Loss 0.44548988342285156 Accuracy 1.0\n",
      "Epoch 1 Loss 0.39798396825790405 Accuracy 1.0\n",
      "Epoch 1 Loss 0.42205891013145447 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3675404191017151 Accuracy 1.0\n",
      "Epoch 1 Loss 0.43406620621681213 Accuracy 1.0\n",
      "Epoch 1 Loss 0.39088332653045654 Accuracy 1.0\n",
      "Epoch 1 Loss 0.52365642786026 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3670150637626648 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.36702829599380493 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.4639769494533539 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.38073474168777466 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3762185871601105 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3663623034954071 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3365345299243927 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3402911126613617 Accuracy 1.0\n",
      "Epoch 1 Loss 0.426110178232193 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3856984078884125 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3649466633796692 Accuracy 1.0\n",
      "Epoch 1 Loss 0.42751017212867737 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4067819118499756 Accuracy 1.0\n",
      "Epoch 1 Loss 0.34675270318984985 Accuracy 1.0\n",
      "Epoch 1 Loss 0.39369112253189087 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4302419126033783 Accuracy 1.0\n",
      "Epoch 1 Loss 0.35907167196273804 Accuracy 1.0\n",
      "Epoch 1 Loss 0.49545931816101074 Accuracy 1.0\n",
      "Epoch 1 Loss 0.40260863304138184 Accuracy 1.0\n",
      "Epoch 1 Loss 0.38195276260375977 Accuracy 1.0\n",
      "Epoch 1 Loss 0.43508777022361755 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3659757375717163 Accuracy 1.0\n",
      "Epoch 1 Loss 0.42232775688171387 Accuracy 1.0\n",
      "Epoch 1 Loss 0.38747844099998474 Accuracy 1.0\n",
      "Epoch 1 Loss 0.38425466418266296 Accuracy 1.0\n",
      "Epoch 1 Loss 0.31817731261253357 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3212127983570099 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3745984435081482 Accuracy 1.0\n",
      "Epoch 1 Loss 0.29946839809417725 Accuracy 1.0\n",
      "Epoch 1 Loss 0.44955140352249146 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3534007668495178 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4680458605289459 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.4606625437736511 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.41524916887283325 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3382904529571533 Accuracy 1.0\n",
      "Epoch 1 Loss 0.41248536109924316 Accuracy 1.0\n",
      "Epoch 1 Loss 0.37899407744407654 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3837588429450989 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4371365010738373 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.44130396842956543 Accuracy 1.0\n",
      "Epoch 1 Loss 0.38707754015922546 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3439484238624573 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3776429295539856 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.43104106187820435 Accuracy 1.0\n",
      "Epoch 1 Loss 0.407637357711792 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4274454116821289 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3841080665588379 Accuracy 1.0\n",
      "Epoch 1 Loss 0.45556291937828064 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4975808262825012 Accuracy 0.800000011920929\n",
      "Epoch 1 Loss 0.3605095148086548 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5099170804023743 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.4402860999107361 Accuracy 1.0\n",
      "Epoch 1 Loss 0.36828145384788513 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3346477448940277 Accuracy 1.0\n",
      "Epoch 1 Loss 0.5034399032592773 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.35797104239463806 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.3485602140426636 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3727574348449707 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3559252917766571 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3913085162639618 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3104289472103119 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4506295621395111 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.40826016664505005 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3304937779903412 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3745955526828766 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4019511640071869 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4612334370613098 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4958624243736267 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3708672523498535 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3295883536338806 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3715839684009552 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4374168813228607 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.43398770689964294 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.4361180365085602 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4300857186317444 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4201451241970062 Accuracy 1.0\n",
      "Epoch 1 Loss 0.35414960980415344 Accuracy 1.0\n",
      "Epoch 1 Loss 0.40665674209594727 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4567932188510895 Accuracy 1.0\n",
      "Epoch 1 Loss 0.33228743076324463 Accuracy 1.0\n",
      "Epoch 1 Loss 0.38561776280403137 Accuracy 1.0\n",
      "Epoch 1 Loss 0.354136198759079 Accuracy 1.0\n",
      "Epoch 1 Loss 0.42513856291770935 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3553643524646759 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4038662910461426 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3380080461502075 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3913875222206116 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3990533947944641 Accuracy 1.0\n",
      "Epoch 1 Loss 0.49101248383522034 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.4063604474067688 Accuracy 1.0\n",
      "Epoch 1 Loss 0.36508649587631226 Accuracy 1.0\n",
      "Epoch 1 Loss 0.35618409514427185 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4330311417579651 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.38757413625717163 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4760587215423584 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3488813042640686 Accuracy 1.0\n",
      "Epoch 1 Loss 0.288360059261322 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4145701825618744 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.39106231927871704 Accuracy 1.0\n",
      "Epoch 1 Loss 0.32310599088668823 Accuracy 1.0\n",
      "Epoch 1 Loss 0.385969340801239 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3478628695011139 Accuracy 1.0\n",
      "Epoch 1 Loss 0.37626591324806213 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4137994349002838 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3704867362976074 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3820520341396332 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3538043797016144 Accuracy 1.0\n",
      "Epoch 1 Loss 0.48020076751708984 Accuracy 1.0\n",
      "Epoch 1 Loss 0.36689725518226624 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.38992995023727417 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3758792281150818 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.41904217004776 Accuracy 1.0\n",
      "Epoch 1 Loss 0.41939201951026917 Accuracy 1.0\n",
      "Epoch 1 Loss 0.35211363434791565 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3309432566165924 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.4421226978302002 Accuracy 1.0\n",
      "Epoch 1 Loss 0.43300938606262207 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.2801709771156311 Accuracy 1.0\n",
      "Epoch 1 Loss 0.41013970971107483 Accuracy 0.800000011920929\n",
      "Epoch 1 Loss 0.3572078049182892 Accuracy 1.0\n",
      "Epoch 1 Loss 0.40118545293807983 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.4407176375389099 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.29021039605140686 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4131080210208893 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3372778296470642 Accuracy 1.0\n",
      "Epoch 1 Loss 0.42045387625694275 Accuracy 1.0\n",
      "Epoch 1 Loss 0.30836039781570435 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3921181261539459 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4205119013786316 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.3196621239185333 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4085550904273987 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3561393916606903 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.3122285008430481 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3679254353046417 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3954915404319763 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.401545912027359 Accuracy 1.0\n",
      "Epoch 1 Loss 0.33817258477211 Accuracy 1.0\n",
      "Epoch 1 Loss 0.2881796360015869 Accuracy 1.0\n",
      "Epoch 1 Loss 0.32643893361091614 Accuracy 1.0\n",
      "Epoch 1 Loss 0.425692617893219 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3762279450893402 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.3034718930721283 Accuracy 1.0\n",
      "Epoch 1 Loss 0.30797359347343445 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3583213686943054 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.3531076908111572 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4635685086250305 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.33056363463401794 Accuracy 1.0\n",
      "Epoch 1 Loss 0.40489158034324646 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.4526046812534332 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.3760811686515808 Accuracy 1.0\n",
      "Epoch 1 Loss 0.41100063920021057 Accuracy 0.800000011920929\n",
      "Epoch 1 Loss 0.45634374022483826 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.387092649936676 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3589572012424469 Accuracy 1.0\n",
      "Epoch 1 Loss 0.26573997735977173 Accuracy 1.0\n",
      "Epoch 1 Loss 0.38985174894332886 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3931560814380646 Accuracy 1.0\n",
      "Epoch 1 Loss 0.39036256074905396 Accuracy 1.0\n",
      "Epoch 1 Loss 0.32717087864875793 Accuracy 1.0\n",
      "Epoch 1 Loss 0.34905800223350525 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3553338348865509 Accuracy 1.0\n",
      "Epoch 1 Loss 0.41547441482543945 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.3785781264305115 Accuracy 1.0\n",
      "Epoch 1 Loss 0.348281592130661 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3787395656108856 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4123782515525818 Accuracy 1.0\n",
      "Epoch 1 Loss 0.31197598576545715 Accuracy 1.0\n",
      "Epoch 1 Loss 0.2933451533317566 Accuracy 1.0\n",
      "Epoch 1 Loss 0.35514235496520996 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.3543683886528015 Accuracy 1.0\n",
      "Epoch 1 Loss 0.40421247482299805 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.3316059410572052 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3814830183982849 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4210658669471741 Accuracy 0.800000011920929\n",
      "Epoch 1 Loss 0.3565530776977539 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.45012378692626953 Accuracy 1.0\n",
      "Epoch 1 Loss 0.38077273964881897 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3234389126300812 Accuracy 1.0\n",
      "Epoch 1 Loss 0.37512630224227905 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3763553500175476 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3357437252998352 Accuracy 1.0\n",
      "Epoch 1 Loss 0.2941776514053345 Accuracy 1.0\n",
      "Epoch 1 Loss 0.36043640971183777 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4098551273345947 Accuracy 1.0\n",
      "Epoch 1 Loss 0.39627403020858765 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3731831908226013 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3242201805114746 Accuracy 1.0\n",
      "Epoch 1 Loss 0.2416631281375885 Accuracy 1.0\n",
      "Epoch 1 Loss 0.32047370076179504 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4900941252708435 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.32651740312576294 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4280703663825989 Accuracy 1.0\n",
      "Epoch 1 Loss 0.42816027998924255 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.35342103242874146 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3124067783355713 Accuracy 1.0\n",
      "Epoch 1 Loss 0.364962100982666 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3461757004261017 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3768377900123596 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.27799731492996216 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3039109706878662 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4168173372745514 Accuracy 1.0\n",
      "Epoch 1 Loss 0.363592267036438 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.360950231552124 Accuracy 1.0\n",
      "Epoch 1 Loss 0.35550743341445923 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.4417697489261627 Accuracy 1.0\n",
      "Epoch 1 Loss 0.37817925214767456 Accuracy 1.0\n",
      "Epoch 1 Loss 0.37789344787597656 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3032611906528473 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.3779771327972412 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.28529027104377747 Accuracy 1.0\n",
      "Epoch 1 Loss 0.45890846848487854 Accuracy 1.0\n",
      "Epoch 1 Loss 0.45300984382629395 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4922148287296295 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.38316476345062256 Accuracy 1.0\n",
      "Epoch 1 Loss 0.28072765469551086 Accuracy 1.0\n",
      "Epoch 1 Loss 0.43279901146888733 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.3087981939315796 Accuracy 1.0\n",
      "Epoch 1 Loss 0.40975475311279297 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.36008593440055847 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3652680814266205 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3636417090892792 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3195092976093292 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.4159334599971771 Accuracy 0.800000011920929\n",
      "Epoch 1 Loss 0.25033503770828247 Accuracy 1.0\n",
      "Epoch 1 Loss 0.44388943910598755 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.31466928124427795 Accuracy 1.0\n",
      "Epoch 1 Loss 0.41772645711898804 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.3590843975543976 Accuracy 1.0\n",
      "Epoch 1 Loss 0.36809873580932617 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3947228491306305 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4436745047569275 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3831605017185211 Accuracy 1.0\n",
      "Epoch 1 Loss 0.31422150135040283 Accuracy 1.0\n",
      "Epoch 1 Loss 0.40094178915023804 Accuracy 1.0\n",
      "Epoch 1 Loss 0.40660619735717773 Accuracy 1.0\n",
      "Epoch 1 Loss 0.36614447832107544 Accuracy 1.0\n",
      "Epoch 1 Loss 0.35697323083877563 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3629212975502014 Accuracy 0.800000011920929\n",
      "Epoch 1 Loss 0.28992122411727905 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4554707407951355 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.358448326587677 Accuracy 1.0\n",
      "Epoch 1 Loss 0.23940718173980713 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3728541433811188 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.3452262878417969 Accuracy 1.0\n",
      "Epoch 1 Loss 0.36963939666748047 Accuracy 1.0\n",
      "Epoch 1 Loss 0.29787150025367737 Accuracy 1.0\n",
      "Epoch 1 Loss 0.4187614321708679 Accuracy 1.0\n",
      "Epoch 1 Loss 0.2840983271598816 Accuracy 1.0\n",
      "Epoch 1 Loss 0.3761723041534424 Accuracy 0.8999999761581421\n",
      "Epoch 1 Loss 0.3205297887325287 Accuracy 1.0\n",
      "Epoch 2 Loss 0.31161943078041077 Accuracy 1.0\n",
      "Epoch 2 Loss 0.31856799125671387 Accuracy 1.0\n",
      "Epoch 2 Loss 0.39039453864097595 Accuracy 1.0\n",
      "Epoch 2 Loss 0.4084360599517822 Accuracy 1.0\n",
      "Epoch 2 Loss 0.29904496669769287 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3660831153392792 Accuracy 1.0\n",
      "Epoch 2 Loss 0.34796229004859924 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3230559229850769 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3293842375278473 Accuracy 1.0\n",
      "Epoch 2 Loss 0.30918824672698975 Accuracy 1.0\n",
      "Epoch 2 Loss 0.33971455693244934 Accuracy 1.0\n",
      "Epoch 2 Loss 0.33147287368774414 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3805081844329834 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3050539791584015 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3225887417793274 Accuracy 1.0\n",
      "Epoch 2 Loss 0.30851420760154724 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2677595615386963 Accuracy 1.0\n",
      "Epoch 2 Loss 0.40561795234680176 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.2916368544101715 Accuracy 1.0\n",
      "Epoch 2 Loss 0.30980974435806274 Accuracy 1.0\n",
      "Epoch 2 Loss 0.31086474657058716 Accuracy 1.0\n",
      "Epoch 2 Loss 0.33204251527786255 Accuracy 1.0\n",
      "Epoch 2 Loss 0.4271346628665924 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3425297141075134 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3461240231990814 Accuracy 1.0\n",
      "Epoch 2 Loss 0.44005751609802246 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.3818313777446747 Accuracy 1.0\n",
      "Epoch 2 Loss 0.280085027217865 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2922978401184082 Accuracy 1.0\n",
      "Epoch 2 Loss 0.32842379808425903 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3096558451652527 Accuracy 1.0\n",
      "Epoch 2 Loss 0.40784281492233276 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3197142481803894 Accuracy 1.0\n",
      "Epoch 2 Loss 0.366011381149292 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3649103045463562 Accuracy 1.0\n",
      "Epoch 2 Loss 0.4843709468841553 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.30834048986434937 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3207967281341553 Accuracy 1.0\n",
      "Epoch 2 Loss 0.38472431898117065 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.26305538415908813 Accuracy 1.0\n",
      "Epoch 2 Loss 0.37559112906455994 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.3287511467933655 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3592955470085144 Accuracy 1.0\n",
      "Epoch 2 Loss 0.35765552520751953 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3488365113735199 Accuracy 1.0\n",
      "Epoch 2 Loss 0.23895207047462463 Accuracy 1.0\n",
      "Epoch 2 Loss 0.33559951186180115 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.31822893023490906 Accuracy 1.0\n",
      "Epoch 2 Loss 0.281598836183548 Accuracy 1.0\n",
      "Epoch 2 Loss 0.41456708312034607 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3918379843235016 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.36862900853157043 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2974289357662201 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3882589340209961 Accuracy 1.0\n",
      "Epoch 2 Loss 0.36056238412857056 Accuracy 1.0\n",
      "Epoch 2 Loss 0.38774099946022034 Accuracy 1.0\n",
      "Epoch 2 Loss 0.4870092272758484 Accuracy 0.800000011920929\n",
      "Epoch 2 Loss 0.4015483856201172 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.3418240547180176 Accuracy 1.0\n",
      "Epoch 2 Loss 0.35333770513534546 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3132299780845642 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3517286777496338 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2745113670825958 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3422752916812897 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3649590313434601 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2955978810787201 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3903641104698181 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3342057764530182 Accuracy 1.0\n",
      "Epoch 2 Loss 0.30073028802871704 Accuracy 1.0\n",
      "Epoch 2 Loss 0.27456265687942505 Accuracy 1.0\n",
      "Epoch 2 Loss 0.336525559425354 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3511687219142914 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2631497383117676 Accuracy 1.0\n",
      "Epoch 2 Loss 0.27917343378067017 Accuracy 1.0\n",
      "Epoch 2 Loss 0.27361366152763367 Accuracy 1.0\n",
      "Epoch 2 Loss 0.32441774010658264 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2785898745059967 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3790813088417053 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3843042254447937 Accuracy 1.0\n",
      "Epoch 2 Loss 0.38326898217201233 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3387679159641266 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2516627311706543 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3168334364891052 Accuracy 1.0\n",
      "Epoch 2 Loss 0.39215829968452454 Accuracy 1.0\n",
      "Epoch 2 Loss 0.40972623229026794 Accuracy 1.0\n",
      "Epoch 2 Loss 0.255240261554718 Accuracy 1.0\n",
      "Epoch 2 Loss 0.32603567838668823 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3253720998764038 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3967241942882538 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3739067018032074 Accuracy 1.0\n",
      "Epoch 2 Loss 0.38648343086242676 Accuracy 1.0\n",
      "Epoch 2 Loss 0.38662320375442505 Accuracy 1.0\n",
      "Epoch 2 Loss 0.24953365325927734 Accuracy 1.0\n",
      "Epoch 2 Loss 0.35380709171295166 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.3106004595756531 Accuracy 1.0\n",
      "Epoch 2 Loss 0.32113581895828247 Accuracy 1.0\n",
      "Epoch 2 Loss 0.4057644009590149 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3230074942111969 Accuracy 1.0\n",
      "Epoch 2 Loss 0.33888325095176697 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3601868152618408 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.3181372284889221 Accuracy 1.0\n",
      "Epoch 2 Loss 0.34056714177131653 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.3625198006629944 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3542427122592926 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3062942326068878 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3898138105869293 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3442000448703766 Accuracy 1.0\n",
      "Epoch 2 Loss 0.35889655351638794 Accuracy 1.0\n",
      "Epoch 2 Loss 0.35991451144218445 Accuracy 1.0\n",
      "Epoch 2 Loss 0.38307756185531616 Accuracy 1.0\n",
      "Epoch 2 Loss 0.35401564836502075 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3083963096141815 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2819232940673828 Accuracy 1.0\n",
      "Epoch 2 Loss 0.279024600982666 Accuracy 1.0\n",
      "Epoch 2 Loss 0.24954824149608612 Accuracy 1.0\n",
      "Epoch 2 Loss 0.29768601059913635 Accuracy 1.0\n",
      "Epoch 2 Loss 0.34059643745422363 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3710850179195404 Accuracy 1.0\n",
      "Epoch 2 Loss 0.30251726508140564 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2983667552471161 Accuracy 1.0\n",
      "Epoch 2 Loss 0.39584022760391235 Accuracy 1.0\n",
      "Epoch 2 Loss 0.37517401576042175 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3648045063018799 Accuracy 1.0\n",
      "Epoch 2 Loss 0.33426183462142944 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.35647639632225037 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2665523886680603 Accuracy 1.0\n",
      "Epoch 2 Loss 0.419221967458725 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2821936309337616 Accuracy 1.0\n",
      "Epoch 2 Loss 0.31187695264816284 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2842291593551636 Accuracy 1.0\n",
      "Epoch 2 Loss 0.31467047333717346 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2597450315952301 Accuracy 1.0\n",
      "Epoch 2 Loss 0.37453681230545044 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3402746617794037 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3805609941482544 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3832504153251648 Accuracy 1.0\n",
      "Epoch 2 Loss 0.22345688939094543 Accuracy 1.0\n",
      "Epoch 2 Loss 0.32884588837623596 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.31838124990463257 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3800327181816101 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3324531614780426 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2911957800388336 Accuracy 1.0\n",
      "Epoch 2 Loss 0.30491796135902405 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3361187279224396 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.2626063823699951 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3210933804512024 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3527209758758545 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.33771267533302307 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3175557851791382 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3727482557296753 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3081691265106201 Accuracy 1.0\n",
      "Epoch 2 Loss 0.35707783699035645 Accuracy 1.0\n",
      "Epoch 2 Loss 0.32432642579078674 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3676976263523102 Accuracy 1.0\n",
      "Epoch 2 Loss 0.25977107882499695 Accuracy 1.0\n",
      "Epoch 2 Loss 0.27899837493896484 Accuracy 1.0\n",
      "Epoch 2 Loss 0.29920345544815063 Accuracy 1.0\n",
      "Epoch 2 Loss 0.31048741936683655 Accuracy 1.0\n",
      "Epoch 2 Loss 0.19984090328216553 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3191147446632385 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3654625713825226 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3307429850101471 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2248782217502594 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3215315043926239 Accuracy 1.0\n",
      "Epoch 2 Loss 0.47756272554397583 Accuracy 1.0\n",
      "Epoch 2 Loss 0.32564374804496765 Accuracy 1.0\n",
      "Epoch 2 Loss 0.23484809696674347 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3243632912635803 Accuracy 1.0\n",
      "Epoch 2 Loss 0.366468608379364 Accuracy 1.0\n",
      "Epoch 2 Loss 0.335114061832428 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3394606113433838 Accuracy 1.0\n",
      "Epoch 2 Loss 0.33201321959495544 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3284890353679657 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3859875202178955 Accuracy 1.0\n",
      "Epoch 2 Loss 0.39943259954452515 Accuracy 1.0\n",
      "Epoch 2 Loss 0.30174994468688965 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3404357135295868 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2892536520957947 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3029846251010895 Accuracy 1.0\n",
      "Epoch 2 Loss 0.29603707790374756 Accuracy 1.0\n",
      "Epoch 2 Loss 0.27897632122039795 Accuracy 1.0\n",
      "Epoch 2 Loss 0.21030130982398987 Accuracy 1.0\n",
      "Epoch 2 Loss 0.22156286239624023 Accuracy 1.0\n",
      "Epoch 2 Loss 0.37254518270492554 Accuracy 1.0\n",
      "Epoch 2 Loss 0.33750542998313904 Accuracy 1.0\n",
      "Epoch 2 Loss 0.29919910430908203 Accuracy 1.0\n",
      "Epoch 2 Loss 0.24757227301597595 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2698691487312317 Accuracy 1.0\n",
      "Epoch 2 Loss 0.39807212352752686 Accuracy 1.0\n",
      "Epoch 2 Loss 0.18567955493927002 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3800855278968811 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.3836528956890106 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2782014310359955 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2935926020145416 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3431006073951721 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3065565228462219 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2919548451900482 Accuracy 1.0\n",
      "Epoch 2 Loss 0.34603139758110046 Accuracy 1.0\n",
      "Epoch 2 Loss 0.31386932730674744 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2794545292854309 Accuracy 1.0\n",
      "Epoch 2 Loss 0.24421660602092743 Accuracy 1.0\n",
      "Epoch 2 Loss 0.4069722592830658 Accuracy 1.0\n",
      "Epoch 2 Loss 0.39066532254219055 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3170687258243561 Accuracy 1.0\n",
      "Epoch 2 Loss 0.32922202348709106 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2806665301322937 Accuracy 1.0\n",
      "Epoch 2 Loss 0.34448665380477905 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.31546738743782043 Accuracy 1.0\n",
      "Epoch 2 Loss 0.24588873982429504 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3304698169231415 Accuracy 1.0\n",
      "Epoch 2 Loss 0.34022825956344604 Accuracy 1.0\n",
      "Epoch 2 Loss 0.388147234916687 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2982760965824127 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2030990570783615 Accuracy 1.0\n",
      "Epoch 2 Loss 0.28512775897979736 Accuracy 1.0\n",
      "Epoch 2 Loss 0.19633856415748596 Accuracy 1.0\n",
      "Epoch 2 Loss 0.29701894521713257 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2888256311416626 Accuracy 1.0\n",
      "Epoch 2 Loss 0.26033931970596313 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3433782458305359 Accuracy 1.0\n",
      "Epoch 2 Loss 0.28415513038635254 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3048704266548157 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2427065670490265 Accuracy 1.0\n",
      "Epoch 2 Loss 0.31704992055892944 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2981255352497101 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2726050019264221 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2589567303657532 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.3790937066078186 Accuracy 1.0\n",
      "Epoch 2 Loss 0.22822745144367218 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3270312547683716 Accuracy 1.0\n",
      "Epoch 2 Loss 0.26481741666793823 Accuracy 1.0\n",
      "Epoch 2 Loss 0.42869073152542114 Accuracy 1.0\n",
      "Epoch 2 Loss 0.28741657733917236 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3654065430164337 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2824453115463257 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2119012176990509 Accuracy 1.0\n",
      "Epoch 2 Loss 0.42842039465904236 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.2376771867275238 Accuracy 1.0\n",
      "Epoch 2 Loss 0.30276399850845337 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3404741883277893 Accuracy 1.0\n",
      "Epoch 2 Loss 0.35411393642425537 Accuracy 1.0\n",
      "Epoch 2 Loss 0.1932830959558487 Accuracy 1.0\n",
      "Epoch 2 Loss 0.36436325311660767 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.27476271986961365 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3180503845214844 Accuracy 1.0\n",
      "Epoch 2 Loss 0.27621230483055115 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3334828019142151 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3248932957649231 Accuracy 1.0\n",
      "Epoch 2 Loss 0.20666047930717468 Accuracy 1.0\n",
      "Epoch 2 Loss 0.29512953758239746 Accuracy 1.0\n",
      "Epoch 2 Loss 0.28332167863845825 Accuracy 1.0\n",
      "Epoch 2 Loss 0.34612077474594116 Accuracy 1.0\n",
      "Epoch 2 Loss 0.15545503795146942 Accuracy 1.0\n",
      "Epoch 2 Loss 0.35346728563308716 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2330060750246048 Accuracy 1.0\n",
      "Epoch 2 Loss 0.30373749136924744 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2949557900428772 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2950799763202667 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2536543011665344 Accuracy 1.0\n",
      "Epoch 2 Loss 0.27887198328971863 Accuracy 1.0\n",
      "Epoch 2 Loss 0.28624990582466125 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3079497218132019 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2918596863746643 Accuracy 1.0\n",
      "Epoch 2 Loss 0.29627037048339844 Accuracy 1.0\n",
      "Epoch 2 Loss 0.23005792498588562 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3107759952545166 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.2731245160102844 Accuracy 1.0\n",
      "Epoch 2 Loss 0.22382786870002747 Accuracy 1.0\n",
      "Epoch 2 Loss 0.40095895528793335 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.27813661098480225 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2929224669933319 Accuracy 1.0\n",
      "Epoch 2 Loss 0.27637162804603577 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2723488211631775 Accuracy 1.0\n",
      "Epoch 2 Loss 0.31498363614082336 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3699795603752136 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2560543417930603 Accuracy 1.0\n",
      "Epoch 2 Loss 0.32088518142700195 Accuracy 1.0\n",
      "Epoch 2 Loss 0.25470593571662903 Accuracy 1.0\n",
      "Epoch 2 Loss 0.271740585565567 Accuracy 1.0\n",
      "Epoch 2 Loss 0.4032295346260071 Accuracy 1.0\n",
      "Epoch 2 Loss 0.36156371235847473 Accuracy 1.0\n",
      "Epoch 2 Loss 0.24032290279865265 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2819742262363434 Accuracy 1.0\n",
      "Epoch 2 Loss 0.4135381579399109 Accuracy 1.0\n",
      "Epoch 2 Loss 0.30321431159973145 Accuracy 1.0\n",
      "Epoch 2 Loss 0.288974404335022 Accuracy 1.0\n",
      "Epoch 2 Loss 0.32034680247306824 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3177231252193451 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2763427793979645 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2916198670864105 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.2932310998439789 Accuracy 1.0\n",
      "Epoch 2 Loss 0.22191528975963593 Accuracy 1.0\n",
      "Epoch 2 Loss 0.257864773273468 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3052227795124054 Accuracy 1.0\n",
      "Epoch 2 Loss 0.20683391392230988 Accuracy 1.0\n",
      "Epoch 2 Loss 0.19806423783302307 Accuracy 1.0\n",
      "Epoch 2 Loss 0.30039894580841064 Accuracy 1.0\n",
      "Epoch 2 Loss 0.39065587520599365 Accuracy 1.0\n",
      "Epoch 2 Loss 0.26596781611442566 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3873257040977478 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2582303285598755 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3727056086063385 Accuracy 1.0\n",
      "Epoch 2 Loss 0.257783979177475 Accuracy 1.0\n",
      "Epoch 2 Loss 0.33396920561790466 Accuracy 1.0\n",
      "Epoch 2 Loss 0.27036935091018677 Accuracy 1.0\n",
      "Epoch 2 Loss 0.288440078496933 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3170667290687561 Accuracy 1.0\n",
      "Epoch 2 Loss 0.28184613585472107 Accuracy 1.0\n",
      "Epoch 2 Loss 0.33943790197372437 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.2996375858783722 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2979782223701477 Accuracy 1.0\n",
      "Epoch 2 Loss 0.32759779691696167 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3436938524246216 Accuracy 1.0\n",
      "Epoch 2 Loss 0.32174426317214966 Accuracy 1.0\n",
      "Epoch 2 Loss 0.22566190361976624 Accuracy 1.0\n",
      "Epoch 2 Loss 0.24939551949501038 Accuracy 1.0\n",
      "Epoch 2 Loss 0.21943160891532898 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2635647654533386 Accuracy 1.0\n",
      "Epoch 2 Loss 0.29637181758880615 Accuracy 1.0\n",
      "Epoch 2 Loss 0.27725061774253845 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2098693549633026 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2265183925628662 Accuracy 1.0\n",
      "Epoch 2 Loss 0.20366112887859344 Accuracy 1.0\n",
      "Epoch 2 Loss 0.22858436405658722 Accuracy 1.0\n",
      "Epoch 2 Loss 0.23653502762317657 Accuracy 1.0\n",
      "Epoch 2 Loss 0.23792824149131775 Accuracy 1.0\n",
      "Epoch 2 Loss 0.22863855957984924 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3146406412124634 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3467211127281189 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2648078501224518 Accuracy 1.0\n",
      "Epoch 2 Loss 0.33790361881256104 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3359631896018982 Accuracy 1.0\n",
      "Epoch 2 Loss 0.31127244234085083 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2949633002281189 Accuracy 1.0\n",
      "Epoch 2 Loss 0.22897903621196747 Accuracy 1.0\n",
      "Epoch 2 Loss 0.34335198998451233 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3260095417499542 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3130055069923401 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3112861216068268 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2386006861925125 Accuracy 1.0\n",
      "Epoch 2 Loss 0.34445253014564514 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.2367336004972458 Accuracy 1.0\n",
      "Epoch 2 Loss 0.25441470742225647 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.37556126713752747 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3267541527748108 Accuracy 1.0\n",
      "Epoch 2 Loss 0.30399495363235474 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2927232086658478 Accuracy 1.0\n",
      "Epoch 2 Loss 0.36423030495643616 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2591169476509094 Accuracy 1.0\n",
      "Epoch 2 Loss 0.24026384949684143 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2600672245025635 Accuracy 1.0\n",
      "Epoch 2 Loss 0.25530537962913513 Accuracy 1.0\n",
      "Epoch 2 Loss 0.22451944649219513 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.3127219080924988 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3031719923019409 Accuracy 1.0\n",
      "Epoch 2 Loss 0.32956379652023315 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2686147391796112 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.18673864006996155 Accuracy 1.0\n",
      "Epoch 2 Loss 0.24729354679584503 Accuracy 1.0\n",
      "Epoch 2 Loss 0.35843148827552795 Accuracy 1.0\n",
      "Epoch 2 Loss 0.18779385089874268 Accuracy 1.0\n",
      "Epoch 2 Loss 0.23994989693164825 Accuracy 1.0\n",
      "Epoch 2 Loss 0.27340543270111084 Accuracy 1.0\n",
      "Epoch 2 Loss 0.31016701459884644 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.2440081387758255 Accuracy 1.0\n",
      "Epoch 2 Loss 0.35532286763191223 Accuracy 1.0\n",
      "Epoch 2 Loss 0.24027256667613983 Accuracy 1.0\n",
      "Epoch 2 Loss 0.26087552309036255 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2878822088241577 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3554326593875885 Accuracy 1.0\n",
      "Epoch 2 Loss 0.42598018050193787 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2569968104362488 Accuracy 1.0\n",
      "Epoch 2 Loss 0.30731257796287537 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3767464756965637 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.25232642889022827 Accuracy 1.0\n",
      "Epoch 2 Loss 0.30893510580062866 Accuracy 1.0\n",
      "Epoch 2 Loss 0.24807238578796387 Accuracy 1.0\n",
      "Epoch 2 Loss 0.43027734756469727 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2637813985347748 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2176245152950287 Accuracy 1.0\n",
      "Epoch 2 Loss 0.31620094180107117 Accuracy 1.0\n",
      "Epoch 2 Loss 0.27474111318588257 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2555575668811798 Accuracy 1.0\n",
      "Epoch 2 Loss 0.30427902936935425 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3646014332771301 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3199549615383148 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2994064688682556 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2314574420452118 Accuracy 1.0\n",
      "Epoch 2 Loss 0.19740097224712372 Accuracy 1.0\n",
      "Epoch 2 Loss 0.36995425820350647 Accuracy 0.800000011920929\n",
      "Epoch 2 Loss 0.3129325211048126 Accuracy 1.0\n",
      "Epoch 2 Loss 0.27806606888771057 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3200077414512634 Accuracy 1.0\n",
      "Epoch 2 Loss 0.23254244029521942 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2259519100189209 Accuracy 1.0\n",
      "Epoch 2 Loss 0.32245081663131714 Accuracy 1.0\n",
      "Epoch 2 Loss 0.20799168944358826 Accuracy 1.0\n",
      "Epoch 2 Loss 0.34950000047683716 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.2832562029361725 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2859610617160797 Accuracy 1.0\n",
      "Epoch 2 Loss 0.33750391006469727 Accuracy 1.0\n",
      "Epoch 2 Loss 0.20658822357654572 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.2852632403373718 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2806595265865326 Accuracy 1.0\n",
      "Epoch 2 Loss 0.17312118411064148 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3488982915878296 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2524154484272003 Accuracy 1.0\n",
      "Epoch 2 Loss 0.28900057077407837 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.28974753618240356 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2248513251543045 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.20229151844978333 Accuracy 1.0\n",
      "Epoch 2 Loss 0.24134135246276855 Accuracy 1.0\n",
      "Epoch 2 Loss 0.32895174622535706 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3891351819038391 Accuracy 1.0\n",
      "Epoch 2 Loss 0.43822044134140015 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.28587719798088074 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3905563950538635 Accuracy 0.800000011920929\n",
      "Epoch 2 Loss 0.19297155737876892 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3521835505962372 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2921310067176819 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3439347445964813 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.18552586436271667 Accuracy 1.0\n",
      "Epoch 2 Loss 0.23526152968406677 Accuracy 1.0\n",
      "Epoch 2 Loss 0.4212220311164856 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.3782362639904022 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.2344728708267212 Accuracy 1.0\n",
      "Epoch 2 Loss 0.35321152210235596 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3737551271915436 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3122778534889221 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2684433162212372 Accuracy 1.0\n",
      "Epoch 2 Loss 0.21551665663719177 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3596819043159485 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3270213305950165 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.31018394231796265 Accuracy 1.0\n",
      "Epoch 2 Loss 0.31269222497940063 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3724221885204315 Accuracy 1.0\n",
      "Epoch 2 Loss 0.1998749077320099 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2292187660932541 Accuracy 1.0\n",
      "Epoch 2 Loss 0.33663424849510193 Accuracy 1.0\n",
      "Epoch 2 Loss 0.23983390629291534 Accuracy 1.0\n",
      "Epoch 2 Loss 0.35486549139022827 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.327718049287796 Accuracy 1.0\n",
      "Epoch 2 Loss 0.38416796922683716 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2598262131214142 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2714014947414398 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3039247393608093 Accuracy 1.0\n",
      "Epoch 2 Loss 0.338967502117157 Accuracy 1.0\n",
      "Epoch 2 Loss 0.19133207201957703 Accuracy 1.0\n",
      "Epoch 2 Loss 0.29333212971687317 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3284943401813507 Accuracy 1.0\n",
      "Epoch 2 Loss 0.25754889845848083 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2961287200450897 Accuracy 1.0\n",
      "Epoch 2 Loss 0.28365039825439453 Accuracy 1.0\n",
      "Epoch 2 Loss 0.24485905468463898 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3222513794898987 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2441284954547882 Accuracy 1.0\n",
      "Epoch 2 Loss 0.18122293055057526 Accuracy 1.0\n",
      "Epoch 2 Loss 0.32323700189590454 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2259916514158249 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2851327359676361 Accuracy 1.0\n",
      "Epoch 2 Loss 0.25093716382980347 Accuracy 1.0\n",
      "Epoch 2 Loss 0.19004730880260468 Accuracy 1.0\n",
      "Epoch 2 Loss 0.23554925620555878 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2915775179862976 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2959342896938324 Accuracy 1.0\n",
      "Epoch 2 Loss 0.38232165575027466 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2761038541793823 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.2950599193572998 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3305925726890564 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.2527213394641876 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2788866460323334 Accuracy 1.0\n",
      "Epoch 2 Loss 0.25227177143096924 Accuracy 1.0\n",
      "Epoch 2 Loss 0.31772446632385254 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3336314857006073 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2914697527885437 Accuracy 1.0\n",
      "Epoch 2 Loss 0.1661568135023117 Accuracy 1.0\n",
      "Epoch 2 Loss 0.20699910819530487 Accuracy 1.0\n",
      "Epoch 2 Loss 0.29121971130371094 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2539573013782501 Accuracy 1.0\n",
      "Epoch 2 Loss 0.22680921852588654 Accuracy 1.0\n",
      "Epoch 2 Loss 0.25682300329208374 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3455083966255188 Accuracy 1.0\n",
      "Epoch 2 Loss 0.24614953994750977 Accuracy 1.0\n",
      "Epoch 2 Loss 0.1693873554468155 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3012212812900543 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3349919617176056 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3022575080394745 Accuracy 1.0\n",
      "Epoch 2 Loss 0.18198975920677185 Accuracy 1.0\n",
      "Epoch 2 Loss 0.25225383043289185 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3125777840614319 Accuracy 1.0\n",
      "Epoch 2 Loss 0.20603108406066895 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2733711302280426 Accuracy 1.0\n",
      "Epoch 2 Loss 0.35788312554359436 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2530152201652527 Accuracy 1.0\n",
      "Epoch 2 Loss 0.21938177943229675 Accuracy 1.0\n",
      "Epoch 2 Loss 0.31405264139175415 Accuracy 1.0\n",
      "Epoch 2 Loss 0.36390218138694763 Accuracy 1.0\n",
      "Epoch 2 Loss 0.24601569771766663 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2591354250907898 Accuracy 1.0\n",
      "Epoch 2 Loss 0.21119233965873718 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2815266251564026 Accuracy 1.0\n",
      "Epoch 2 Loss 0.31208866834640503 Accuracy 1.0\n",
      "Epoch 2 Loss 0.1989879310131073 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2019346058368683 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2316657304763794 Accuracy 1.0\n",
      "Epoch 2 Loss 0.34321507811546326 Accuracy 1.0\n",
      "Epoch 2 Loss 0.21970410645008087 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3596465587615967 Accuracy 1.0\n",
      "Epoch 2 Loss 0.32050129771232605 Accuracy 1.0\n",
      "Epoch 2 Loss 0.22922316193580627 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3100150227546692 Accuracy 1.0\n",
      "Epoch 2 Loss 0.25810375809669495 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2828076183795929 Accuracy 1.0\n",
      "Epoch 2 Loss 0.19930770993232727 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3328857123851776 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2091149389743805 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3234913945198059 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2125992476940155 Accuracy 1.0\n",
      "Epoch 2 Loss 0.25550025701522827 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3856290876865387 Accuracy 1.0\n",
      "Epoch 2 Loss 0.18086126446723938 Accuracy 1.0\n",
      "Epoch 2 Loss 0.25758713483810425 Accuracy 1.0\n",
      "Epoch 2 Loss 0.295693576335907 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2706868052482605 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.3130638897418976 Accuracy 1.0\n",
      "Epoch 2 Loss 0.1793990433216095 Accuracy 1.0\n",
      "Epoch 2 Loss 0.14852195978164673 Accuracy 1.0\n",
      "Epoch 2 Loss 0.21315047144889832 Accuracy 1.0\n",
      "Epoch 2 Loss 0.28417912125587463 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3342698812484741 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.14069662988185883 Accuracy 1.0\n",
      "Epoch 2 Loss 0.27615293860435486 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2656762897968292 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3363472819328308 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2565630078315735 Accuracy 1.0\n",
      "Epoch 2 Loss 0.29426029324531555 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2746620774269104 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2852359712123871 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2883926033973694 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3685069978237152 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.24857155978679657 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3189312219619751 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.17584361135959625 Accuracy 1.0\n",
      "Epoch 2 Loss 0.238625168800354 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.256727397441864 Accuracy 1.0\n",
      "Epoch 2 Loss 0.34817594289779663 Accuracy 0.800000011920929\n",
      "Epoch 2 Loss 0.19347700476646423 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2120567113161087 Accuracy 1.0\n",
      "Epoch 2 Loss 0.21849782764911652 Accuracy 1.0\n",
      "Epoch 2 Loss 0.26019975543022156 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2620576024055481 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2751612067222595 Accuracy 1.0\n",
      "Epoch 2 Loss 0.30198100209236145 Accuracy 1.0\n",
      "Epoch 2 Loss 0.26621729135513306 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.2392573356628418 Accuracy 1.0\n",
      "Epoch 2 Loss 0.24597379565238953 Accuracy 1.0\n",
      "Epoch 2 Loss 0.23502802848815918 Accuracy 1.0\n",
      "Epoch 2 Loss 0.1440388262271881 Accuracy 1.0\n",
      "Epoch 2 Loss 0.25931501388549805 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3189888894557953 Accuracy 1.0\n",
      "Epoch 2 Loss 0.16358642280101776 Accuracy 1.0\n",
      "Epoch 2 Loss 0.19885939359664917 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2967202663421631 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2223229855298996 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2923854887485504 Accuracy 1.0\n",
      "Epoch 2 Loss 0.269358366727829 Accuracy 1.0\n",
      "Epoch 2 Loss 0.30158501863479614 Accuracy 1.0\n",
      "Epoch 2 Loss 0.22770710289478302 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2102258950471878 Accuracy 1.0\n",
      "Epoch 2 Loss 0.22148995101451874 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2836414873600006 Accuracy 0.800000011920929\n",
      "Epoch 2 Loss 0.1702984869480133 Accuracy 1.0\n",
      "Epoch 2 Loss 0.25239765644073486 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2774065434932709 Accuracy 1.0\n",
      "Epoch 2 Loss 0.32285547256469727 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.22357165813446045 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2543107867240906 Accuracy 1.0\n",
      "Epoch 2 Loss 0.29649072885513306 Accuracy 1.0\n",
      "Epoch 2 Loss 0.26330628991127014 Accuracy 1.0\n",
      "Epoch 2 Loss 0.20003607869148254 Accuracy 1.0\n",
      "Epoch 2 Loss 0.27278220653533936 Accuracy 1.0\n",
      "Epoch 2 Loss 0.34692782163619995 Accuracy 1.0\n",
      "Epoch 2 Loss 0.20208370685577393 Accuracy 1.0\n",
      "Epoch 2 Loss 0.17788247764110565 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3644648492336273 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.3199412524700165 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2385839968919754 Accuracy 1.0\n",
      "Epoch 2 Loss 0.26107051968574524 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2173364907503128 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.17770114541053772 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3336831033229828 Accuracy 1.0\n",
      "Epoch 2 Loss 0.28549593687057495 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.2932995855808258 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3607887327671051 Accuracy 1.0\n",
      "Epoch 2 Loss 0.17790068686008453 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2578809857368469 Accuracy 1.0\n",
      "Epoch 2 Loss 0.27579814195632935 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2830404043197632 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.28135085105895996 Accuracy 1.0\n",
      "Epoch 2 Loss 0.23928268253803253 Accuracy 1.0\n",
      "Epoch 2 Loss 0.13734224438667297 Accuracy 1.0\n",
      "Epoch 2 Loss 0.20816285908222198 Accuracy 1.0\n",
      "Epoch 2 Loss 0.22518369555473328 Accuracy 1.0\n",
      "Epoch 2 Loss 0.21021974086761475 Accuracy 1.0\n",
      "Epoch 2 Loss 0.26389262080192566 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3046882748603821 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.28753623366355896 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3286520540714264 Accuracy 1.0\n",
      "Epoch 2 Loss 0.21631579101085663 Accuracy 1.0\n",
      "Epoch 2 Loss 0.33318424224853516 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3832419216632843 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.1924915611743927 Accuracy 1.0\n",
      "Epoch 2 Loss 0.201467826962471 Accuracy 1.0\n",
      "Epoch 2 Loss 0.21490991115570068 Accuracy 1.0\n",
      "Epoch 2 Loss 0.15424953401088715 Accuracy 1.0\n",
      "Epoch 2 Loss 0.19212308526039124 Accuracy 1.0\n",
      "Epoch 2 Loss 0.1750805377960205 Accuracy 1.0\n",
      "Epoch 2 Loss 0.32074204087257385 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2251470386981964 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3244372010231018 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.22440819442272186 Accuracy 1.0\n",
      "Epoch 2 Loss 0.277576744556427 Accuracy 1.0\n",
      "Epoch 2 Loss 0.23110218346118927 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3201904892921448 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2343418300151825 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2465045154094696 Accuracy 1.0\n",
      "Epoch 2 Loss 0.22516004741191864 Accuracy 1.0\n",
      "Epoch 2 Loss 0.1480521261692047 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2866796851158142 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2969636917114258 Accuracy 1.0\n",
      "Epoch 2 Loss 0.26232990622520447 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3646130859851837 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.28651511669158936 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2907629609107971 Accuracy 1.0\n",
      "Epoch 2 Loss 0.25636643171310425 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2996126413345337 Accuracy 1.0\n",
      "Epoch 2 Loss 0.26606041193008423 Accuracy 1.0\n",
      "Epoch 2 Loss 0.22723135352134705 Accuracy 1.0\n",
      "Epoch 2 Loss 0.19146347045898438 Accuracy 1.0\n",
      "Epoch 2 Loss 0.16798359155654907 Accuracy 1.0\n",
      "Epoch 2 Loss 0.32552945613861084 Accuracy 1.0\n",
      "Epoch 2 Loss 0.31201449036598206 Accuracy 1.0\n",
      "Epoch 2 Loss 0.1797342747449875 Accuracy 1.0\n",
      "Epoch 2 Loss 0.1902209222316742 Accuracy 1.0\n",
      "Epoch 2 Loss 0.24309949576854706 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2072053700685501 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3896740972995758 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.27411025762557983 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.25804728269577026 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.22842583060264587 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3006369471549988 Accuracy 0.800000011920929\n",
      "Epoch 2 Loss 0.31413140892982483 Accuracy 1.0\n",
      "Epoch 2 Loss 0.27472275495529175 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.28398066759109497 Accuracy 1.0\n",
      "Epoch 2 Loss 0.26611191034317017 Accuracy 1.0\n",
      "Epoch 2 Loss 0.21635103225708008 Accuracy 1.0\n",
      "Epoch 2 Loss 0.199422687292099 Accuracy 1.0\n",
      "Epoch 2 Loss 0.29529353976249695 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2757263779640198 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.2164313793182373 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2887948155403137 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.21134114265441895 Accuracy 1.0\n",
      "Epoch 2 Loss 0.24622543156147003 Accuracy 1.0\n",
      "Epoch 2 Loss 0.27579087018966675 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2142469435930252 Accuracy 1.0\n",
      "Epoch 2 Loss 0.24132156372070312 Accuracy 1.0\n",
      "Epoch 2 Loss 0.1650247871875763 Accuracy 1.0\n",
      "Epoch 2 Loss 0.15356577932834625 Accuracy 1.0\n",
      "Epoch 2 Loss 0.22168919444084167 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2998144030570984 Accuracy 1.0\n",
      "Epoch 2 Loss 0.28385278582572937 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2955406606197357 Accuracy 1.0\n",
      "Epoch 2 Loss 0.34255725145339966 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.35502082109451294 Accuracy 1.0\n",
      "Epoch 2 Loss 0.28989240527153015 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2192775309085846 Accuracy 1.0\n",
      "Epoch 2 Loss 0.21722915768623352 Accuracy 1.0\n",
      "Epoch 2 Loss 0.22648653388023376 Accuracy 1.0\n",
      "Epoch 2 Loss 0.1661367565393448 Accuracy 1.0\n",
      "Epoch 2 Loss 0.205101877450943 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.29330572485923767 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2471555769443512 Accuracy 1.0\n",
      "Epoch 2 Loss 0.276760458946228 Accuracy 1.0\n",
      "Epoch 2 Loss 0.21104995906352997 Accuracy 1.0\n",
      "Epoch 2 Loss 0.20101113617420197 Accuracy 1.0\n",
      "Epoch 2 Loss 0.30782225728034973 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.27733516693115234 Accuracy 1.0\n",
      "Epoch 2 Loss 0.17462925612926483 Accuracy 1.0\n",
      "Epoch 2 Loss 0.17585837841033936 Accuracy 1.0\n",
      "Epoch 2 Loss 0.21871575713157654 Accuracy 1.0\n",
      "Epoch 2 Loss 0.22418633103370667 Accuracy 1.0\n",
      "Epoch 2 Loss 0.14840564131736755 Accuracy 1.0\n",
      "Epoch 2 Loss 0.30802279710769653 Accuracy 1.0\n",
      "Epoch 2 Loss 0.1953342705965042 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2758951783180237 Accuracy 1.0\n",
      "Epoch 2 Loss 0.22092580795288086 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2677437365055084 Accuracy 1.0\n",
      "Epoch 2 Loss 0.14117562770843506 Accuracy 1.0\n",
      "Epoch 2 Loss 0.316977858543396 Accuracy 1.0\n",
      "Epoch 2 Loss 0.22449152171611786 Accuracy 1.0\n",
      "Epoch 2 Loss 0.24963252246379852 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3077968657016754 Accuracy 1.0\n",
      "Epoch 2 Loss 0.22785906493663788 Accuracy 1.0\n",
      "Epoch 2 Loss 0.26000672578811646 Accuracy 1.0\n",
      "Epoch 2 Loss 0.14889156818389893 Accuracy 1.0\n",
      "Epoch 2 Loss 0.18609866499900818 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3284841775894165 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.28941988945007324 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.19001734256744385 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2462984323501587 Accuracy 1.0\n",
      "Epoch 2 Loss 0.23617012798786163 Accuracy 1.0\n",
      "Epoch 2 Loss 0.20570965111255646 Accuracy 1.0\n",
      "Epoch 2 Loss 0.22975027561187744 Accuracy 1.0\n",
      "Epoch 2 Loss 0.20292150974273682 Accuracy 1.0\n",
      "Epoch 2 Loss 0.19682753086090088 Accuracy 1.0\n",
      "Epoch 2 Loss 0.14404873549938202 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2186492383480072 Accuracy 1.0\n",
      "Epoch 2 Loss 0.19980040192604065 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3395877480506897 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3411598801612854 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.21650651097297668 Accuracy 1.0\n",
      "Epoch 2 Loss 0.155976340174675 Accuracy 1.0\n",
      "Epoch 2 Loss 0.16972388327121735 Accuracy 1.0\n",
      "Epoch 2 Loss 0.23809775710105896 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3028404116630554 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.2943935692310333 Accuracy 1.0\n",
      "Epoch 2 Loss 0.31444376707077026 Accuracy 1.0\n",
      "Epoch 2 Loss 0.27524834871292114 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2198803722858429 Accuracy 1.0\n",
      "Epoch 2 Loss 0.21595986187458038 Accuracy 1.0\n",
      "Epoch 2 Loss 0.1803271323442459 Accuracy 1.0\n",
      "Epoch 2 Loss 0.28206273913383484 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.2047777622938156 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2693009078502655 Accuracy 1.0\n",
      "Epoch 2 Loss 0.26608288288116455 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2027570903301239 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.2987728714942932 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2023930847644806 Accuracy 1.0\n",
      "Epoch 2 Loss 0.23370055854320526 Accuracy 1.0\n",
      "Epoch 2 Loss 0.21107685565948486 Accuracy 1.0\n",
      "Epoch 2 Loss 0.1634114682674408 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2701593041419983 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2076655924320221 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2793453633785248 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2198418378829956 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2790488600730896 Accuracy 1.0\n",
      "Epoch 2 Loss 0.18953943252563477 Accuracy 1.0\n",
      "Epoch 2 Loss 0.24874889850616455 Accuracy 1.0\n",
      "Epoch 2 Loss 0.20689812302589417 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3883031904697418 Accuracy 1.0\n",
      "Epoch 2 Loss 0.19494062662124634 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.21636733412742615 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.31339356303215027 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.202067568898201 Accuracy 1.0\n",
      "Epoch 2 Loss 0.19150501489639282 Accuracy 1.0\n",
      "Epoch 2 Loss 0.1896827667951584 Accuracy 1.0\n",
      "Epoch 2 Loss 0.15314824879169464 Accuracy 1.0\n",
      "Epoch 2 Loss 0.15078990161418915 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2662019729614258 Accuracy 1.0\n",
      "Epoch 2 Loss 0.21049948036670685 Accuracy 1.0\n",
      "Epoch 2 Loss 0.1790408194065094 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2631097733974457 Accuracy 1.0\n",
      "Epoch 2 Loss 0.23592975735664368 Accuracy 1.0\n",
      "Epoch 2 Loss 0.1834314614534378 Accuracy 1.0\n",
      "Epoch 2 Loss 0.22491571307182312 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2509622275829315 Accuracy 1.0\n",
      "Epoch 2 Loss 0.1865372210741043 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3658309578895569 Accuracy 1.0\n",
      "Epoch 2 Loss 0.23965506255626678 Accuracy 1.0\n",
      "Epoch 2 Loss 0.22414088249206543 Accuracy 1.0\n",
      "Epoch 2 Loss 0.28218427300453186 Accuracy 1.0\n",
      "Epoch 2 Loss 0.20142969489097595 Accuracy 1.0\n",
      "Epoch 2 Loss 0.264486700296402 Accuracy 1.0\n",
      "Epoch 2 Loss 0.23588664829730988 Accuracy 1.0\n",
      "Epoch 2 Loss 0.22252726554870605 Accuracy 1.0\n",
      "Epoch 2 Loss 0.14013895392417908 Accuracy 1.0\n",
      "Epoch 2 Loss 0.14396098256111145 Accuracy 1.0\n",
      "Epoch 2 Loss 0.20577581226825714 Accuracy 1.0\n",
      "Epoch 2 Loss 0.11419125646352768 Accuracy 1.0\n",
      "Epoch 2 Loss 0.27682608366012573 Accuracy 1.0\n",
      "Epoch 2 Loss 0.18378561735153198 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3095995783805847 Accuracy 1.0\n",
      "Epoch 2 Loss 0.32584434747695923 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.25828126072883606 Accuracy 1.0\n",
      "Epoch 2 Loss 0.18088051676750183 Accuracy 1.0\n",
      "Epoch 2 Loss 0.26029282808303833 Accuracy 1.0\n",
      "Epoch 2 Loss 0.21251292526721954 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2022857666015625 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2977462112903595 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2653225064277649 Accuracy 1.0\n",
      "Epoch 2 Loss 0.21694903075695038 Accuracy 1.0\n",
      "Epoch 2 Loss 0.16189108788967133 Accuracy 1.0\n",
      "Epoch 2 Loss 0.22331197559833527 Accuracy 1.0\n",
      "Epoch 2 Loss 0.27069440484046936 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2514578104019165 Accuracy 1.0\n",
      "Epoch 2 Loss 0.28886228799819946 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2205214947462082 Accuracy 1.0\n",
      "Epoch 2 Loss 0.32195717096328735 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3885402977466583 Accuracy 1.0\n",
      "Epoch 2 Loss 0.20133757591247559 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3791208863258362 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.281425803899765 Accuracy 1.0\n",
      "Epoch 2 Loss 0.20531782507896423 Accuracy 1.0\n",
      "Epoch 2 Loss 0.15707746148109436 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3842083513736725 Accuracy 1.0\n",
      "Epoch 2 Loss 0.1988580822944641 Accuracy 1.0\n",
      "Epoch 2 Loss 0.17311084270477295 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2143450677394867 Accuracy 1.0\n",
      "Epoch 2 Loss 0.18828296661376953 Accuracy 1.0\n",
      "Epoch 2 Loss 0.24184677004814148 Accuracy 1.0\n",
      "Epoch 2 Loss 0.1289243996143341 Accuracy 1.0\n",
      "Epoch 2 Loss 0.31566908955574036 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.2608712613582611 Accuracy 1.0\n",
      "Epoch 2 Loss 0.1695825606584549 Accuracy 1.0\n",
      "Epoch 2 Loss 0.22495293617248535 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2510541081428528 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3279186487197876 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3674386441707611 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2163129597902298 Accuracy 1.0\n",
      "Epoch 2 Loss 0.16808852553367615 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2070622742176056 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2959200143814087 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.3019161820411682 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.2905029058456421 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2741650938987732 Accuracy 1.0\n",
      "Epoch 2 Loss 0.27406027913093567 Accuracy 1.0\n",
      "Epoch 2 Loss 0.19985923171043396 Accuracy 1.0\n",
      "Epoch 2 Loss 0.24877765774726868 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2951044738292694 Accuracy 1.0\n",
      "Epoch 2 Loss 0.1924518495798111 Accuracy 1.0\n",
      "Epoch 2 Loss 0.23516447842121124 Accuracy 1.0\n",
      "Epoch 2 Loss 0.17491263151168823 Accuracy 1.0\n",
      "Epoch 2 Loss 0.28244107961654663 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2051614224910736 Accuracy 1.0\n",
      "Epoch 2 Loss 0.24589991569519043 Accuracy 1.0\n",
      "Epoch 2 Loss 0.1760595738887787 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2385316640138626 Accuracy 1.0\n",
      "Epoch 2 Loss 0.24536064267158508 Accuracy 1.0\n",
      "Epoch 2 Loss 0.372235506772995 Accuracy 1.0\n",
      "Epoch 2 Loss 0.23680858314037323 Accuracy 1.0\n",
      "Epoch 2 Loss 0.22120197117328644 Accuracy 1.0\n",
      "Epoch 2 Loss 0.187026709318161 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2974771559238434 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.2262052595615387 Accuracy 1.0\n",
      "Epoch 2 Loss 0.33465081453323364 Accuracy 1.0\n",
      "Epoch 2 Loss 0.18172210454940796 Accuracy 1.0\n",
      "Epoch 2 Loss 0.11111700534820557 Accuracy 1.0\n",
      "Epoch 2 Loss 0.250160276889801 Accuracy 1.0\n",
      "Epoch 2 Loss 0.22069188952445984 Accuracy 1.0\n",
      "Epoch 2 Loss 0.1391846239566803 Accuracy 1.0\n",
      "Epoch 2 Loss 0.21899732947349548 Accuracy 1.0\n",
      "Epoch 2 Loss 0.19528809189796448 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2254016399383545 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2746517062187195 Accuracy 1.0\n",
      "Epoch 2 Loss 0.20858344435691833 Accuracy 1.0\n",
      "Epoch 2 Loss 0.21804673969745636 Accuracy 1.0\n",
      "Epoch 2 Loss 0.19310081005096436 Accuracy 1.0\n",
      "Epoch 2 Loss 0.34870538115501404 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2061789333820343 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.24027733504772186 Accuracy 1.0\n",
      "Epoch 2 Loss 0.23698480427265167 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.27089205384254456 Accuracy 1.0\n",
      "Epoch 2 Loss 0.27690526843070984 Accuracy 1.0\n",
      "Epoch 2 Loss 0.20078250765800476 Accuracy 1.0\n",
      "Epoch 2 Loss 0.18300721049308777 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.2995370924472809 Accuracy 1.0\n",
      "Epoch 2 Loss 0.295846551656723 Accuracy 1.0\n",
      "Epoch 2 Loss 0.10350193083286285 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2710344195365906 Accuracy 1.0\n",
      "Epoch 2 Loss 0.215052992105484 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2505302429199219 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.30010271072387695 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.12787720561027527 Accuracy 1.0\n",
      "Epoch 2 Loss 0.25192955136299133 Accuracy 1.0\n",
      "Epoch 2 Loss 0.1739591658115387 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2785499393939972 Accuracy 1.0\n",
      "Epoch 2 Loss 0.1514616310596466 Accuracy 1.0\n",
      "Epoch 2 Loss 0.25215572118759155 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2761093080043793 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.16149559617042542 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2802480161190033 Accuracy 1.0\n",
      "Epoch 2 Loss 0.20456667244434357 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.14885637164115906 Accuracy 1.0\n",
      "Epoch 2 Loss 0.21146854758262634 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2722826898097992 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.2700454890727997 Accuracy 1.0\n",
      "Epoch 2 Loss 0.16426940262317657 Accuracy 1.0\n",
      "Epoch 2 Loss 0.13883836567401886 Accuracy 1.0\n",
      "Epoch 2 Loss 0.17388245463371277 Accuracy 1.0\n",
      "Epoch 2 Loss 0.28711622953414917 Accuracy 1.0\n",
      "Epoch 2 Loss 0.23219212889671326 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.1414274126291275 Accuracy 1.0\n",
      "Epoch 2 Loss 0.14663778245449066 Accuracy 1.0\n",
      "Epoch 2 Loss 0.212694451212883 Accuracy 1.0\n",
      "Epoch 2 Loss 0.1751691848039627 Accuracy 1.0\n",
      "Epoch 2 Loss 0.33122342824935913 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.16410812735557556 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2490074187517166 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.336926132440567 Accuracy 1.0\n",
      "Epoch 2 Loss 0.22361299395561218 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2786750793457031 Accuracy 0.800000011920929\n",
      "Epoch 2 Loss 0.2988972067832947 Accuracy 1.0\n",
      "Epoch 2 Loss 0.224800705909729 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2111365795135498 Accuracy 1.0\n",
      "Epoch 2 Loss 0.13039441406726837 Accuracy 1.0\n",
      "Epoch 2 Loss 0.22038397192955017 Accuracy 1.0\n",
      "Epoch 2 Loss 0.24671950936317444 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2534812092781067 Accuracy 1.0\n",
      "Epoch 2 Loss 0.1624552309513092 Accuracy 1.0\n",
      "Epoch 2 Loss 0.1722336709499359 Accuracy 1.0\n",
      "Epoch 2 Loss 0.19508446753025055 Accuracy 1.0\n",
      "Epoch 2 Loss 0.29353219270706177 Accuracy 1.0\n",
      "Epoch 2 Loss 0.23298747837543488 Accuracy 1.0\n",
      "Epoch 2 Loss 0.20073334872722626 Accuracy 1.0\n",
      "Epoch 2 Loss 0.24358265101909637 Accuracy 1.0\n",
      "Epoch 2 Loss 0.28659382462501526 Accuracy 1.0\n",
      "Epoch 2 Loss 0.1526612788438797 Accuracy 1.0\n",
      "Epoch 2 Loss 0.1342376321554184 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2158193588256836 Accuracy 1.0\n",
      "Epoch 2 Loss 0.20098665356636047 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2755343019962311 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.17115575075149536 Accuracy 1.0\n",
      "Epoch 2 Loss 0.22034187614917755 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2903518080711365 Accuracy 0.800000011920929\n",
      "Epoch 2 Loss 0.2105398178100586 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.34430593252182007 Accuracy 1.0\n",
      "Epoch 2 Loss 0.21935603022575378 Accuracy 1.0\n",
      "Epoch 2 Loss 0.16376496851444244 Accuracy 1.0\n",
      "Epoch 2 Loss 0.23001790046691895 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2522166669368744 Accuracy 1.0\n",
      "Epoch 2 Loss 0.19292305409908295 Accuracy 1.0\n",
      "Epoch 2 Loss 0.13143174350261688 Accuracy 1.0\n",
      "Epoch 2 Loss 0.23573806881904602 Accuracy 1.0\n",
      "Epoch 2 Loss 0.27423062920570374 Accuracy 1.0\n",
      "Epoch 2 Loss 0.25837546586990356 Accuracy 1.0\n",
      "Epoch 2 Loss 0.22399480640888214 Accuracy 1.0\n",
      "Epoch 2 Loss 0.16271580755710602 Accuracy 1.0\n",
      "Epoch 2 Loss 0.09890757501125336 Accuracy 1.0\n",
      "Epoch 2 Loss 0.14849922060966492 Accuracy 1.0\n",
      "Epoch 2 Loss 0.371092289686203 Accuracy 1.0\n",
      "Epoch 2 Loss 0.17495304346084595 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3092727065086365 Accuracy 1.0\n",
      "Epoch 2 Loss 0.28352394700050354 Accuracy 1.0\n",
      "Epoch 2 Loss 0.19885413348674774 Accuracy 1.0\n",
      "Epoch 2 Loss 0.17216739058494568 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2008422166109085 Accuracy 1.0\n",
      "Epoch 2 Loss 0.22883817553520203 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2306375801563263 Accuracy 1.0\n",
      "Epoch 2 Loss 0.12712208926677704 Accuracy 1.0\n",
      "Epoch 2 Loss 0.17173780500888824 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2921443581581116 Accuracy 1.0\n",
      "Epoch 2 Loss 0.22486081719398499 Accuracy 1.0\n",
      "Epoch 2 Loss 0.20858168601989746 Accuracy 1.0\n",
      "Epoch 2 Loss 0.22896233201026917 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.31296080350875854 Accuracy 1.0\n",
      "Epoch 2 Loss 0.24221286177635193 Accuracy 1.0\n",
      "Epoch 2 Loss 0.22057966887950897 Accuracy 1.0\n",
      "Epoch 2 Loss 0.15702521800994873 Accuracy 1.0\n",
      "Epoch 2 Loss 0.24357661604881287 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.13122670352458954 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3365273177623749 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3373357653617859 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3758125901222229 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2553410530090332 Accuracy 1.0\n",
      "Epoch 2 Loss 0.12128806114196777 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2942770719528198 Accuracy 1.0\n",
      "Epoch 2 Loss 0.1479131579399109 Accuracy 1.0\n",
      "Epoch 2 Loss 0.27378708124160767 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.21898193657398224 Accuracy 1.0\n",
      "Epoch 2 Loss 0.23047256469726562 Accuracy 1.0\n",
      "Epoch 2 Loss 0.23665888607501984 Accuracy 1.0\n",
      "Epoch 2 Loss 0.18351803719997406 Accuracy 1.0\n",
      "Epoch 2 Loss 0.30145564675331116 Accuracy 0.800000011920929\n",
      "Epoch 2 Loss 0.09856561571359634 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3397441804409027 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.1823868602514267 Accuracy 1.0\n",
      "Epoch 2 Loss 0.28388553857803345 Accuracy 1.0\n",
      "Epoch 2 Loss 0.22021010518074036 Accuracy 1.0\n",
      "Epoch 2 Loss 0.24634654819965363 Accuracy 1.0\n",
      "Epoch 2 Loss 0.26457899808883667 Accuracy 1.0\n",
      "Epoch 2 Loss 0.3092269003391266 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2515723705291748 Accuracy 1.0\n",
      "Epoch 2 Loss 0.1694071739912033 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2792297303676605 Accuracy 1.0\n",
      "Epoch 2 Loss 0.26905325055122375 Accuracy 1.0\n",
      "Epoch 2 Loss 0.22365720570087433 Accuracy 1.0\n",
      "Epoch 2 Loss 0.198329895734787 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2385452538728714 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.15268217027187347 Accuracy 1.0\n",
      "Epoch 2 Loss 0.35168689489364624 Accuracy 1.0\n",
      "Epoch 2 Loss 0.1886967122554779 Accuracy 1.0\n",
      "Epoch 2 Loss 0.10306589305400848 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2498139590024948 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2214767038822174 Accuracy 1.0\n",
      "Epoch 2 Loss 0.22055569291114807 Accuracy 1.0\n",
      "Epoch 2 Loss 0.1559470146894455 Accuracy 1.0\n",
      "Epoch 2 Loss 0.2762068808078766 Accuracy 1.0\n",
      "Epoch 2 Loss 0.13821636140346527 Accuracy 1.0\n",
      "Epoch 2 Loss 0.252771258354187 Accuracy 0.8999999761581421\n",
      "Epoch 2 Loss 0.1677858531475067 Accuracy 1.0\n",
      "Epoch 3 Loss 0.14471259713172913 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16265307366847992 Accuracy 1.0\n",
      "Epoch 3 Loss 0.24227432906627655 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2819446921348572 Accuracy 1.0\n",
      "Epoch 3 Loss 0.14228609204292297 Accuracy 1.0\n",
      "Epoch 3 Loss 0.22540661692619324 Accuracy 1.0\n",
      "Epoch 3 Loss 0.20055115222930908 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16762132942676544 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21636632084846497 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1793728470802307 Accuracy 1.0\n",
      "Epoch 3 Loss 0.20393212139606476 Accuracy 1.0\n",
      "Epoch 3 Loss 0.20358295738697052 Accuracy 1.0\n",
      "Epoch 3 Loss 0.24027004837989807 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1528632640838623 Accuracy 1.0\n",
      "Epoch 3 Loss 0.17981521785259247 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16177372634410858 Accuracy 1.0\n",
      "Epoch 3 Loss 0.13937532901763916 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2809685468673706 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.14286381006240845 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1753496676683426 Accuracy 1.0\n",
      "Epoch 3 Loss 0.17169739305973053 Accuracy 1.0\n",
      "Epoch 3 Loss 0.19794169068336487 Accuracy 1.0\n",
      "Epoch 3 Loss 0.3045654892921448 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2063920795917511 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2095223367214203 Accuracy 1.0\n",
      "Epoch 3 Loss 0.30340251326560974 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2618439793586731 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1306287795305252 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16219142079353333 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18313436210155487 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16440005600452423 Accuracy 1.0\n",
      "Epoch 3 Loss 0.26861053705215454 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18276137113571167 Accuracy 1.0\n",
      "Epoch 3 Loss 0.22817134857177734 Accuracy 1.0\n",
      "Epoch 3 Loss 0.221965953707695 Accuracy 1.0\n",
      "Epoch 3 Loss 0.38561147451400757 Accuracy 1.0\n",
      "Epoch 3 Loss 0.172431081533432 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1807304173707962 Accuracy 1.0\n",
      "Epoch 3 Loss 0.24196703732013702 Accuracy 1.0\n",
      "Epoch 3 Loss 0.13182920217514038 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2268708199262619 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.194255068898201 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2318808138370514 Accuracy 1.0\n",
      "Epoch 3 Loss 0.22890496253967285 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21241696178913116 Accuracy 1.0\n",
      "Epoch 3 Loss 0.10555250942707062 Accuracy 1.0\n",
      "Epoch 3 Loss 0.20952148735523224 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.1641954779624939 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1547211855649948 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2915869355201721 Accuracy 1.0\n",
      "Epoch 3 Loss 0.29032739996910095 Accuracy 1.0\n",
      "Epoch 3 Loss 0.22537446022033691 Accuracy 1.0\n",
      "Epoch 3 Loss 0.15211208164691925 Accuracy 1.0\n",
      "Epoch 3 Loss 0.24696597456932068 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21988196671009064 Accuracy 1.0\n",
      "Epoch 3 Loss 0.27105027437210083 Accuracy 1.0\n",
      "Epoch 3 Loss 0.38203150033950806 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.26975327730178833 Accuracy 1.0\n",
      "Epoch 3 Loss 0.20523424446582794 Accuracy 1.0\n",
      "Epoch 3 Loss 0.23096124827861786 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1741163581609726 Accuracy 1.0\n",
      "Epoch 3 Loss 0.23310621082782745 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1272275149822235 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21380138397216797 Accuracy 1.0\n",
      "Epoch 3 Loss 0.23632636666297913 Accuracy 1.0\n",
      "Epoch 3 Loss 0.14505784213542938 Accuracy 1.0\n",
      "Epoch 3 Loss 0.26900243759155273 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2078593522310257 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16536971926689148 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1366288810968399 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21158191561698914 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2025318145751953 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12760880589485168 Accuracy 1.0\n",
      "Epoch 3 Loss 0.13507021963596344 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12899044156074524 Accuracy 1.0\n",
      "Epoch 3 Loss 0.20395246148109436 Accuracy 1.0\n",
      "Epoch 3 Loss 0.15011845529079437 Accuracy 1.0\n",
      "Epoch 3 Loss 0.26081690192222595 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2503567039966583 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2724175453186035 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2166517674922943 Accuracy 1.0\n",
      "Epoch 3 Loss 0.13292691111564636 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18795664608478546 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2586429715156555 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2839757800102234 Accuracy 1.0\n",
      "Epoch 3 Loss 0.11905763298273087 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18953558802604675 Accuracy 1.0\n",
      "Epoch 3 Loss 0.17761990427970886 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2708784341812134 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2474788874387741 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2578226625919342 Accuracy 1.0\n",
      "Epoch 3 Loss 0.27497464418411255 Accuracy 1.0\n",
      "Epoch 3 Loss 0.13203178346157074 Accuracy 1.0\n",
      "Epoch 3 Loss 0.23522627353668213 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.1796003133058548 Accuracy 1.0\n",
      "Epoch 3 Loss 0.20929250121116638 Accuracy 1.0\n",
      "Epoch 3 Loss 0.29295486211776733 Accuracy 1.0\n",
      "Epoch 3 Loss 0.19956476986408234 Accuracy 1.0\n",
      "Epoch 3 Loss 0.202846497297287 Accuracy 1.0\n",
      "Epoch 3 Loss 0.24556699395179749 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.20228934288024902 Accuracy 1.0\n",
      "Epoch 3 Loss 0.23908808827400208 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.238399937748909 Accuracy 1.0\n",
      "Epoch 3 Loss 0.23388981819152832 Accuracy 1.0\n",
      "Epoch 3 Loss 0.17547383904457092 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2588202655315399 Accuracy 1.0\n",
      "Epoch 3 Loss 0.22435171902179718 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2562706470489502 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2295258790254593 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2597064971923828 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21975822746753693 Accuracy 1.0\n",
      "Epoch 3 Loss 0.17710888385772705 Accuracy 1.0\n",
      "Epoch 3 Loss 0.15777507424354553 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1375352144241333 Accuracy 1.0\n",
      "Epoch 3 Loss 0.11310654878616333 Accuracy 1.0\n",
      "Epoch 3 Loss 0.15719906985759735 Accuracy 1.0\n",
      "Epoch 3 Loss 0.20796962082386017 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2394581288099289 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1694534420967102 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16005831956863403 Accuracy 1.0\n",
      "Epoch 3 Loss 0.277698814868927 Accuracy 1.0\n",
      "Epoch 3 Loss 0.23767057061195374 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2411072999238968 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21298928558826447 Accuracy 1.0\n",
      "Epoch 3 Loss 0.22635146975517273 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1291508972644806 Accuracy 1.0\n",
      "Epoch 3 Loss 0.31264784932136536 Accuracy 1.0\n",
      "Epoch 3 Loss 0.13970324397087097 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1884772777557373 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1502073109149933 Accuracy 1.0\n",
      "Epoch 3 Loss 0.19970694184303284 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12682214379310608 Accuracy 1.0\n",
      "Epoch 3 Loss 0.24231772124767303 Accuracy 1.0\n",
      "Epoch 3 Loss 0.20606276392936707 Accuracy 1.0\n",
      "Epoch 3 Loss 0.24866874516010284 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2842206060886383 Accuracy 1.0\n",
      "Epoch 3 Loss 0.09761065244674683 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21665175259113312 Accuracy 1.0\n",
      "Epoch 3 Loss 0.19100841879844666 Accuracy 1.0\n",
      "Epoch 3 Loss 0.26713141798973083 Accuracy 1.0\n",
      "Epoch 3 Loss 0.20799390971660614 Accuracy 1.0\n",
      "Epoch 3 Loss 0.19031350314617157 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16254359483718872 Accuracy 1.0\n",
      "Epoch 3 Loss 0.22172443568706512 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1588830053806305 Accuracy 1.0\n",
      "Epoch 3 Loss 0.206701397895813 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2354389876127243 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2180415689945221 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1932145655155182 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2541425824165344 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1879388689994812 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21178846061229706 Accuracy 1.0\n",
      "Epoch 3 Loss 0.19917581975460052 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2548956274986267 Accuracy 1.0\n",
      "Epoch 3 Loss 0.14396415650844574 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1608942747116089 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16924799978733063 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1711806356906891 Accuracy 1.0\n",
      "Epoch 3 Loss 0.07938464730978012 Accuracy 1.0\n",
      "Epoch 3 Loss 0.19403178989887238 Accuracy 1.0\n",
      "Epoch 3 Loss 0.23771516978740692 Accuracy 1.0\n",
      "Epoch 3 Loss 0.22573721408843994 Accuracy 1.0\n",
      "Epoch 3 Loss 0.10312457382678986 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21037650108337402 Accuracy 1.0\n",
      "Epoch 3 Loss 0.3850948214530945 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18986381590366364 Accuracy 1.0\n",
      "Epoch 3 Loss 0.10443315654993057 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2151401937007904 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2555222809314728 Accuracy 1.0\n",
      "Epoch 3 Loss 0.23189611732959747 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2438817024230957 Accuracy 1.0\n",
      "Epoch 3 Loss 0.22471816837787628 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2124744951725006 Accuracy 1.0\n",
      "Epoch 3 Loss 0.265813946723938 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2826346755027771 Accuracy 1.0\n",
      "Epoch 3 Loss 0.17631955444812775 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21314139664173126 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1661822497844696 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1937125027179718 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16947844624519348 Accuracy 1.0\n",
      "Epoch 3 Loss 0.14212681353092194 Accuracy 1.0\n",
      "Epoch 3 Loss 0.10605321079492569 Accuracy 1.0\n",
      "Epoch 3 Loss 0.09289923310279846 Accuracy 1.0\n",
      "Epoch 3 Loss 0.26129788160324097 Accuracy 1.0\n",
      "Epoch 3 Loss 0.20589199662208557 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16399778425693512 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12258221209049225 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1485985517501831 Accuracy 1.0\n",
      "Epoch 3 Loss 0.28945067524909973 Accuracy 1.0\n",
      "Epoch 3 Loss 0.0715673640370369 Accuracy 1.0\n",
      "Epoch 3 Loss 0.27649956941604614 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.2704898715019226 Accuracy 1.0\n",
      "Epoch 3 Loss 0.15059898793697357 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16018681228160858 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21985585987567902 Accuracy 1.0\n",
      "Epoch 3 Loss 0.19391481578350067 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16798296570777893 Accuracy 1.0\n",
      "Epoch 3 Loss 0.22262868285179138 Accuracy 1.0\n",
      "Epoch 3 Loss 0.17898325622081757 Accuracy 1.0\n",
      "Epoch 3 Loss 0.15861231088638306 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1188335195183754 Accuracy 1.0\n",
      "Epoch 3 Loss 0.27433016896247864 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2718699872493744 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1936071366071701 Accuracy 1.0\n",
      "Epoch 3 Loss 0.19796068966388702 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16344119608402252 Accuracy 1.0\n",
      "Epoch 3 Loss 0.24569813907146454 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.17484082281589508 Accuracy 1.0\n",
      "Epoch 3 Loss 0.11941130459308624 Accuracy 1.0\n",
      "Epoch 3 Loss 0.23232468962669373 Accuracy 1.0\n",
      "Epoch 3 Loss 0.22057512402534485 Accuracy 1.0\n",
      "Epoch 3 Loss 0.27826187014579773 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18173182010650635 Accuracy 1.0\n",
      "Epoch 3 Loss 0.09001858532428741 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1739310771226883 Accuracy 1.0\n",
      "Epoch 3 Loss 0.07429435104131699 Accuracy 1.0\n",
      "Epoch 3 Loss 0.17245560884475708 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18554331362247467 Accuracy 1.0\n",
      "Epoch 3 Loss 0.14632192254066467 Accuracy 1.0\n",
      "Epoch 3 Loss 0.22122438251972198 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18157552182674408 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18196876347064972 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12791864573955536 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18443679809570312 Accuracy 1.0\n",
      "Epoch 3 Loss 0.17414745688438416 Accuracy 1.0\n",
      "Epoch 3 Loss 0.15025176107883453 Accuracy 1.0\n",
      "Epoch 3 Loss 0.15225253999233246 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.27356189489364624 Accuracy 1.0\n",
      "Epoch 3 Loss 0.11369462311267853 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21120724081993103 Accuracy 1.0\n",
      "Epoch 3 Loss 0.14385566115379333 Accuracy 1.0\n",
      "Epoch 3 Loss 0.31828755140304565 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1885734349489212 Accuracy 1.0\n",
      "Epoch 3 Loss 0.23491239547729492 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16688592731952667 Accuracy 1.0\n",
      "Epoch 3 Loss 0.09581970423460007 Accuracy 1.0\n",
      "Epoch 3 Loss 0.3321407437324524 Accuracy 1.0\n",
      "Epoch 3 Loss 0.11585938930511475 Accuracy 1.0\n",
      "Epoch 3 Loss 0.17645663022994995 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21585795283317566 Accuracy 1.0\n",
      "Epoch 3 Loss 0.24174363911151886 Accuracy 1.0\n",
      "Epoch 3 Loss 0.07349148392677307 Accuracy 1.0\n",
      "Epoch 3 Loss 0.25921523571014404 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.1577051877975464 Accuracy 1.0\n",
      "Epoch 3 Loss 0.20761799812316895 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16798247396945953 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21589156985282898 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21611300110816956 Accuracy 1.0\n",
      "Epoch 3 Loss 0.09381424635648727 Accuracy 1.0\n",
      "Epoch 3 Loss 0.19558380544185638 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16641013324260712 Accuracy 1.0\n",
      "Epoch 3 Loss 0.23376496136188507 Accuracy 1.0\n",
      "Epoch 3 Loss 0.05037817358970642 Accuracy 1.0\n",
      "Epoch 3 Loss 0.25360792875289917 Accuracy 1.0\n",
      "Epoch 3 Loss 0.11977650225162506 Accuracy 1.0\n",
      "Epoch 3 Loss 0.20005889236927032 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1826554387807846 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1695600003004074 Accuracy 1.0\n",
      "Epoch 3 Loss 0.13686059415340424 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16954506933689117 Accuracy 1.0\n",
      "Epoch 3 Loss 0.17750872671604156 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18486616015434265 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16073210537433624 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1988605558872223 Accuracy 1.0\n",
      "Epoch 3 Loss 0.11210314929485321 Accuracy 1.0\n",
      "Epoch 3 Loss 0.22321519255638123 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16295453906059265 Accuracy 1.0\n",
      "Epoch 3 Loss 0.11618639528751373 Accuracy 1.0\n",
      "Epoch 3 Loss 0.30616599321365356 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16265587508678436 Accuracy 1.0\n",
      "Epoch 3 Loss 0.17122559249401093 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1537010818719864 Accuracy 1.0\n",
      "Epoch 3 Loss 0.137819305062294 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18927907943725586 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2512989342212677 Accuracy 1.0\n",
      "Epoch 3 Loss 0.14197322726249695 Accuracy 1.0\n",
      "Epoch 3 Loss 0.22818100452423096 Accuracy 1.0\n",
      "Epoch 3 Loss 0.14972549676895142 Accuracy 1.0\n",
      "Epoch 3 Loss 0.15555685758590698 Accuracy 1.0\n",
      "Epoch 3 Loss 0.31284695863723755 Accuracy 1.0\n",
      "Epoch 3 Loss 0.26353204250335693 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12085721641778946 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1625884771347046 Accuracy 1.0\n",
      "Epoch 3 Loss 0.303240031003952 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1932726502418518 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18020808696746826 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2152198851108551 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2016303986310959 Accuracy 1.0\n",
      "Epoch 3 Loss 0.15874283015727997 Accuracy 1.0\n",
      "Epoch 3 Loss 0.19021065533161163 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.19123053550720215 Accuracy 1.0\n",
      "Epoch 3 Loss 0.09976847469806671 Accuracy 1.0\n",
      "Epoch 3 Loss 0.15044058859348297 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21076419949531555 Accuracy 1.0\n",
      "Epoch 3 Loss 0.11010418087244034 Accuracy 1.0\n",
      "Epoch 3 Loss 0.09370989352464676 Accuracy 1.0\n",
      "Epoch 3 Loss 0.19102849066257477 Accuracy 1.0\n",
      "Epoch 3 Loss 0.29574599862098694 Accuracy 1.0\n",
      "Epoch 3 Loss 0.135493203997612 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2869910001754761 Accuracy 1.0\n",
      "Epoch 3 Loss 0.15551665425300598 Accuracy 1.0\n",
      "Epoch 3 Loss 0.27717432379722595 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1405792236328125 Accuracy 1.0\n",
      "Epoch 3 Loss 0.22543983161449432 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16415835916996002 Accuracy 1.0\n",
      "Epoch 3 Loss 0.19237524271011353 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1986948549747467 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18406233191490173 Accuracy 1.0\n",
      "Epoch 3 Loss 0.25216010212898254 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18565979599952698 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18005013465881348 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21836566925048828 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2541651725769043 Accuracy 1.0\n",
      "Epoch 3 Loss 0.210404634475708 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12302408367395401 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1297641098499298 Accuracy 1.0\n",
      "Epoch 3 Loss 0.11370296776294708 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16877028346061707 Accuracy 1.0\n",
      "Epoch 3 Loss 0.19107310473918915 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1622716635465622 Accuracy 1.0\n",
      "Epoch 3 Loss 0.11403889954090118 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12363727390766144 Accuracy 1.0\n",
      "Epoch 3 Loss 0.09430006891489029 Accuracy 1.0\n",
      "Epoch 3 Loss 0.11849409341812134 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12584766745567322 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12182404845952988 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12032399326562881 Accuracy 1.0\n",
      "Epoch 3 Loss 0.22080948948860168 Accuracy 1.0\n",
      "Epoch 3 Loss 0.23347048461437225 Accuracy 1.0\n",
      "Epoch 3 Loss 0.15723195672035217 Accuracy 1.0\n",
      "Epoch 3 Loss 0.22288668155670166 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2351342737674713 Accuracy 1.0\n",
      "Epoch 3 Loss 0.20130792260169983 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2067829668521881 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1316499412059784 Accuracy 1.0\n",
      "Epoch 3 Loss 0.24987724423408508 Accuracy 1.0\n",
      "Epoch 3 Loss 0.22737427055835724 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21074692904949188 Accuracy 1.0\n",
      "Epoch 3 Loss 0.20813381671905518 Accuracy 1.0\n",
      "Epoch 3 Loss 0.11991745233535767 Accuracy 1.0\n",
      "Epoch 3 Loss 0.22841951251029968 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.13551858067512512 Accuracy 1.0\n",
      "Epoch 3 Loss 0.14942719042301178 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.27217814326286316 Accuracy 1.0\n",
      "Epoch 3 Loss 0.22057576477527618 Accuracy 1.0\n",
      "Epoch 3 Loss 0.19764673709869385 Accuracy 1.0\n",
      "Epoch 3 Loss 0.19389371573925018 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2737235426902771 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1610381305217743 Accuracy 1.0\n",
      "Epoch 3 Loss 0.13354650139808655 Accuracy 1.0\n",
      "Epoch 3 Loss 0.136869415640831 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1455121636390686 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12984639406204224 Accuracy 1.0\n",
      "Epoch 3 Loss 0.20853383839130402 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2092345952987671 Accuracy 1.0\n",
      "Epoch 3 Loss 0.23858113586902618 Accuracy 1.0\n",
      "Epoch 3 Loss 0.19128718972206116 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.08638586103916168 Accuracy 1.0\n",
      "Epoch 3 Loss 0.13744045794010162 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2530665993690491 Accuracy 1.0\n",
      "Epoch 3 Loss 0.07994961738586426 Accuracy 1.0\n",
      "Epoch 3 Loss 0.11658114194869995 Accuracy 1.0\n",
      "Epoch 3 Loss 0.17373976111412048 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21681873500347137 Accuracy 1.0\n",
      "Epoch 3 Loss 0.14225712418556213 Accuracy 1.0\n",
      "Epoch 3 Loss 0.25026148557662964 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1301312893629074 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1643097996711731 Accuracy 1.0\n",
      "Epoch 3 Loss 0.17331495881080627 Accuracy 1.0\n",
      "Epoch 3 Loss 0.24666288495063782 Accuracy 1.0\n",
      "Epoch 3 Loss 0.339038223028183 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1491258144378662 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21818093955516815 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2938075065612793 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.15876108407974243 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21929128468036652 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16853757202625275 Accuracy 1.0\n",
      "Epoch 3 Loss 0.3485988676548004 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1553194671869278 Accuracy 1.0\n",
      "Epoch 3 Loss 0.11128044128417969 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21585531532764435 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18147750198841095 Accuracy 1.0\n",
      "Epoch 3 Loss 0.15990237891674042 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1956990361213684 Accuracy 1.0\n",
      "Epoch 3 Loss 0.26271283626556396 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21558082103729248 Accuracy 1.0\n",
      "Epoch 3 Loss 0.19623759388923645 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12962432205677032 Accuracy 1.0\n",
      "Epoch 3 Loss 0.09034056961536407 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2860044240951538 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.20200471580028534 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1756783276796341 Accuracy 1.0\n",
      "Epoch 3 Loss 0.222011536359787 Accuracy 1.0\n",
      "Epoch 3 Loss 0.13500545918941498 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1216527670621872 Accuracy 1.0\n",
      "Epoch 3 Loss 0.23338329792022705 Accuracy 1.0\n",
      "Epoch 3 Loss 0.09979043900966644 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2535853385925293 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.1808405965566635 Accuracy 1.0\n",
      "Epoch 3 Loss 0.17891183495521545 Accuracy 1.0\n",
      "Epoch 3 Loss 0.24472060799598694 Accuracy 1.0\n",
      "Epoch 3 Loss 0.11740569770336151 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.1962537169456482 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1750865876674652 Accuracy 1.0\n",
      "Epoch 3 Loss 0.07410252839326859 Accuracy 1.0\n",
      "Epoch 3 Loss 0.26944640278816223 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1636934131383896 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1935790777206421 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.16741381585597992 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1415732353925705 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.1102793961763382 Accuracy 1.0\n",
      "Epoch 3 Loss 0.15717600286006927 Accuracy 1.0\n",
      "Epoch 3 Loss 0.22433283925056458 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2926017642021179 Accuracy 1.0\n",
      "Epoch 3 Loss 0.34579142928123474 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.18936626613140106 Accuracy 1.0\n",
      "Epoch 3 Loss 0.31599798798561096 Accuracy 0.800000011920929\n",
      "Epoch 3 Loss 0.08810853958129883 Accuracy 1.0\n",
      "Epoch 3 Loss 0.26666513085365295 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21512384712696075 Accuracy 1.0\n",
      "Epoch 3 Loss 0.23855121433734894 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.08783548325300217 Accuracy 1.0\n",
      "Epoch 3 Loss 0.13970091938972473 Accuracy 1.0\n",
      "Epoch 3 Loss 0.3286494314670563 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.26321083307266235 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1400453895330429 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2426072657108307 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2780974507331848 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2085121124982834 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16439440846443176 Accuracy 1.0\n",
      "Epoch 3 Loss 0.11041083186864853 Accuracy 1.0\n",
      "Epoch 3 Loss 0.25216934084892273 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2362908124923706 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21850347518920898 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21358346939086914 Accuracy 1.0\n",
      "Epoch 3 Loss 0.27295413613319397 Accuracy 1.0\n",
      "Epoch 3 Loss 0.08792789280414581 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12077274173498154 Accuracy 1.0\n",
      "Epoch 3 Loss 0.24369697272777557 Accuracy 1.0\n",
      "Epoch 3 Loss 0.13403549790382385 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2567718029022217 Accuracy 1.0\n",
      "Epoch 3 Loss 0.230798602104187 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2954219579696655 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18324148654937744 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1947491466999054 Accuracy 1.0\n",
      "Epoch 3 Loss 0.206882044672966 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2622900605201721 Accuracy 1.0\n",
      "Epoch 3 Loss 0.08925598859786987 Accuracy 1.0\n",
      "Epoch 3 Loss 0.20173987746238708 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2463502585887909 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16193023324012756 Accuracy 1.0\n",
      "Epoch 3 Loss 0.19069458544254303 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16909560561180115 Accuracy 1.0\n",
      "Epoch 3 Loss 0.15453387796878815 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21613678336143494 Accuracy 1.0\n",
      "Epoch 3 Loss 0.15195471048355103 Accuracy 1.0\n",
      "Epoch 3 Loss 0.09558327496051788 Accuracy 1.0\n",
      "Epoch 3 Loss 0.23526641726493835 Accuracy 1.0\n",
      "Epoch 3 Loss 0.13187947869300842 Accuracy 1.0\n",
      "Epoch 3 Loss 0.17857882380485535 Accuracy 1.0\n",
      "Epoch 3 Loss 0.14231345057487488 Accuracy 1.0\n",
      "Epoch 3 Loss 0.10091204941272736 Accuracy 1.0\n",
      "Epoch 3 Loss 0.13722875714302063 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18675868213176727 Accuracy 1.0\n",
      "Epoch 3 Loss 0.19857564568519592 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2732469439506531 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18121233582496643 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.1987883746623993 Accuracy 1.0\n",
      "Epoch 3 Loss 0.24642400443553925 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1556042730808258 Accuracy 1.0\n",
      "Epoch 3 Loss 0.19894863665103912 Accuracy 1.0\n",
      "Epoch 3 Loss 0.14933902025222778 Accuracy 1.0\n",
      "Epoch 3 Loss 0.23614270985126495 Accuracy 1.0\n",
      "Epoch 3 Loss 0.23738911747932434 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18281564116477966 Accuracy 1.0\n",
      "Epoch 3 Loss 0.07185085862874985 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12564104795455933 Accuracy 1.0\n",
      "Epoch 3 Loss 0.20141272246837616 Accuracy 1.0\n",
      "Epoch 3 Loss 0.14764277637004852 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12197695672512054 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1550145447254181 Accuracy 1.0\n",
      "Epoch 3 Loss 0.26881909370422363 Accuracy 1.0\n",
      "Epoch 3 Loss 0.13328424096107483 Accuracy 1.0\n",
      "Epoch 3 Loss 0.07241079211235046 Accuracy 1.0\n",
      "Epoch 3 Loss 0.20620103180408478 Accuracy 1.0\n",
      "Epoch 3 Loss 0.23672059178352356 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21333102881908417 Accuracy 1.0\n",
      "Epoch 3 Loss 0.0866413339972496 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1711934357881546 Accuracy 1.0\n",
      "Epoch 3 Loss 0.23359303176403046 Accuracy 1.0\n",
      "Epoch 3 Loss 0.09912039339542389 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18430675566196442 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2707524001598358 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16017532348632812 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1145685687661171 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21718411147594452 Accuracy 1.0\n",
      "Epoch 3 Loss 0.27557721734046936 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1490117609500885 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1448695808649063 Accuracy 1.0\n",
      "Epoch 3 Loss 0.11285750567913055 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18599370121955872 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21683868765830994 Accuracy 1.0\n",
      "Epoch 3 Loss 0.09722811728715897 Accuracy 1.0\n",
      "Epoch 3 Loss 0.10564180463552475 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12564781308174133 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2565748393535614 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12486615031957626 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2711995244026184 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2249850034713745 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12902754545211792 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21315470337867737 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18185092508792877 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18282926082611084 Accuracy 1.0\n",
      "Epoch 3 Loss 0.11145524680614471 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2444372922182083 Accuracy 1.0\n",
      "Epoch 3 Loss 0.11553923040628433 Accuracy 1.0\n",
      "Epoch 3 Loss 0.23833413422107697 Accuracy 1.0\n",
      "Epoch 3 Loss 0.13308420777320862 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16151274740695953 Accuracy 1.0\n",
      "Epoch 3 Loss 0.30812346935272217 Accuracy 1.0\n",
      "Epoch 3 Loss 0.09302730858325958 Accuracy 1.0\n",
      "Epoch 3 Loss 0.15114852786064148 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2077484130859375 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18541178107261658 Accuracy 1.0\n",
      "Epoch 3 Loss 0.23331455886363983 Accuracy 1.0\n",
      "Epoch 3 Loss 0.08647677302360535 Accuracy 1.0\n",
      "Epoch 3 Loss 0.07482612133026123 Accuracy 1.0\n",
      "Epoch 3 Loss 0.11589137464761734 Accuracy 1.0\n",
      "Epoch 3 Loss 0.20461830496788025 Accuracy 1.0\n",
      "Epoch 3 Loss 0.24763858318328857 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.061458468437194824 Accuracy 1.0\n",
      "Epoch 3 Loss 0.19772453606128693 Accuracy 1.0\n",
      "Epoch 3 Loss 0.17029854655265808 Accuracy 1.0\n",
      "Epoch 3 Loss 0.26270154118537903 Accuracy 1.0\n",
      "Epoch 3 Loss 0.17774400115013123 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2115022838115692 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18455353379249573 Accuracy 1.0\n",
      "Epoch 3 Loss 0.19902203977108002 Accuracy 1.0\n",
      "Epoch 3 Loss 0.19658848643302917 Accuracy 1.0\n",
      "Epoch 3 Loss 0.27900558710098267 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.16644622385501862 Accuracy 1.0\n",
      "Epoch 3 Loss 0.23674997687339783 Accuracy 1.0\n",
      "Epoch 3 Loss 0.07646069675683975 Accuracy 1.0\n",
      "Epoch 3 Loss 0.15101340413093567 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1545639932155609 Accuracy 1.0\n",
      "Epoch 3 Loss 0.27147403359413147 Accuracy 1.0\n",
      "Epoch 3 Loss 0.11270759999752045 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12616005539894104 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1450742483139038 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1778176873922348 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1725064516067505 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1859346181154251 Accuracy 1.0\n",
      "Epoch 3 Loss 0.20832426846027374 Accuracy 1.0\n",
      "Epoch 3 Loss 0.19236263632774353 Accuracy 1.0\n",
      "Epoch 3 Loss 0.14118632674217224 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16195762157440186 Accuracy 1.0\n",
      "Epoch 3 Loss 0.13542863726615906 Accuracy 1.0\n",
      "Epoch 3 Loss 0.05899155139923096 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1634514480829239 Accuracy 1.0\n",
      "Epoch 3 Loss 0.230922669172287 Accuracy 1.0\n",
      "Epoch 3 Loss 0.08010314404964447 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1011083722114563 Accuracy 1.0\n",
      "Epoch 3 Loss 0.205244779586792 Accuracy 1.0\n",
      "Epoch 3 Loss 0.13952107727527618 Accuracy 1.0\n",
      "Epoch 3 Loss 0.20761361718177795 Accuracy 1.0\n",
      "Epoch 3 Loss 0.17650793492794037 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1996959149837494 Accuracy 1.0\n",
      "Epoch 3 Loss 0.14603325724601746 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12989622354507446 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12287591397762299 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21328361332416534 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.08638526499271393 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16268262267112732 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18286550045013428 Accuracy 1.0\n",
      "Epoch 3 Loss 0.23980072140693665 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.15031227469444275 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16576847434043884 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2066703587770462 Accuracy 1.0\n",
      "Epoch 3 Loss 0.17661680281162262 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12244274467229843 Accuracy 1.0\n",
      "Epoch 3 Loss 0.17315460741519928 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2416088581085205 Accuracy 1.0\n",
      "Epoch 3 Loss 0.11101987212896347 Accuracy 1.0\n",
      "Epoch 3 Loss 0.09896983951330185 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2924934923648834 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21807503700256348 Accuracy 1.0\n",
      "Epoch 3 Loss 0.14625237882137299 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16442957520484924 Accuracy 1.0\n",
      "Epoch 3 Loss 0.13533876836299896 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.08993780612945557 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2618287205696106 Accuracy 1.0\n",
      "Epoch 3 Loss 0.20859989523887634 Accuracy 1.0\n",
      "Epoch 3 Loss 0.19696108996868134 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2871689796447754 Accuracy 1.0\n",
      "Epoch 3 Loss 0.08953100442886353 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1591407060623169 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18022261559963226 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1937616914510727 Accuracy 1.0\n",
      "Epoch 3 Loss 0.19909827411174774 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1595815271139145 Accuracy 1.0\n",
      "Epoch 3 Loss 0.05529385805130005 Accuracy 1.0\n",
      "Epoch 3 Loss 0.127882719039917 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1436459720134735 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12825796008110046 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1691686362028122 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2342383861541748 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.20771503448486328 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2452356070280075 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1269805133342743 Accuracy 1.0\n",
      "Epoch 3 Loss 0.25148048996925354 Accuracy 1.0\n",
      "Epoch 3 Loss 0.3004606366157532 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.1149565577507019 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12988640367984772 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1301887333393097 Accuracy 1.0\n",
      "Epoch 3 Loss 0.09054339677095413 Accuracy 1.0\n",
      "Epoch 3 Loss 0.11918356269598007 Accuracy 1.0\n",
      "Epoch 3 Loss 0.08099035173654556 Accuracy 1.0\n",
      "Epoch 3 Loss 0.22609774768352509 Accuracy 1.0\n",
      "Epoch 3 Loss 0.13322702050209045 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2545320689678192 Accuracy 1.0\n",
      "Epoch 3 Loss 0.151607945561409 Accuracy 1.0\n",
      "Epoch 3 Loss 0.20575949549674988 Accuracy 1.0\n",
      "Epoch 3 Loss 0.13156089186668396 Accuracy 1.0\n",
      "Epoch 3 Loss 0.22254319489002228 Accuracy 1.0\n",
      "Epoch 3 Loss 0.145010843873024 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1809399425983429 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12931594252586365 Accuracy 1.0\n",
      "Epoch 3 Loss 0.06450118869543076 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2094440758228302 Accuracy 1.0\n",
      "Epoch 3 Loss 0.207681804895401 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1533905416727066 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2849555015563965 Accuracy 1.0\n",
      "Epoch 3 Loss 0.19946089386940002 Accuracy 1.0\n",
      "Epoch 3 Loss 0.19908921420574188 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16577216982841492 Accuracy 1.0\n",
      "Epoch 3 Loss 0.23429551720619202 Accuracy 1.0\n",
      "Epoch 3 Loss 0.17332175374031067 Accuracy 1.0\n",
      "Epoch 3 Loss 0.14504271745681763 Accuracy 1.0\n",
      "Epoch 3 Loss 0.11241105943918228 Accuracy 1.0\n",
      "Epoch 3 Loss 0.08575909584760666 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2525464594364166 Accuracy 1.0\n",
      "Epoch 3 Loss 0.22700563073158264 Accuracy 1.0\n",
      "Epoch 3 Loss 0.09731043875217438 Accuracy 1.0\n",
      "Epoch 3 Loss 0.10511837154626846 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1464032679796219 Accuracy 1.0\n",
      "Epoch 3 Loss 0.11606977134943008 Accuracy 1.0\n",
      "Epoch 3 Loss 0.3220975995063782 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.20396403968334198 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1905137598514557 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.14504030346870422 Accuracy 1.0\n",
      "Epoch 3 Loss 0.22904188930988312 Accuracy 0.800000011920929\n",
      "Epoch 3 Loss 0.2572035789489746 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1929982602596283 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.19621998071670532 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1717749834060669 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12694303691387177 Accuracy 1.0\n",
      "Epoch 3 Loss 0.11848320066928864 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21173062920570374 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21928437054157257 Accuracy 1.0\n",
      "Epoch 3 Loss 0.13590344786643982 Accuracy 1.0\n",
      "Epoch 3 Loss 0.19629541039466858 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12945187091827393 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18389545381069183 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18741244077682495 Accuracy 1.0\n",
      "Epoch 3 Loss 0.13964806497097015 Accuracy 1.0\n",
      "Epoch 3 Loss 0.15136288106441498 Accuracy 1.0\n",
      "Epoch 3 Loss 0.08371230214834213 Accuracy 1.0\n",
      "Epoch 3 Loss 0.08010299503803253 Accuracy 1.0\n",
      "Epoch 3 Loss 0.13257689774036407 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2159658670425415 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2034655064344406 Accuracy 1.0\n",
      "Epoch 3 Loss 0.24221329391002655 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2533378601074219 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2730787396430969 Accuracy 1.0\n",
      "Epoch 3 Loss 0.211603045463562 Accuracy 1.0\n",
      "Epoch 3 Loss 0.14202269911766052 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12261562049388885 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1470508873462677 Accuracy 1.0\n",
      "Epoch 3 Loss 0.07600502669811249 Accuracy 1.0\n",
      "Epoch 3 Loss 0.13487471640110016 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.22403593361377716 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1705830693244934 Accuracy 1.0\n",
      "Epoch 3 Loss 0.20247706770896912 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12197092920541763 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12450222671031952 Accuracy 1.0\n",
      "Epoch 3 Loss 0.22673988342285156 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.19128844141960144 Accuracy 1.0\n",
      "Epoch 3 Loss 0.0944586843252182 Accuracy 1.0\n",
      "Epoch 3 Loss 0.097906693816185 Accuracy 1.0\n",
      "Epoch 3 Loss 0.13635419309139252 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16094978153705597 Accuracy 1.0\n",
      "Epoch 3 Loss 0.07666647434234619 Accuracy 1.0\n",
      "Epoch 3 Loss 0.23899546265602112 Accuracy 1.0\n",
      "Epoch 3 Loss 0.10651715099811554 Accuracy 1.0\n",
      "Epoch 3 Loss 0.19016720354557037 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1377159059047699 Accuracy 1.0\n",
      "Epoch 3 Loss 0.17234691977500916 Accuracy 1.0\n",
      "Epoch 3 Loss 0.06789042800664902 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2398412525653839 Accuracy 1.0\n",
      "Epoch 3 Loss 0.13594089448451996 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16443905234336853 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2311134785413742 Accuracy 1.0\n",
      "Epoch 3 Loss 0.14507170021533966 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1643405258655548 Accuracy 1.0\n",
      "Epoch 3 Loss 0.07413559406995773 Accuracy 1.0\n",
      "Epoch 3 Loss 0.09945592284202576 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2524470388889313 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.21548548340797424 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.11448343843221664 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16752099990844727 Accuracy 1.0\n",
      "Epoch 3 Loss 0.15149642527103424 Accuracy 1.0\n",
      "Epoch 3 Loss 0.11974556744098663 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1628837287425995 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12807442247867584 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12001408636569977 Accuracy 1.0\n",
      "Epoch 3 Loss 0.07491359114646912 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1255495846271515 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12640267610549927 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2696252465248108 Accuracy 1.0\n",
      "Epoch 3 Loss 0.27701836824417114 Accuracy 1.0\n",
      "Epoch 3 Loss 0.15285766124725342 Accuracy 1.0\n",
      "Epoch 3 Loss 0.08291958272457123 Accuracy 1.0\n",
      "Epoch 3 Loss 0.08113886415958405 Accuracy 1.0\n",
      "Epoch 3 Loss 0.159640833735466 Accuracy 1.0\n",
      "Epoch 3 Loss 0.23640258610248566 Accuracy 1.0\n",
      "Epoch 3 Loss 0.20776169002056122 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2311292588710785 Accuracy 1.0\n",
      "Epoch 3 Loss 0.19089177250862122 Accuracy 1.0\n",
      "Epoch 3 Loss 0.13103197515010834 Accuracy 1.0\n",
      "Epoch 3 Loss 0.13897408545017242 Accuracy 1.0\n",
      "Epoch 3 Loss 0.09475156664848328 Accuracy 1.0\n",
      "Epoch 3 Loss 0.22134175896644592 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.12588192522525787 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18716736137866974 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1920616775751114 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12948733568191528 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21156851947307587 Accuracy 1.0\n",
      "Epoch 3 Loss 0.13215935230255127 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1622733175754547 Accuracy 1.0\n",
      "Epoch 3 Loss 0.13249599933624268 Accuracy 1.0\n",
      "Epoch 3 Loss 0.10051009804010391 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18968752026557922 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12323179095983505 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1889805793762207 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1329089105129242 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21160630881786346 Accuracy 1.0\n",
      "Epoch 3 Loss 0.11240444332361221 Accuracy 1.0\n",
      "Epoch 3 Loss 0.14695198833942413 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12283160537481308 Accuracy 1.0\n",
      "Epoch 3 Loss 0.3008900284767151 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12668441236019135 Accuracy 1.0\n",
      "Epoch 3 Loss 0.15781530737876892 Accuracy 1.0\n",
      "Epoch 3 Loss 0.231540709733963 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.12320321798324585 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1082698255777359 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12137897312641144 Accuracy 1.0\n",
      "Epoch 3 Loss 0.07784459739923477 Accuracy 1.0\n",
      "Epoch 3 Loss 0.07397465407848358 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18925586342811584 Accuracy 1.0\n",
      "Epoch 3 Loss 0.13342201709747314 Accuracy 1.0\n",
      "Epoch 3 Loss 0.10007061809301376 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18066629767417908 Accuracy 1.0\n",
      "Epoch 3 Loss 0.15512976050376892 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12134554237127304 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1467587649822235 Accuracy 1.0\n",
      "Epoch 3 Loss 0.15506792068481445 Accuracy 1.0\n",
      "Epoch 3 Loss 0.11745850741863251 Accuracy 1.0\n",
      "Epoch 3 Loss 0.28635111451148987 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1619921624660492 Accuracy 1.0\n",
      "Epoch 3 Loss 0.15556088089942932 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2085518091917038 Accuracy 1.0\n",
      "Epoch 3 Loss 0.13482359051704407 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18818902969360352 Accuracy 1.0\n",
      "Epoch 3 Loss 0.17492491006851196 Accuracy 1.0\n",
      "Epoch 3 Loss 0.14867404103279114 Accuracy 1.0\n",
      "Epoch 3 Loss 0.07107214629650116 Accuracy 1.0\n",
      "Epoch 3 Loss 0.07782600820064545 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12957248091697693 Accuracy 1.0\n",
      "Epoch 3 Loss 0.04740416258573532 Accuracy 1.0\n",
      "Epoch 3 Loss 0.17867916822433472 Accuracy 1.0\n",
      "Epoch 3 Loss 0.11389008909463882 Accuracy 1.0\n",
      "Epoch 3 Loss 0.22037550806999207 Accuracy 1.0\n",
      "Epoch 3 Loss 0.26399070024490356 Accuracy 1.0\n",
      "Epoch 3 Loss 0.17976923286914825 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12432742118835449 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18732935190200806 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1372155100107193 Accuracy 1.0\n",
      "Epoch 3 Loss 0.11437009274959564 Accuracy 1.0\n",
      "Epoch 3 Loss 0.23850210011005402 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16653995215892792 Accuracy 1.0\n",
      "Epoch 3 Loss 0.13381262123584747 Accuracy 1.0\n",
      "Epoch 3 Loss 0.0826992467045784 Accuracy 1.0\n",
      "Epoch 3 Loss 0.15807422995567322 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18736782670021057 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1752796769142151 Accuracy 1.0\n",
      "Epoch 3 Loss 0.22393564879894257 Accuracy 1.0\n",
      "Epoch 3 Loss 0.14499518275260925 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2540658414363861 Accuracy 1.0\n",
      "Epoch 3 Loss 0.3335234522819519 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1327820122241974 Accuracy 1.0\n",
      "Epoch 3 Loss 0.30636894702911377 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.19800660014152527 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1411927342414856 Accuracy 1.0\n",
      "Epoch 3 Loss 0.08314784616231918 Accuracy 1.0\n",
      "Epoch 3 Loss 0.3208189606666565 Accuracy 1.0\n",
      "Epoch 3 Loss 0.13345623016357422 Accuracy 1.0\n",
      "Epoch 3 Loss 0.10003284364938736 Accuracy 1.0\n",
      "Epoch 3 Loss 0.14618785679340363 Accuracy 1.0\n",
      "Epoch 3 Loss 0.11368390172719955 Accuracy 1.0\n",
      "Epoch 3 Loss 0.17605045437812805 Accuracy 1.0\n",
      "Epoch 3 Loss 0.0583769753575325 Accuracy 1.0\n",
      "Epoch 3 Loss 0.24346216022968292 Accuracy 1.0\n",
      "Epoch 3 Loss 0.192506343126297 Accuracy 1.0\n",
      "Epoch 3 Loss 0.10688155889511108 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16007786989212036 Accuracy 1.0\n",
      "Epoch 3 Loss 0.17839311063289642 Accuracy 1.0\n",
      "Epoch 3 Loss 0.26130029559135437 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2939302325248718 Accuracy 1.0\n",
      "Epoch 3 Loss 0.15115031599998474 Accuracy 1.0\n",
      "Epoch 3 Loss 0.10097364336252213 Accuracy 1.0\n",
      "Epoch 3 Loss 0.128233402967453 Accuracy 1.0\n",
      "Epoch 3 Loss 0.22842831909656525 Accuracy 1.0\n",
      "Epoch 3 Loss 0.23926544189453125 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21338017284870148 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18946196138858795 Accuracy 1.0\n",
      "Epoch 3 Loss 0.19433173537254333 Accuracy 1.0\n",
      "Epoch 3 Loss 0.13424226641654968 Accuracy 1.0\n",
      "Epoch 3 Loss 0.17489084601402283 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21055400371551514 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1420830339193344 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16832752525806427 Accuracy 1.0\n",
      "Epoch 3 Loss 0.09520161896944046 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21398112177848816 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1452590972185135 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16377225518226624 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1088302880525589 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1679990440607071 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16915659606456757 Accuracy 1.0\n",
      "Epoch 3 Loss 0.30228644609451294 Accuracy 1.0\n",
      "Epoch 3 Loss 0.15204159915447235 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16090813279151917 Accuracy 1.0\n",
      "Epoch 3 Loss 0.11606751382350922 Accuracy 1.0\n",
      "Epoch 3 Loss 0.231648251414299 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.1527373045682907 Accuracy 1.0\n",
      "Epoch 3 Loss 0.251286119222641 Accuracy 1.0\n",
      "Epoch 3 Loss 0.10634927451610565 Accuracy 1.0\n",
      "Epoch 3 Loss 0.04797469824552536 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16959072649478912 Accuracy 1.0\n",
      "Epoch 3 Loss 0.14241471886634827 Accuracy 1.0\n",
      "Epoch 3 Loss 0.06553910672664642 Accuracy 1.0\n",
      "Epoch 3 Loss 0.137126162648201 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12940216064453125 Accuracy 1.0\n",
      "Epoch 3 Loss 0.15651282668113708 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2092481553554535 Accuracy 1.0\n",
      "Epoch 3 Loss 0.13838928937911987 Accuracy 1.0\n",
      "Epoch 3 Loss 0.141641765832901 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1280916929244995 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2786825895309448 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1391366869211197 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.16582146286964417 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18142205476760864 Accuracy 1.0\n",
      "Epoch 3 Loss 0.19396314024925232 Accuracy 1.0\n",
      "Epoch 3 Loss 0.20679745078086853 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1359657347202301 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12634891271591187 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.2294384241104126 Accuracy 1.0\n",
      "Epoch 3 Loss 0.23317714035511017 Accuracy 1.0\n",
      "Epoch 3 Loss 0.042037270963191986 Accuracy 1.0\n",
      "Epoch 3 Loss 0.20405098795890808 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1577376425266266 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18188849091529846 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.23001758754253387 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.0647788867354393 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16570083796977997 Accuracy 1.0\n",
      "Epoch 3 Loss 0.10707984119653702 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21185632050037384 Accuracy 1.0\n",
      "Epoch 3 Loss 0.09179659187793732 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18927201628684998 Accuracy 1.0\n",
      "Epoch 3 Loss 0.204412579536438 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.09595728665590286 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2250046283006668 Accuracy 1.0\n",
      "Epoch 3 Loss 0.14056852459907532 Accuracy 1.0\n",
      "Epoch 3 Loss 0.08049272000789642 Accuracy 1.0\n",
      "Epoch 3 Loss 0.13437946140766144 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21756353974342346 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.21089772880077362 Accuracy 1.0\n",
      "Epoch 3 Loss 0.08607061207294464 Accuracy 1.0\n",
      "Epoch 3 Loss 0.08644656836986542 Accuracy 1.0\n",
      "Epoch 3 Loss 0.11390423774719238 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21651995182037354 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16850146651268005 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.07989564538002014 Accuracy 1.0\n",
      "Epoch 3 Loss 0.08327778428792953 Accuracy 1.0\n",
      "Epoch 3 Loss 0.14653031527996063 Accuracy 1.0\n",
      "Epoch 3 Loss 0.09363757073879242 Accuracy 1.0\n",
      "Epoch 3 Loss 0.25994229316711426 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.09302783012390137 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1698632836341858 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2850882112979889 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1498228907585144 Accuracy 1.0\n",
      "Epoch 3 Loss 0.216344952583313 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.21105584502220154 Accuracy 1.0\n",
      "Epoch 3 Loss 0.14044936001300812 Accuracy 1.0\n",
      "Epoch 3 Loss 0.14789941906929016 Accuracy 1.0\n",
      "Epoch 3 Loss 0.0891309529542923 Accuracy 1.0\n",
      "Epoch 3 Loss 0.13546478748321533 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1735583394765854 Accuracy 1.0\n",
      "Epoch 3 Loss 0.19257080554962158 Accuracy 1.0\n",
      "Epoch 3 Loss 0.09173844754695892 Accuracy 1.0\n",
      "Epoch 3 Loss 0.09088228642940521 Accuracy 1.0\n",
      "Epoch 3 Loss 0.11683696508407593 Accuracy 1.0\n",
      "Epoch 3 Loss 0.23265984654426575 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1658116579055786 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1425025910139084 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18160530924797058 Accuracy 1.0\n",
      "Epoch 3 Loss 0.22371402382850647 Accuracy 1.0\n",
      "Epoch 3 Loss 0.08648864924907684 Accuracy 1.0\n",
      "Epoch 3 Loss 0.073305144906044 Accuracy 1.0\n",
      "Epoch 3 Loss 0.15300317108631134 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12784543633460999 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21643510460853577 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.10468342155218124 Accuracy 1.0\n",
      "Epoch 3 Loss 0.14145883917808533 Accuracy 1.0\n",
      "Epoch 3 Loss 0.22782054543495178 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.14679959416389465 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2880372405052185 Accuracy 1.0\n",
      "Epoch 3 Loss 0.13645973801612854 Accuracy 1.0\n",
      "Epoch 3 Loss 0.09413820505142212 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1685921549797058 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1958674192428589 Accuracy 1.0\n",
      "Epoch 3 Loss 0.13019177317619324 Accuracy 1.0\n",
      "Epoch 3 Loss 0.066149041056633 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18644237518310547 Accuracy 1.0\n",
      "Epoch 3 Loss 0.20692329108715057 Accuracy 1.0\n",
      "Epoch 3 Loss 0.191123366355896 Accuracy 1.0\n",
      "Epoch 3 Loss 0.14754009246826172 Accuracy 1.0\n",
      "Epoch 3 Loss 0.09248246252536774 Accuracy 1.0\n",
      "Epoch 3 Loss 0.055292658507823944 Accuracy 1.0\n",
      "Epoch 3 Loss 0.0756082832813263 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2991320490837097 Accuracy 1.0\n",
      "Epoch 3 Loss 0.11063959449529648 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2498384714126587 Accuracy 1.0\n",
      "Epoch 3 Loss 0.20442047715187073 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12898191809654236 Accuracy 1.0\n",
      "Epoch 3 Loss 0.11537067592144012 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12308522313833237 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18792423605918884 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16629458963871002 Accuracy 1.0\n",
      "Epoch 3 Loss 0.07090412080287933 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12499894946813583 Accuracy 1.0\n",
      "Epoch 3 Loss 0.22925099730491638 Accuracy 1.0\n",
      "Epoch 3 Loss 0.15928006172180176 Accuracy 1.0\n",
      "Epoch 3 Loss 0.13366606831550598 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1763395369052887 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.23824460804462433 Accuracy 1.0\n",
      "Epoch 3 Loss 0.17371636629104614 Accuracy 1.0\n",
      "Epoch 3 Loss 0.13903701305389404 Accuracy 1.0\n",
      "Epoch 3 Loss 0.10339619964361191 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18311932682991028 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.07005814462900162 Accuracy 1.0\n",
      "Epoch 3 Loss 0.26683351397514343 Accuracy 1.0\n",
      "Epoch 3 Loss 0.27111274003982544 Accuracy 1.0\n",
      "Epoch 3 Loss 0.30546146631240845 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18805013597011566 Accuracy 1.0\n",
      "Epoch 3 Loss 0.06202477216720581 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21879318356513977 Accuracy 1.0\n",
      "Epoch 3 Loss 0.0820227712392807 Accuracy 1.0\n",
      "Epoch 3 Loss 0.20965802669525146 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.15145830810070038 Accuracy 1.0\n",
      "Epoch 3 Loss 0.16655585169792175 Accuracy 1.0\n",
      "Epoch 3 Loss 0.18129447102546692 Accuracy 1.0\n",
      "Epoch 3 Loss 0.12970879673957825 Accuracy 1.0\n",
      "Epoch 3 Loss 0.24707956612110138 Accuracy 0.800000011920929\n",
      "Epoch 3 Loss 0.044325992465019226 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2938931882381439 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.12995842099189758 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21508391201496124 Accuracy 1.0\n",
      "Epoch 3 Loss 0.15360869467258453 Accuracy 1.0\n",
      "Epoch 3 Loss 0.19313515722751617 Accuracy 1.0\n",
      "Epoch 3 Loss 0.20797212421894073 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2311016023159027 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1824248731136322 Accuracy 1.0\n",
      "Epoch 3 Loss 0.11188888549804688 Accuracy 1.0\n",
      "Epoch 3 Loss 0.21881398558616638 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1953345239162445 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1546907126903534 Accuracy 1.0\n",
      "Epoch 3 Loss 0.11964975297451019 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1865110844373703 Accuracy 1.0\n",
      "Epoch 3 Loss 0.10140202194452286 Accuracy 1.0\n",
      "Epoch 3 Loss 0.2953812777996063 Accuracy 1.0\n",
      "Epoch 3 Loss 0.10492992401123047 Accuracy 1.0\n",
      "Epoch 3 Loss 0.058342911303043365 Accuracy 1.0\n",
      "Epoch 3 Loss 0.19049468636512756 Accuracy 1.0\n",
      "Epoch 3 Loss 0.17212054133415222 Accuracy 1.0\n",
      "Epoch 3 Loss 0.14772236347198486 Accuracy 1.0\n",
      "Epoch 3 Loss 0.09934075176715851 Accuracy 1.0\n",
      "Epoch 3 Loss 0.1984146535396576 Accuracy 1.0\n",
      "Epoch 3 Loss 0.07921499013900757 Accuracy 1.0\n",
      "Epoch 3 Loss 0.19729182124137878 Accuracy 0.8999999761581421\n",
      "Epoch 3 Loss 0.09887918084859848 Accuracy 1.0\n",
      "Test Accuracy: 0.9990000128746033\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "data = np.random.rand(10000).reshape((-1, 1))\n",
    "y = data < 0.5\n",
    "y = y.reshape((-1, 1))\n",
    "\n",
    "data_test = np.random.rand(1000).reshape((-1, 1))\n",
    "y_test = data_test < 0.5\n",
    "y_test = y_test.reshape((-1, 1))\n",
    "\n",
    "y = y.astype(float)\n",
    "y_test = y_test.astype(float)\n",
    "\n",
    "data = torch.from_numpy(data)\n",
    "y = torch.from_numpy(y)\n",
    "data_test = torch.from_numpy(data_test)\n",
    "y_test = torch.from_numpy(y_test)\n",
    "\n",
    "y = y.type(torch.float32)\n",
    "y_test = y_test.type(torch.float32)\n",
    "\n",
    "data = data.type(torch.float32)\n",
    "data_test = data_test.type(torch.float32)\n",
    "\n",
    "\n",
    "class SampleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(1, 16)\n",
    "        self.layer2 = nn.Linear(16, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        # print(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "sample_model = SampleModel()\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(sample_model.parameters())\n",
    "batch_size = 10\n",
    "data = torch.split(data, batch_size)\n",
    "y = torch.split(y, batch_size)\n",
    "\n",
    "for epoch in range(4):\n",
    "    for i in range(len(data)):\n",
    "        optimizer.zero_grad()\n",
    "        output = sample_model(data[i])\n",
    "        loss = loss_fn(output, y[i])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        accuracy = (output.round() == y[i]).float().mean()\n",
    "        print('Epoch {} Loss {} Accuracy {}'.format(epoch, loss.item(), accuracy))\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_test = sample_model(data_test)\n",
    "    accuracy_test = (output_test.round() == y_test).float().mean()\n",
    "    print('Test Accuracy:', accuracy_test.item())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Encrypting data\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Create the TenSEAL security context\n",
    "def create_ctx():\n",
    "    \"\"\"Helper for creating the CKKS context.\n",
    "    CKKS params:\n",
    "        - Polynomial degree: 8192.\n",
    "        - Coefficient modulus size: [40, 21, 21, 21, 21, 21, 21, 40].\n",
    "        - Scale: 2 ** 21.\n",
    "        - The setup requires the Galois keys for evaluating the convolutions.\n",
    "    \"\"\"\n",
    "    bits_scale = 26\n",
    "\n",
    "    poly_mod_degree = 16384\n",
    "\n",
    "    coeff_mod_bit_sizes = [31, bits_scale, bits_scale, bits_scale, bits_scale, bits_scale, bits_scale, bits_scale,\n",
    "                           bits_scale, 31]\n",
    "    ctx = ts.context(ts.SCHEME_TYPE.CKKS, poly_mod_degree, -1, coeff_mod_bit_sizes)\n",
    "    ctx.global_scale = pow(2, bits_scale)\n",
    "    ctx.generate_galois_keys()\n",
    "\n",
    "    # We prepare the context for the server, by making it public(we drop the secret key)\n",
    "    server_context = ctx.copy()\n",
    "    server_context.make_context_public()\n",
    "\n",
    "    return ctx, server_context\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Helper for encoding the image\n",
    "def prepare_input(ctx, plain_input):\n",
    "    enc_input = ts.ckks_vector(ctx, plain_input)\n",
    "    return enc_input\n",
    "\n",
    "\n",
    "def prepare_input_encrypted(context: bytes, ckks_vector: bytes) -> ts.CKKSVector:\n",
    "    try:\n",
    "        ctx = ts.context_from(context)\n",
    "        enc_x = ts.ckks_vector_from(ctx, ckks_vector)\n",
    "    except:\n",
    "        raise DeserializationError(\"cannot deserialize context or ckks_vector\")\n",
    "    try:\n",
    "        _ = ctx.galois_keys()\n",
    "    except:\n",
    "        raise InvalidContext(\"the context doesn't hold galois keys\")\n",
    "    return enc_x\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "context, server_context = create_ctx()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "data = np.random.rand(10000).reshape((-1, 1))\n",
    "y = data < 0.5\n",
    "y = y.reshape((-1, 1))\n",
    "\n",
    "data_test = np.random.rand(1000).reshape((-1, 1))\n",
    "y_test = data_test < 0.5\n",
    "y_test = y_test.reshape((-1, 1))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
      "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
      "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n"
     ]
    },
    {
     "data": {
      "text/plain": "(array([6.109e+03, 2.394e+03, 9.440e+02, 3.470e+02, 1.320e+02, 4.500e+01,\n        2.400e+01, 3.000e+00, 0.000e+00, 2.000e+00]),\n array([4.14641765e-10, 2.73004851e-05, 5.46005556e-05, 8.19006260e-05,\n        1.09200696e-04, 1.36500767e-04, 1.63800837e-04, 1.91100908e-04,\n        2.18400978e-04, 2.45701049e-04, 2.73001119e-04]),\n <BarContainer object of 10 artists>)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(\n",
    "    np.abs(np.array(prepare_input(context, data.reshape((-1, 10000))[0]).decrypt()) - data.reshape((-1, 10000)))[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "\n",
    "# encrypted model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_400962/4037029950.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  output = torch.tensor(output).view(1, -1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for data: [0.84314153] we got output : tensor([[0.0356]])\n",
      "for data: [0.69927622] we got output : tensor([[0.1102]])\n",
      "for data: [0.10943258] we got output : tensor([[0.9862]])\n",
      "for data: [0.41922445] we got output : tensor([[0.7084]])\n",
      "for data: [0.53654333] we got output : tensor([[0.3939]])\n",
      "for data: [0.21688907] we got output : tensor([[0.9595]])\n",
      "for data: [0.48367266] we got output : tensor([[0.5389]])\n",
      "for data: [0.04066893] we got output : tensor([[0.9927]])\n",
      "for data: [0.68583913] we got output : tensor([[0.1232]])\n",
      "for data: [0.66580169] we got output : tensor([[0.1455]])\n",
      "for data: [0.89399781] we got output : tensor([[0.0250]])\n",
      "for data: [0.060259] we got output : tensor([[0.9913]])\n",
      "for data: [0.26171306] we got output : tensor([[0.9356]])\n",
      "for data: [0.84277026] we got output : tensor([[0.0357]])\n",
      "for data: [0.81986668] we got output : tensor([[0.0422]])\n",
      "for data: [0.9164547] we got output : tensor([[0.0216]])\n",
      "for data: [0.75472844] we got output : tensor([[0.0700]])\n",
      "for data: [0.5500369] we got output : tensor([[0.3596]])\n",
      "for data: [0.23704328] we got output : tensor([[0.9501]])\n",
      "for data: [0.73887616] we got output : tensor([[0.0796]])\n",
      "for data: [0.84713959] we got output : tensor([[0.0346]])\n",
      "for data: [0.07399675] we got output : tensor([[0.9901]])\n",
      "for data: [0.08285433] we got output : tensor([[0.9892]])\n",
      "for data: [0.68435059] we got output : tensor([[0.1248]])\n",
      "for data: [0.56304028] we got output : tensor([[0.3279]])\n",
      "for data: [0.41726207] we got output : tensor([[0.7130]])\n",
      "for data: [0.28906347] we got output : tensor([[0.9146]])\n",
      "for data: [0.82734354] we got output : tensor([[0.0399]])\n",
      "for data: [0.49257781] we got output : tensor([[0.5139]])\n",
      "for data: [0.25702886] we got output : tensor([[0.9386]])\n",
      "for data: [0.3182284] we got output : tensor([[0.8852]])\n",
      "for data: [0.11962526] we got output : tensor([[0.9848]])\n",
      "for data: [0.35374118] we got output : tensor([[0.8372]])\n",
      "for data: [0.80504427] we got output : tensor([[0.0472]])\n",
      "for data: [0.35744696] we got output : tensor([[0.8315]])\n",
      "for data: [0.66234804] we got output : tensor([[0.1497]])\n",
      "for data: [0.63540915] we got output : tensor([[0.1869]])\n",
      "for data: [0.9441682] we got output : tensor([[0.0181]])\n",
      "for data: [0.36267491] we got output : tensor([[0.8228]])\n",
      "for data: [0.02533831] we got output : tensor([[0.9936]])\n",
      "for data: [0.20229062] we got output : tensor([[0.9651]])\n",
      "for data: [0.9140746] we got output : tensor([[0.0219]])\n",
      "for data: [0.04141857] we got output : tensor([[0.9926]])\n",
      "for data: [0.56240159] we got output : tensor([[0.3294]])\n",
      "for data: [0.33366687] we got output : tensor([[0.8661]])\n",
      "for data: [0.06522198] we got output : tensor([[0.9909]])\n",
      "for data: [0.7612386] we got output : tensor([[0.0664]])\n",
      "for data: [0.14645154] we got output : tensor([[0.9802]])\n",
      "for data: [0.15955399] we got output : tensor([[0.9774]])\n",
      "for data: [0.72209198] we got output : tensor([[0.0913]])\n",
      "for data: [0.26231854] we got output : tensor([[0.9352]])\n",
      "for data: [0.50215312] we got output : tensor([[0.4872]])\n",
      "for data: [0.22935624] we got output : tensor([[0.9539]])\n",
      "for data: [0.83677531] we got output : tensor([[0.0373]])\n",
      "for data: [0.83175806] we got output : tensor([[0.0386]])\n",
      "for data: [0.86191991] we got output : tensor([[0.0311]])\n",
      "for data: [0.97717928] we got output : tensor([[0.0149]])\n",
      "for data: [0.43608923] we got output : tensor([[0.6671]])\n",
      "for data: [0.25773902] we got output : tensor([[0.9382]])\n",
      "for data: [0.94238649] we got output : tensor([[0.0183]])\n",
      "for data: [0.15390057] we got output : tensor([[0.9786]])\n",
      "for data: [0.23831343] we got output : tensor([[0.9494]])\n",
      "for data: [0.77317335] we got output : tensor([[0.0604]])\n",
      "for data: [0.48584117] we got output : tensor([[0.5328]])\n",
      "for data: [0.61197265] we got output : tensor([[0.2259]])\n",
      "for data: [0.91122378] we got output : tensor([[0.0223]])\n",
      "for data: [0.74531789] we got output : tensor([[0.0755]])\n",
      "for data: [0.31478754] we got output : tensor([[0.8891]])\n",
      "for data: [0.10675167] we got output : tensor([[0.9865]])\n",
      "for data: [0.7632955] we got output : tensor([[0.0654]])\n",
      "for data: [0.09463351] we got output : tensor([[0.9880]])\n",
      "for data: [0.4111967] we got output : tensor([[0.7270]])\n",
      "for data: [0.89872918] we got output : tensor([[0.0242]])\n",
      "for data: [0.51558155] we got output : tensor([[0.4500]])\n",
      "for data: [0.10517683] we got output : tensor([[0.9867]])\n",
      "for data: [0.67759673] we got output : tensor([[0.1319]])\n",
      "for data: [0.38617371] we got output : tensor([[0.7801]])\n",
      "for data: [0.38344578] we got output : tensor([[0.7854]])\n",
      "for data: [0.26972604] we got output : tensor([[0.9300]])\n",
      "for data: [0.44066085] we got output : tensor([[0.6555]])\n",
      "for data: [0.48741281] we got output : tensor([[0.5285]])\n",
      "for data: [0.19275845] we got output : tensor([[0.9684]])\n",
      "for data: [0.90351253] we got output : tensor([[0.0235]])\n",
      "for data: [0.03760858] we got output : tensor([[0.9929]])\n",
      "for data: [0.51907193] we got output : tensor([[0.4407]])\n",
      "for data: [0.01977211] we got output : tensor([[0.9939]])\n",
      "for data: [0.86288979] we got output : tensor([[0.0309]])\n",
      "for data: [0.80728992] we got output : tensor([[0.0464]])\n",
      "for data: [0.62397364] we got output : tensor([[0.2052]])\n",
      "for data: [0.72100283] we got output : tensor([[0.0921]])\n",
      "for data: [0.88170978] we got output : tensor([[0.0271]])\n",
      "for data: [0.76843896] we got output : tensor([[0.0627]])\n",
      "for data: [0.6660036] we got output : tensor([[0.1453]])\n",
      "for data: [0.30990248] we got output : tensor([[0.8944]])\n",
      "for data: [0.63651248] we got output : tensor([[0.1853]])\n",
      "for data: [0.56647334] we got output : tensor([[0.3199]])\n",
      "for data: [0.14661622] we got output : tensor([[0.9801]])\n",
      "for data: [0.55567951] we got output : tensor([[0.3456]])\n",
      "for data: [0.89989562] we got output : tensor([[0.0240]])\n",
      "for data: [0.88952305] we got output : tensor([[0.0257]])\n",
      "for data: [0.54669513] we got output : tensor([[0.3678]])\n",
      "for data: [0.88313932] we got output : tensor([[0.0269]])\n",
      "for data: [0.80799368] we got output : tensor([[0.0462]])\n",
      "for data: [0.73751904] we got output : tensor([[0.0804]])\n",
      "for data: [0.22893998] we got output : tensor([[0.9541]])\n",
      "for data: [0.91432373] we got output : tensor([[0.0219]])\n",
      "for data: [0.71392359] we got output : tensor([[0.0976]])\n",
      "for data: [0.47387516] we got output : tensor([[0.5661]])\n",
      "for data: [0.40624344] we got output : tensor([[0.7381]])\n",
      "for data: [0.69106211] we got output : tensor([[0.1180]])\n",
      "for data: [0.92946326] we got output : tensor([[0.0199]])\n",
      "for data: [0.63051124] we got output : tensor([[0.1946]])\n",
      "for data: [0.03008426] we got output : tensor([[0.9933]])\n",
      "for data: [0.88550924] we got output : tensor([[0.0265]])\n",
      "for data: [0.76313081] we got output : tensor([[0.0654]])\n",
      "for data: [0.436295] we got output : tensor([[0.6665]])\n",
      "for data: [0.16979041] we got output : tensor([[0.9750]])\n",
      "for data: [0.34957642] we got output : tensor([[0.8436]])\n",
      "for data: [0.83817198] we got output : tensor([[0.0369]])\n",
      "for data: [0.9092369] we got output : tensor([[0.0226]])\n",
      "for data: [0.22026527] we got output : tensor([[0.9580]])\n",
      "for data: [0.89112202] we got output : tensor([[0.0255]])\n",
      "for data: [0.02645909] we got output : tensor([[0.9935]])\n",
      "for data: [0.58740936] we got output : tensor([[0.2736]])\n",
      "for data: [0.92013794] we got output : tensor([[0.0211]])\n",
      "for data: [0.32687078] we got output : tensor([[0.8748]])\n",
      "for data: [0.37906925] we got output : tensor([[0.7938]])\n",
      "for data: [0.01629702] we got output : tensor([[0.9941]])\n",
      "for data: [0.45078603] we got output : tensor([[0.6288]])\n",
      "for data: [0.01618443] we got output : tensor([[0.9941]])\n",
      "for data: [0.37183424] we got output : tensor([[0.8070]])\n",
      "for data: [0.11069291] we got output : tensor([[0.9860]])\n",
      "for data: [0.6599846] we got output : tensor([[0.1528]])\n",
      "for data: [0.73711057] we got output : tensor([[0.0807]])\n",
      "for data: [0.4320474] we got output : tensor([[0.6772]])\n",
      "for data: [0.59237458] we got output : tensor([[0.2633]])\n",
      "for data: [0.28675887] we got output : tensor([[0.9166]])\n",
      "for data: [0.96131069] we got output : tensor([[0.0164]])\n",
      "for data: [0.41457424] we got output : tensor([[0.7192]])\n",
      "for data: [0.43450223] we got output : tensor([[0.6711]])\n",
      "for data: [0.5701077] we got output : tensor([[0.3116]])\n",
      "for data: [0.25662241] we got output : tensor([[0.9389]])\n",
      "for data: [0.99783033] we got output : tensor([[0.0133]])\n",
      "for data: [0.7434138] we got output : tensor([[0.0767]])\n",
      "for data: [0.82601053] we got output : tensor([[0.0403]])\n",
      "for data: [0.3547749] we got output : tensor([[0.8356]])\n",
      "for data: [0.14031778] we got output : tensor([[0.9813]])\n",
      "for data: [0.47229174] we got output : tensor([[0.5705]])\n",
      "for data: [0.25832107] we got output : tensor([[0.9378]])\n",
      "for data: [0.13401818] we got output : tensor([[0.9825]])\n",
      "for data: [0.74769803] we got output : tensor([[0.0741]])\n",
      "for data: [0.19096212] we got output : tensor([[0.9689]])\n",
      "for data: [0.0070471] we got output : tensor([[0.9945]])\n",
      "for data: [0.41466493] we got output : tensor([[0.7191]])\n",
      "for data: [0.84548813] we got output : tensor([[0.0350]])\n",
      "for data: [0.37382854] we got output : tensor([[0.8034]])\n",
      "for data: [0.74784243] we got output : tensor([[0.0740]])\n",
      "for data: [0.22792488] we got output : tensor([[0.9546]])\n",
      "for data: [0.02493394] we got output : tensor([[0.9936]])\n",
      "for data: [0.61969261] we got output : tensor([[0.2123]])\n",
      "for data: [0.07284706] we got output : tensor([[0.9902]])\n",
      "for data: [0.11508516] we got output : tensor([[0.9854]])\n",
      "for data: [0.24417612] we got output : tensor([[0.9463]])\n",
      "for data: [0.11870779] we got output : tensor([[0.9849]])\n",
      "for data: [0.07500974] we got output : tensor([[0.9900]])\n",
      "for data: [0.8546743] we got output : tensor([[0.0328]])\n",
      "for data: [0.17285743] we got output : tensor([[0.9742]])\n",
      "for data: [0.46273641] we got output : tensor([[0.5968]])\n",
      "for data: [0.87268241] we got output : tensor([[0.0289]])\n",
      "for data: [0.43147412] we got output : tensor([[0.6786]])\n",
      "for data: [0.89348935] we got output : tensor([[0.0251]])\n",
      "for data: [0.75092613] we got output : tensor([[0.0721]])\n",
      "for data: [0.12140148] we got output : tensor([[0.9845]])\n",
      "for data: [0.51290887] we got output : tensor([[0.4575]])\n",
      "for data: [0.35695527] we got output : tensor([[0.8321]])\n",
      "for data: [0.09241337] we got output : tensor([[0.9882]])\n",
      "for data: [0.273867] we got output : tensor([[0.9270]])\n",
      "for data: [0.67847225] we got output : tensor([[0.1310]])\n",
      "for data: [0.90986512] we got output : tensor([[0.0225]])\n",
      "for data: [0.38244392] we got output : tensor([[0.7873]])\n",
      "for data: [0.33926542] we got output : tensor([[0.8585]])\n",
      "for data: [0.49823533] we got output : tensor([[0.4982]])\n",
      "for data: [0.7999132] we got output : tensor([[0.0491]])\n",
      "for data: [0.97281495] we got output : tensor([[0.0153]])\n",
      "for data: [0.05456344] we got output : tensor([[0.9917]])\n",
      "for data: [0.89574732] we got output : tensor([[0.0247]])\n",
      "for data: [0.73644964] we got output : tensor([[0.0811]])\n",
      "for data: [0.16231613] we got output : tensor([[0.9768]])\n",
      "for data: [0.53196701] we got output : tensor([[0.4060]])\n",
      "for data: [0.31964299] we got output : tensor([[0.8835]])\n",
      "for data: [0.89838844] we got output : tensor([[0.0243]])\n",
      "for data: [0.4314829] we got output : tensor([[0.6787]])\n",
      "for data: [0.48260398] we got output : tensor([[0.5419]])\n",
      "for data: [0.5969887] we got output : tensor([[0.2541]])\n",
      "for data: [0.43095464] we got output : tensor([[0.6799]])\n",
      "for data: [0.60553439] we got output : tensor([[0.2377]])\n",
      "for data: [0.07255512] we got output : tensor([[0.9902]])\n",
      "for data: [0.18020793] we got output : tensor([[0.9722]])\n",
      "for data: [0.15929713] we got output : tensor([[0.9775]])\n",
      "for data: [0.03273184] we got output : tensor([[0.9932]])\n",
      "for data: [0.32607936] we got output : tensor([[0.8758]])\n",
      "for data: [0.61174981] we got output : tensor([[0.2263]])\n",
      "for data: [0.51950561] we got output : tensor([[0.4394]])\n",
      "for data: [0.83096582] we got output : tensor([[0.0389]])\n",
      "for data: [0.16883462] we got output : tensor([[0.9752]])\n",
      "for data: [0.6182181] we got output : tensor([[0.2148]])\n",
      "for data: [0.36064791] we got output : tensor([[0.8261]])\n",
      "for data: [0.87287583] we got output : tensor([[0.0288]])\n",
      "for data: [0.93059736] we got output : tensor([[0.0197]])\n",
      "for data: [0.72091547] we got output : tensor([[0.0922]])\n",
      "for data: [0.76649704] we got output : tensor([[0.0637]])\n",
      "for data: [0.80505074] we got output : tensor([[0.0472]])\n",
      "for data: [0.34719922] we got output : tensor([[0.8472]])\n",
      "for data: [0.68088159] we got output : tensor([[0.1284]])\n",
      "for data: [0.93096462] we got output : tensor([[0.0197]])\n",
      "for data: [0.35479908] we got output : tensor([[0.8355]])\n",
      "for data: [0.11114816] we got output : tensor([[0.9859]])\n",
      "for data: [0.70930438] we got output : tensor([[0.1014]])\n",
      "for data: [0.21212331] we got output : tensor([[0.9614]])\n",
      "for data: [0.66359303] we got output : tensor([[0.1482]])\n",
      "for data: [0.98198625] we got output : tensor([[0.0145]])\n",
      "for data: [0.89306335] we got output : tensor([[0.0251]])\n",
      "for data: [0.15814545] we got output : tensor([[0.9777]])\n",
      "for data: [0.04531916] we got output : tensor([[0.9924]])\n",
      "for data: [0.14438982] we got output : tensor([[0.9806]])\n",
      "for data: [0.46879984] we got output : tensor([[0.5802]])\n",
      "for data: [0.67753489] we got output : tensor([[0.1320]])\n",
      "for data: [0.36888324] we got output : tensor([[0.8122]])\n",
      "for data: [0.69795265] we got output : tensor([[0.1114]])\n",
      "for data: [0.13066699] we got output : tensor([[0.9830]])\n",
      "for data: [0.99781796] we got output : tensor([[0.0133]])\n",
      "for data: [0.55860226] we got output : tensor([[0.3385]])\n",
      "for data: [0.93710333] we got output : tensor([[0.0189]])\n",
      "for data: [0.53753319] we got output : tensor([[0.3913]])\n",
      "for data: [0.41732753] we got output : tensor([[0.7128]])\n",
      "for data: [0.74487736] we got output : tensor([[0.0758]])\n",
      "for data: [0.63859696] we got output : tensor([[0.1821]])\n",
      "for data: [0.99309663] we got output : tensor([[0.0136]])\n",
      "for data: [0.5183871] we got output : tensor([[0.4426]])\n",
      "for data: [0.35865287] we got output : tensor([[0.8294]])\n",
      "for data: [0.02546147] we got output : tensor([[0.9936]])\n",
      "for data: [0.13148118] we got output : tensor([[0.9829]])\n",
      "for data: [0.15319566] we got output : tensor([[0.9788]])\n",
      "for data: [0.20297222] we got output : tensor([[0.9649]])\n",
      "for data: [0.75995415] we got output : tensor([[0.0672]])\n",
      "for data: [0.80750189] we got output : tensor([[0.0463]])\n",
      "for data: [0.68241377] we got output : tensor([[0.1268]])\n",
      "for data: [0.96155683] we got output : tensor([[0.0163]])\n",
      "for data: [0.77554575] we got output : tensor([[0.0593]])\n",
      "for data: [0.27786464] we got output : tensor([[0.9239]])\n",
      "for data: [0.94919146] we got output : tensor([[0.0176]])\n",
      "for data: [0.67575464] we got output : tensor([[0.1340]])\n",
      "for data: [0.68355219] we got output : tensor([[0.1256]])\n",
      "for data: [0.60921538] we got output : tensor([[0.2308]])\n",
      "for data: [0.94115913] we got output : tensor([[0.0185]])\n",
      "for data: [0.24783112] we got output : tensor([[0.9442]])\n",
      "for data: [0.19834229] we got output : tensor([[0.9665]])\n",
      "for data: [0.57893486] we got output : tensor([[0.2917]])\n",
      "for data: [0.54980661] we got output : tensor([[0.3602]])\n",
      "for data: [0.33597557] we got output : tensor([[0.8630]])\n",
      "for data: [0.73538915] we got output : tensor([[0.0819]])\n",
      "for data: [0.26477888] we got output : tensor([[0.9335]])\n",
      "for data: [0.74673377] we got output : tensor([[0.0747]])\n",
      "for data: [0.93593004] we got output : tensor([[0.0191]])\n",
      "for data: [0.78531063] we got output : tensor([[0.0549]])\n",
      "for data: [0.3464213] we got output : tensor([[0.8483]])\n",
      "for data: [0.87982301] we got output : tensor([[0.0275]])\n",
      "for data: [0.57722465] we got output : tensor([[0.2955]])\n",
      "for data: [0.59855288] we got output : tensor([[0.2511]])\n",
      "for data: [0.20321148] we got output : tensor([[0.9648]])\n",
      "for data: [0.60649168] we got output : tensor([[0.2359]])\n",
      "for data: [0.02875442] we got output : tensor([[0.9934]])\n",
      "for data: [0.47006289] we got output : tensor([[0.5767]])\n",
      "for data: [0.98104447] we got output : tensor([[0.0146]])\n",
      "for data: [0.0629186] we got output : tensor([[0.9910]])\n",
      "for data: [0.35877109] we got output : tensor([[0.8293]])\n",
      "for data: [0.71586777] we got output : tensor([[0.0961]])\n",
      "for data: [0.21665131] we got output : tensor([[0.9596]])\n",
      "for data: [0.66785764] we got output : tensor([[0.1430]])\n",
      "for data: [0.05769029] we got output : tensor([[0.9915]])\n",
      "for data: [0.35724095] we got output : tensor([[0.8317]])\n",
      "for data: [0.55300258] we got output : tensor([[0.3522]])\n",
      "for data: [0.82102162] we got output : tensor([[0.0418]])\n",
      "for data: [0.38475614] we got output : tensor([[0.7829]])\n",
      "for data: [0.89053892] we got output : tensor([[0.0256]])\n",
      "for data: [0.10861129] we got output : tensor([[0.9863]])\n",
      "for data: [0.02061934] we got output : tensor([[0.9938]])\n",
      "for data: [0.73177987] we got output : tensor([[0.0843]])\n",
      "for data: [0.13832717] we got output : tensor([[0.9817]])\n",
      "for data: [0.85650854] we got output : tensor([[0.0323]])\n",
      "for data: [0.74693242] we got output : tensor([[0.0745]])\n",
      "for data: [0.63022046] we got output : tensor([[0.1951]])\n",
      "for data: [0.25732445] we got output : tensor([[0.9384]])\n",
      "for data: [0.23356851] we got output : tensor([[0.9519]])\n",
      "for data: [0.45403213] we got output : tensor([[0.6203]])\n",
      "for data: [0.83859605] we got output : tensor([[0.0368]])\n",
      "for data: [0.89240479] we got output : tensor([[0.0253]])\n",
      "for data: [0.30233205] we got output : tensor([[0.9022]])\n",
      "for data: [0.17460556] we got output : tensor([[0.9737]])\n",
      "for data: [0.48578709] we got output : tensor([[0.5329]])\n",
      "for data: [0.2254438] we got output : tensor([[0.9558]])\n",
      "for data: [0.55210144] we got output : tensor([[0.3544]])\n",
      "for data: [0.60278001] we got output : tensor([[0.2429]])\n",
      "for data: [0.72830397] we got output : tensor([[0.0867]])\n",
      "for data: [0.6351677] we got output : tensor([[0.1873]])\n",
      "for data: [0.80626423] we got output : tensor([[0.0468]])\n",
      "for data: [0.93701604] we got output : tensor([[0.0190]])\n",
      "for data: [0.56483153] we got output : tensor([[0.3236]])\n",
      "for data: [0.50928317] we got output : tensor([[0.4674]])\n",
      "for data: [0.58694611] we got output : tensor([[0.2746]])\n",
      "for data: [0.92029672] we got output : tensor([[0.0211]])\n",
      "for data: [0.25980536] we got output : tensor([[0.9369]])\n",
      "for data: [0.27929874] we got output : tensor([[0.9227]])\n",
      "for data: [0.9210599] we got output : tensor([[0.0210]])\n",
      "for data: [0.83986261] we got output : tensor([[0.0364]])\n",
      "for data: [0.01324696] we got output : tensor([[0.9942]])\n",
      "for data: [0.55943748] we got output : tensor([[0.3364]])\n",
      "for data: [0.1076818] we got output : tensor([[0.9864]])\n",
      "for data: [0.9602639] we got output : tensor([[0.0165]])\n",
      "for data: [0.82612148] we got output : tensor([[0.0403]])\n",
      "for data: [0.12259907] we got output : tensor([[0.9843]])\n",
      "for data: [0.61279961] we got output : tensor([[0.2244]])\n",
      "for data: [0.17773315] we got output : tensor([[0.9729]])\n",
      "for data: [0.53585549] we got output : tensor([[0.3958]])\n",
      "for data: [0.64877808] we got output : tensor([[0.1676]])\n",
      "for data: [0.4350936] we got output : tensor([[0.6696]])\n",
      "for data: [0.45277894] we got output : tensor([[0.6235]])\n",
      "for data: [0.33852929] we got output : tensor([[0.8595]])\n",
      "for data: [0.98491472] we got output : tensor([[0.0143]])\n",
      "for data: [0.86505397] we got output : tensor([[0.0304]])\n",
      "for data: [0.18050115] we got output : tensor([[0.9721]])\n",
      "for data: [0.58051396] we got output : tensor([[0.2883]])\n",
      "for data: [0.25577586] we got output : tensor([[0.9394]])\n",
      "for data: [0.45486073] we got output : tensor([[0.6180]])\n",
      "for data: [0.27494791] we got output : tensor([[0.9262]])\n",
      "for data: [0.6469178] we got output : tensor([[0.1701]])\n",
      "for data: [0.90522269] we got output : tensor([[0.0232]])\n",
      "for data: [0.07130413] we got output : tensor([[0.9903]])\n",
      "for data: [0.0113143] we got output : tensor([[0.9943]])\n",
      "for data: [0.12268197] we got output : tensor([[0.9843]])\n",
      "for data: [0.93517584] we got output : tensor([[0.0192]])\n",
      "for data: [0.37580826] we got output : tensor([[0.7998]])\n",
      "for data: [0.41302279] we got output : tensor([[0.7228]])\n",
      "for data: [0.2328112] we got output : tensor([[0.9522]])\n",
      "for data: [0.74790199] we got output : tensor([[0.0740]])\n",
      "for data: [0.02048258] we got output : tensor([[0.9939]])\n",
      "for data: [0.872451] we got output : tensor([[0.0289]])\n",
      "for data: [0.58185946] we got output : tensor([[0.2854]])\n",
      "for data: [0.72067889] we got output : tensor([[0.0923]])\n",
      "for data: [0.4163427] we got output : tensor([[0.7152]])\n",
      "for data: [0.75906826] we got output : tensor([[0.0676]])\n",
      "for data: [0.0078372] we got output : tensor([[0.9945]])\n",
      "for data: [0.41176791] we got output : tensor([[0.7258]])\n",
      "for data: [0.49287663] we got output : tensor([[0.5130]])\n",
      "for data: [0.38936324] we got output : tensor([[0.7737]])\n",
      "for data: [0.42847012] we got output : tensor([[0.6861]])\n",
      "for data: [0.42029846] we got output : tensor([[0.7057]])\n",
      "for data: [0.09657526] we got output : tensor([[0.9878]])\n",
      "for data: [0.11616781] we got output : tensor([[0.9853]])\n",
      "for data: [0.56835747] we got output : tensor([[0.3155]])\n",
      "for data: [0.36679898] we got output : tensor([[0.8157]])\n",
      "for data: [0.33902399] we got output : tensor([[0.8588]])\n",
      "for data: [0.97887806] we got output : tensor([[0.0148]])\n",
      "for data: [0.09007382] we got output : tensor([[0.9885]])\n",
      "for data: [0.79612044] we got output : tensor([[0.0506]])\n",
      "for data: [0.99945537] we got output : tensor([[0.0132]])\n",
      "for data: [0.35774997] we got output : tensor([[0.8308]])\n",
      "for data: [0.13668382] we got output : tensor([[0.9820]])\n",
      "for data: [0.84285484] we got output : tensor([[0.0357]])\n",
      "for data: [0.69371673] we got output : tensor([[0.1154]])\n",
      "for data: [0.80923617] we got output : tensor([[0.0457]])\n",
      "for data: [0.51701079] we got output : tensor([[0.4461]])\n",
      "for data: [0.13669398] we got output : tensor([[0.9820]])\n",
      "for data: [0.12956988] we got output : tensor([[0.9832]])\n",
      "for data: [0.03684719] we got output : tensor([[0.9929]])\n",
      "for data: [0.2127411] we got output : tensor([[0.9612]])\n",
      "for data: [0.17035566] we got output : tensor([[0.9748]])\n",
      "for data: [0.67944047] we got output : tensor([[0.1299]])\n",
      "for data: [0.10993785] we got output : tensor([[0.9861]])\n",
      "for data: [0.92401462] we got output : tensor([[0.0206]])\n",
      "for data: [0.68136416] we got output : tensor([[0.1279]])\n",
      "for data: [0.88586463] we got output : tensor([[0.0264]])\n",
      "for data: [0.45821842] we got output : tensor([[0.6091]])\n",
      "for data: [0.89488913] we got output : tensor([[0.0248]])\n",
      "for data: [0.19764966] we got output : tensor([[0.9668]])\n",
      "for data: [0.91234666] we got output : tensor([[0.0222]])\n",
      "for data: [0.44179978] we got output : tensor([[0.6525]])\n",
      "for data: [0.37046694] we got output : tensor([[0.8094]])\n",
      "for data: [0.0734192] we got output : tensor([[0.9901]])\n",
      "for data: [0.04452367] we got output : tensor([[0.9924]])\n",
      "for data: [0.79629447] we got output : tensor([[0.0504]])\n",
      "for data: [0.86904079] we got output : tensor([[0.0296]])\n",
      "for data: [0.87758884] we got output : tensor([[0.0279]])\n",
      "for data: [0.59284416] we got output : tensor([[0.2625]])\n",
      "for data: [0.13977999] we got output : tensor([[0.9814]])\n",
      "for data: [0.79649915] we got output : tensor([[0.0504]])\n",
      "for data: [0.48087728] we got output : tensor([[0.5466]])\n",
      "for data: [0.06783402] we got output : tensor([[0.9906]])\n",
      "for data: [0.77588914] we got output : tensor([[0.0592]])\n",
      "for data: [0.54570123] we got output : tensor([[0.3704]])\n",
      "for data: [0.98671491] we got output : tensor([[0.0141]])\n",
      "for data: [0.80095945] we got output : tensor([[0.0487]])\n",
      "for data: [0.58210263] we got output : tensor([[0.2849]])\n",
      "for data: [0.51065021] we got output : tensor([[0.4637]])\n",
      "for data: [0.56115585] we got output : tensor([[0.3324]])\n",
      "for data: [0.52802301] we got output : tensor([[0.4164]])\n",
      "for data: [0.13812767] we got output : tensor([[0.9817]])\n",
      "for data: [0.10183991] we got output : tensor([[0.9871]])\n",
      "for data: [0.29538816] we got output : tensor([[0.9089]])\n",
      "for data: [0.78959697] we got output : tensor([[0.0532]])\n",
      "for data: [0.20153268] we got output : tensor([[0.9654]])\n",
      "for data: [0.8556761] we got output : tensor([[0.0325]])\n",
      "for data: [0.01668377] we got output : tensor([[0.9941]])\n",
      "for data: [0.59364588] we got output : tensor([[0.2608]])\n",
      "for data: [0.26472043] we got output : tensor([[0.9336]])\n",
      "for data: [0.22384565] we got output : tensor([[0.9565]])\n",
      "for data: [0.46490985] we got output : tensor([[0.5908]])\n",
      "for data: [0.66300152] we got output : tensor([[0.1490]])\n",
      "for data: [0.81830373] we got output : tensor([[0.0427]])\n",
      "for data: [0.85613517] we got output : tensor([[0.0324]])\n",
      "for data: [0.10703632] we got output : tensor([[0.9865]])\n",
      "for data: [0.90835957] we got output : tensor([[0.0227]])\n",
      "for data: [0.1233039] we got output : tensor([[0.9842]])\n",
      "for data: [0.5561282] we got output : tensor([[0.3445]])\n",
      "for data: [0.76724742] we got output : tensor([[0.0633]])\n",
      "for data: [0.50276545] we got output : tensor([[0.4855]])\n",
      "for data: [0.06165905] we got output : tensor([[0.9912]])\n",
      "for data: [0.87716351] we got output : tensor([[0.0280]])\n",
      "for data: [0.84013198] we got output : tensor([[0.0364]])\n",
      "for data: [0.85652074] we got output : tensor([[0.0323]])\n",
      "for data: [0.27544171] we got output : tensor([[0.9257]])\n",
      "for data: [0.60275482] we got output : tensor([[0.2430]])\n",
      "for data: [0.50498088] we got output : tensor([[0.4794]])\n",
      "for data: [0.23811002] we got output : tensor([[0.9495]])\n",
      "for data: [0.81159908] we got output : tensor([[0.0449]])\n",
      "for data: [0.38882659] we got output : tensor([[0.7748]])\n",
      "for data: [0.01982565] we got output : tensor([[0.9939]])\n",
      "for data: [0.82301209] we got output : tensor([[0.0412]])\n",
      "for data: [0.00996333] we got output : tensor([[0.9944]])\n",
      "for data: [0.12888895] we got output : tensor([[0.9833]])\n",
      "for data: [0.50846519] we got output : tensor([[0.4698]])\n",
      "for data: [0.33300108] we got output : tensor([[0.8669]])\n",
      "for data: [0.03939089] we got output : tensor([[0.9928]])\n",
      "for data: [0.05649576] we got output : tensor([[0.9916]])\n",
      "for data: [0.60478072] we got output : tensor([[0.2391]])\n",
      "for data: [0.60001295] we got output : tensor([[0.2482]])\n",
      "for data: [0.66408988] we got output : tensor([[0.1476]])\n",
      "for data: [0.27501861] we got output : tensor([[0.9261]])\n",
      "for data: [0.1481684] we got output : tensor([[0.9798]])\n",
      "for data: [0.9016636] we got output : tensor([[0.0238]])\n",
      "for data: [0.21488091] we got output : tensor([[0.9603]])\n",
      "for data: [0.63904299] we got output : tensor([[0.1815]])\n",
      "for data: [0.06631416] we got output : tensor([[0.9908]])\n",
      "for data: [0.99050496] we got output : tensor([[0.0138]])\n",
      "for data: [0.25247625] we got output : tensor([[0.9415]])\n",
      "for data: [0.38524175] we got output : tensor([[0.7818]])\n",
      "for data: [0.48960823] we got output : tensor([[0.5222]])\n",
      "for data: [0.26125081] we got output : tensor([[0.9359]])\n",
      "for data: [0.34891347] we got output : tensor([[0.8447]])\n",
      "for data: [0.3321141] we got output : tensor([[0.8681]])\n",
      "for data: [0.43176048] we got output : tensor([[0.6779]])\n",
      "for data: [0.7495808] we got output : tensor([[0.0729]])\n",
      "for data: [0.23949732] we got output : tensor([[0.9488]])\n",
      "for data: [0.37307141] we got output : tensor([[0.8048]])\n",
      "for data: [0.57729237] we got output : tensor([[0.2953]])\n",
      "for data: [0.24037052] we got output : tensor([[0.9484]])\n",
      "for data: [0.15018425] we got output : tensor([[0.9794]])\n",
      "for data: [0.78218388] we got output : tensor([[0.0563]])\n",
      "for data: [0.2913974] we got output : tensor([[0.9125]])\n",
      "for data: [0.5080742] we got output : tensor([[0.4707]])\n",
      "for data: [0.12981369] we got output : tensor([[0.9832]])\n",
      "for data: [0.44024588] we got output : tensor([[0.6565]])\n",
      "for data: [0.82797459] we got output : tensor([[0.0398]])\n",
      "for data: [0.97456378] we got output : tensor([[0.0152]])\n",
      "for data: [0.61204405] we got output : tensor([[0.2257]])\n",
      "for data: [0.2280363] we got output : tensor([[0.9546]])\n",
      "for data: [0.78676123] we got output : tensor([[0.0543]])\n",
      "for data: [0.92180012] we got output : tensor([[0.0209]])\n",
      "for data: [0.73882193] we got output : tensor([[0.0796]])\n",
      "for data: [0.4738559] we got output : tensor([[0.5662]])\n",
      "for data: [0.93579084] we got output : tensor([[0.0191]])\n",
      "for data: [0.29853629] we got output : tensor([[0.9059]])\n",
      "for data: [0.51213322] we got output : tensor([[0.4595]])\n",
      "for data: [0.50962171] we got output : tensor([[0.4665]])\n",
      "for data: [0.44358428] we got output : tensor([[0.6478]])\n",
      "for data: [0.45702176] we got output : tensor([[0.6122]])\n",
      "for data: [0.88880801] we got output : tensor([[0.0259]])\n",
      "for data: [0.24454435] we got output : tensor([[0.9461]])\n",
      "for data: [0.48764987] we got output : tensor([[0.5278]])\n",
      "for data: [0.67542843] we got output : tensor([[0.1343]])\n",
      "for data: [0.37197052] we got output : tensor([[0.8067]])\n",
      "for data: [0.62382198] we got output : tensor([[0.2054]])\n",
      "for data: [0.79047485] we got output : tensor([[0.0528]])\n",
      "for data: [0.54620323] we got output : tensor([[0.3692]])\n",
      "for data: [0.28482742] we got output : tensor([[0.9182]])\n",
      "for data: [0.5091118] we got output : tensor([[0.4680]])\n",
      "for data: [0.15007029] we got output : tensor([[0.9794]])\n",
      "for data: [0.43848935] we got output : tensor([[0.6611]])\n",
      "for data: [0.86510928] we got output : tensor([[0.0304]])\n",
      "for data: [0.17097792] we got output : tensor([[0.9747]])\n",
      "for data: [0.11075683] we got output : tensor([[0.9860]])\n",
      "for data: [0.01467352] we got output : tensor([[0.9942]])\n",
      "for data: [0.2759088] we got output : tensor([[0.9254]])\n",
      "for data: [0.34933927] we got output : tensor([[0.8439]])\n",
      "for data: [0.06454234] we got output : tensor([[0.9909]])\n",
      "for data: [0.69308475] we got output : tensor([[0.1160]])\n",
      "for data: [0.38074719] we got output : tensor([[0.7906]])\n",
      "for data: [0.35661015] we got output : tensor([[0.8326]])\n",
      "for data: [0.84738509] we got output : tensor([[0.0345]])\n",
      "for data: [0.49177002] we got output : tensor([[0.5162]])\n",
      "for data: [0.5802152] we got output : tensor([[0.2889]])\n",
      "for data: [0.26016719] we got output : tensor([[0.9366]])\n",
      "for data: [0.0259484] we got output : tensor([[0.9936]])\n",
      "for data: [0.37302593] we got output : tensor([[0.8049]])\n",
      "for data: [0.99397074] we got output : tensor([[0.0136]])\n",
      "for data: [0.74582005] we got output : tensor([[0.0752]])\n",
      "for data: [0.78468272] we got output : tensor([[0.0552]])\n",
      "for data: [0.66374975] we got output : tensor([[0.1480]])\n",
      "for data: [0.4924556] we got output : tensor([[0.5142]])\n",
      "for data: [0.13735431] we got output : tensor([[0.9819]])\n",
      "for data: [0.76233756] we got output : tensor([[0.0659]])\n",
      "for data: [0.0511517] we got output : tensor([[0.9919]])\n",
      "for data: [0.87631341] we got output : tensor([[0.0282]])\n",
      "for data: [0.13361896] we got output : tensor([[0.9825]])\n",
      "for data: [0.92974222] we got output : tensor([[0.0198]])\n",
      "for data: [0.81354698] we got output : tensor([[0.0443]])\n",
      "for data: [0.67401386] we got output : tensor([[0.1360]])\n",
      "for data: [0.64917698] we got output : tensor([[0.1670]])\n",
      "for data: [0.83751307] we got output : tensor([[0.0371]])\n",
      "for data: [0.18573643] we got output : tensor([[0.9706]])\n",
      "for data: [0.57521763] we got output : tensor([[0.2999]])\n",
      "for data: [0.98139815] we got output : tensor([[0.0146]])\n",
      "for data: [0.40161855] we got output : tensor([[0.7481]])\n",
      "for data: [0.99905695] we got output : tensor([[0.0132]])\n",
      "for data: [0.29145705] we got output : tensor([[0.9125]])\n",
      "for data: [0.40137871] we got output : tensor([[0.7488]])\n",
      "for data: [0.75122654] we got output : tensor([[0.0720]])\n",
      "for data: [0.03403756] we got output : tensor([[0.9931]])\n",
      "for data: [0.38825456] we got output : tensor([[0.7759]])\n",
      "for data: [0.62290705] we got output : tensor([[0.2069]])\n",
      "for data: [0.31167488] we got output : tensor([[0.8925]])\n",
      "for data: [0.98619629] we got output : tensor([[0.0142]])\n",
      "for data: [0.9864594] we got output : tensor([[0.0142]])\n",
      "for data: [0.88001642] we got output : tensor([[0.0275]])\n",
      "for data: [0.73262429] we got output : tensor([[0.0837]])\n",
      "for data: [0.92625488] we got output : tensor([[0.0203]])\n",
      "for data: [0.53565454] we got output : tensor([[0.3964]])\n",
      "for data: [0.46149485] we got output : tensor([[0.6002]])\n",
      "for data: [0.86309427] we got output : tensor([[0.0309]])\n",
      "for data: [0.36430179] we got output : tensor([[0.8200]])\n",
      "for data: [0.12629606] we got output : tensor([[0.9837]])\n",
      "for data: [0.31000248] we got output : tensor([[0.8943]])\n",
      "for data: [0.15677931] we got output : tensor([[0.9780]])\n",
      "for data: [0.44672934] we got output : tensor([[0.6396]])\n",
      "for data: [0.8393946] we got output : tensor([[0.0366]])\n",
      "for data: [0.80265553] we got output : tensor([[0.0481]])\n",
      "for data: [0.34290979] we got output : tensor([[0.8534]])\n",
      "for data: [0.54919652] we got output : tensor([[0.3616]])\n",
      "for data: [0.37933316] we got output : tensor([[0.7932]])\n",
      "for data: [0.5226726] we got output : tensor([[0.4308]])\n",
      "for data: [0.92742654] we got output : tensor([[0.0201]])\n",
      "for data: [0.00509292] we got output : tensor([[0.9946]])\n",
      "for data: [0.91676383] we got output : tensor([[0.0215]])\n",
      "for data: [0.04151171] we got output : tensor([[0.9926]])\n",
      "for data: [0.36382618] we got output : tensor([[0.8209]])\n",
      "for data: [0.23809273] we got output : tensor([[0.9495]])\n",
      "for data: [0.49218507] we got output : tensor([[0.5150]])\n",
      "for data: [0.15325246] we got output : tensor([[0.9788]])\n",
      "for data: [0.41064585] we got output : tensor([[0.7282]])\n",
      "for data: [0.43857728] we got output : tensor([[0.6607]])\n",
      "for data: [0.13438916] we got output : tensor([[0.9824]])\n",
      "for data: [0.72476424] we got output : tensor([[0.0893]])\n",
      "for data: [0.9125007] we got output : tensor([[0.0221]])\n",
      "for data: [0.49324465] we got output : tensor([[0.5122]])\n",
      "for data: [0.3258006] we got output : tensor([[0.8761]])\n",
      "for data: [0.33077827] we got output : tensor([[0.8698]])\n",
      "for data: [0.62815329] we got output : tensor([[0.1983]])\n",
      "for data: [0.79290453] we got output : tensor([[0.0518]])\n",
      "for data: [0.68697613] we got output : tensor([[0.1220]])\n",
      "for data: [0.49292705] we got output : tensor([[0.5129]])\n",
      "for data: [0.53667697] we got output : tensor([[0.3936]])\n",
      "for data: [0.50141687] we got output : tensor([[0.4894]])\n",
      "for data: [0.62112752] we got output : tensor([[0.2100]])\n",
      "for data: [0.22929682] we got output : tensor([[0.9539]])\n",
      "for data: [0.34467208] we got output : tensor([[0.8508]])\n",
      "for data: [0.70346048] we got output : tensor([[0.1064]])\n",
      "for data: [0.84434879] we got output : tensor([[0.0353]])\n",
      "for data: [0.81526784] we got output : tensor([[0.0437]])\n",
      "for data: [0.90191633] we got output : tensor([[0.0237]])\n",
      "for data: [0.85576883] we got output : tensor([[0.0325]])\n",
      "for data: [0.04753524] we got output : tensor([[0.9922]])\n",
      "for data: [0.21580581] we got output : tensor([[0.9599]])\n",
      "for data: [0.56806423] we got output : tensor([[0.3162]])\n",
      "for data: [0.98445812] we got output : tensor([[0.0143]])\n",
      "for data: [0.72474199] we got output : tensor([[0.0893]])\n",
      "for data: [0.6100775] we got output : tensor([[0.2293]])\n",
      "for data: [0.98308438] we got output : tensor([[0.0144]])\n",
      "for data: [0.61953085] we got output : tensor([[0.2127]])\n",
      "for data: [0.43241742] we got output : tensor([[0.6763]])\n",
      "for data: [0.05559911] we got output : tensor([[0.9916]])\n",
      "for data: [0.10497992] we got output : tensor([[0.9867]])\n",
      "for data: [0.48255837] we got output : tensor([[0.5420]])\n",
      "for data: [0.34357418] we got output : tensor([[0.8524]])\n",
      "for data: [0.86530362] we got output : tensor([[0.0304]])\n",
      "for data: [0.98978003] we got output : tensor([[0.0139]])\n",
      "for data: [0.51748314] we got output : tensor([[0.4449]])\n",
      "for data: [0.1512999] we got output : tensor([[0.9792]])\n",
      "for data: [0.23284239] we got output : tensor([[0.9522]])\n",
      "for data: [0.00625035] we got output : tensor([[0.9946]])\n",
      "for data: [0.76348065] we got output : tensor([[0.0653]])\n",
      "for data: [0.4953859] we got output : tensor([[0.5061]])\n",
      "for data: [0.65961358] we got output : tensor([[0.1532]])\n",
      "for data: [0.05192604] we got output : tensor([[0.9919]])\n",
      "for data: [0.35742465] we got output : tensor([[0.8314]])\n",
      "for data: [0.8132264] we got output : tensor([[0.0444]])\n",
      "for data: [0.96357858] we got output : tensor([[0.0161]])\n",
      "for data: [0.01146719] we got output : tensor([[0.9943]])\n",
      "for data: [0.94574237] we got output : tensor([[0.0180]])\n",
      "for data: [0.67640526] we got output : tensor([[0.1333]])\n",
      "for data: [0.35165361] we got output : tensor([[0.8404]])\n",
      "for data: [0.20910587] we got output : tensor([[0.9626]])\n",
      "for data: [0.55404529] we got output : tensor([[0.3497]])\n",
      "for data: [0.84392379] we got output : tensor([[0.0354]])\n",
      "for data: [0.91968514] we got output : tensor([[0.0211]])\n",
      "for data: [0.98032406] we got output : tensor([[0.0147]])\n",
      "for data: [0.8729177] we got output : tensor([[0.0288]])\n",
      "for data: [0.88892531] we got output : tensor([[0.0258]])\n",
      "for data: [0.546291] we got output : tensor([[0.3690]])\n",
      "for data: [0.27158048] we got output : tensor([[0.9287]])\n",
      "for data: [0.22664883] we got output : tensor([[0.9552]])\n",
      "for data: [0.73752206] we got output : tensor([[0.0805]])\n",
      "for data: [0.38040198] we got output : tensor([[0.7912]])\n",
      "for data: [0.78572745] we got output : tensor([[0.0548]])\n",
      "for data: [0.31749142] we got output : tensor([[0.8860]])\n",
      "for data: [0.44697492] we got output : tensor([[0.6389]])\n",
      "for data: [0.43401335] we got output : tensor([[0.6723]])\n",
      "for data: [0.08738098] we got output : tensor([[0.9888]])\n",
      "for data: [0.9569739] we got output : tensor([[0.0168]])\n",
      "for data: [0.9531887] we got output : tensor([[0.0172]])\n",
      "for data: [0.02795919] we got output : tensor([[0.9934]])\n",
      "for data: [0.14253269] we got output : tensor([[0.9809]])\n",
      "for data: [0.02769123] we got output : tensor([[0.9935]])\n",
      "for data: [0.61990255] we got output : tensor([[0.2120]])\n",
      "for data: [0.10153623] we got output : tensor([[0.9872]])\n",
      "for data: [0.40501413] we got output : tensor([[0.7408]])\n",
      "for data: [0.63075526] we got output : tensor([[0.1941]])\n",
      "for data: [0.36323812] we got output : tensor([[0.8218]])\n",
      "for data: [0.25330221] we got output : tensor([[0.9410]])\n",
      "for data: [0.28447899] we got output : tensor([[0.9185]])\n",
      "for data: [0.25403685] we got output : tensor([[0.9405]])\n",
      "for data: [0.44358432] we got output : tensor([[0.6479]])\n",
      "for data: [0.33283353] we got output : tensor([[0.8672]])\n",
      "for data: [0.70829958] we got output : tensor([[0.1023]])\n",
      "for data: [0.73888499] we got output : tensor([[0.0796]])\n",
      "for data: [0.66492058] we got output : tensor([[0.1466]])\n",
      "for data: [0.5346986] we got output : tensor([[0.3987]])\n",
      "for data: [0.43623798] we got output : tensor([[0.6666]])\n",
      "for data: [0.18210571] we got output : tensor([[0.9716]])\n",
      "for data: [0.20663375] we got output : tensor([[0.9635]])\n",
      "for data: [0.22183764] we got output : tensor([[0.9574]])\n",
      "for data: [0.3333362] we got output : tensor([[0.8665]])\n",
      "for data: [0.89888683] we got output : tensor([[0.0242]])\n",
      "for data: [0.37236068] we got output : tensor([[0.8060]])\n",
      "for data: [0.42911336] we got output : tensor([[0.6844]])\n",
      "for data: [0.02050973] we got output : tensor([[0.9938]])\n",
      "for data: [0.87497104] we got output : tensor([[0.0284]])\n",
      "for data: [0.47855271] we got output : tensor([[0.5531]])\n",
      "for data: [0.02002926] we got output : tensor([[0.9939]])\n",
      "for data: [0.44673185] we got output : tensor([[0.6397]])\n",
      "for data: [0.85822474] we got output : tensor([[0.0319]])\n",
      "for data: [0.98607367] we got output : tensor([[0.0142]])\n",
      "for data: [0.97139922] we got output : tensor([[0.0154]])\n",
      "for data: [0.36337769] we got output : tensor([[0.8216]])\n",
      "for data: [0.75622235] we got output : tensor([[0.0692]])\n",
      "for data: [0.59863138] we got output : tensor([[0.2509]])\n",
      "for data: [0.3537495] we got output : tensor([[0.8372]])\n",
      "for data: [0.63949767] we got output : tensor([[0.1808]])\n",
      "for data: [0.13431887] we got output : tensor([[0.9824]])\n",
      "for data: [0.71463233] we got output : tensor([[0.0971]])\n",
      "for data: [0.89083545] we got output : tensor([[0.0255]])\n",
      "for data: [0.94041159] we got output : tensor([[0.0186]])\n",
      "for data: [0.3771531] we got output : tensor([[0.7973]])\n",
      "for data: [0.43497359] we got output : tensor([[0.6698]])\n",
      "for data: [0.53969227] we got output : tensor([[0.3857]])\n",
      "for data: [0.10793794] we got output : tensor([[0.9864]])\n",
      "for data: [0.43772975] we got output : tensor([[0.6629]])\n",
      "for data: [0.32713968] we got output : tensor([[0.8744]])\n",
      "for data: [0.03024402] we got output : tensor([[0.9933]])\n",
      "for data: [0.74100761] we got output : tensor([[0.0782]])\n",
      "for data: [0.27691218] we got output : tensor([[0.9246]])\n",
      "for data: [0.4239146] we got output : tensor([[0.6972]])\n",
      "for data: [0.93110555] we got output : tensor([[0.0197]])\n",
      "for data: [0.34376801] we got output : tensor([[0.8522]])\n",
      "for data: [0.10754096] we got output : tensor([[0.9864]])\n",
      "for data: [0.61186833] we got output : tensor([[0.2261]])\n",
      "for data: [0.94511246] we got output : tensor([[0.0180]])\n",
      "for data: [0.13406148] we got output : tensor([[0.9824]])\n",
      "for data: [0.77469948] we got output : tensor([[0.0597]])\n",
      "for data: [0.03039991] we got output : tensor([[0.9933]])\n",
      "for data: [0.25491421] we got output : tensor([[0.9400]])\n",
      "for data: [0.56014944] we got output : tensor([[0.3347]])\n",
      "for data: [0.04122864] we got output : tensor([[0.9926]])\n",
      "for data: [0.8749067] we got output : tensor([[0.0284]])\n",
      "for data: [0.86071618] we got output : tensor([[0.0314]])\n",
      "for data: [0.65165996] we got output : tensor([[0.1636]])\n",
      "for data: [0.4644618] we got output : tensor([[0.5919]])\n",
      "for data: [0.45994927] we got output : tensor([[0.6043]])\n",
      "for data: [0.29610295] we got output : tensor([[0.9082]])\n",
      "for data: [0.61496356] we got output : tensor([[0.2205]])\n",
      "for data: [0.09376649] we got output : tensor([[0.9881]])\n",
      "for data: [0.50871185] we got output : tensor([[0.4691]])\n",
      "for data: [0.75455357] we got output : tensor([[0.0701]])\n",
      "for data: [0.75936858] we got output : tensor([[0.0675]])\n",
      "for data: [0.64843146] we got output : tensor([[0.1681]])\n",
      "for data: [0.54764306] we got output : tensor([[0.3656]])\n",
      "for data: [0.68612263] we got output : tensor([[0.1229]])\n",
      "for data: [0.33348769] we got output : tensor([[0.8663]])\n",
      "for data: [0.57987314] we got output : tensor([[0.2897]])\n",
      "for data: [0.61353742] we got output : tensor([[0.2230]])\n",
      "for data: [0.95729683] we got output : tensor([[0.0168]])\n",
      "for data: [0.05202936] we got output : tensor([[0.9919]])\n",
      "for data: [0.50889862] we got output : tensor([[0.4685]])\n",
      "for data: [0.93686054] we got output : tensor([[0.0190]])\n",
      "for data: [0.17668968] we got output : tensor([[0.9731]])\n",
      "for data: [0.57646515] we got output : tensor([[0.2971]])\n",
      "for data: [0.02558624] we got output : tensor([[0.9936]])\n",
      "for data: [0.28210054] we got output : tensor([[0.9205]])\n",
      "for data: [0.40645187] we got output : tensor([[0.7376]])\n",
      "for data: [0.62585183] we got output : tensor([[0.2021]])\n",
      "for data: [0.99719979] we got output : tensor([[0.0133]])\n",
      "for data: [0.90391327] we got output : tensor([[0.0234]])\n",
      "for data: [0.23704142] we got output : tensor([[0.9501]])\n",
      "for data: [0.99874985] we got output : tensor([[0.0132]])\n",
      "for data: [0.46106244] we got output : tensor([[0.6013]])\n",
      "for data: [0.04034583] we got output : tensor([[0.9927]])\n",
      "for data: [0.3458354] we got output : tensor([[0.8492]])\n",
      "for data: [0.96598389] we got output : tensor([[0.0159]])\n",
      "for data: [0.70081753] we got output : tensor([[0.1088]])\n",
      "for data: [0.02845808] we got output : tensor([[0.9934]])\n",
      "for data: [0.20405334] we got output : tensor([[0.9645]])\n",
      "for data: [0.97358972] we got output : tensor([[0.0152]])\n",
      "for data: [0.74785292] we got output : tensor([[0.0740]])\n",
      "for data: [0.91934128] we got output : tensor([[0.0212]])\n",
      "for data: [0.17462982] we got output : tensor([[0.9737]])\n",
      "for data: [0.22546899] we got output : tensor([[0.9557]])\n",
      "for data: [0.24014028] we got output : tensor([[0.9485]])\n",
      "for data: [0.05549196] we got output : tensor([[0.9916]])\n",
      "for data: [0.07702546] we got output : tensor([[0.9898]])\n",
      "for data: [0.53839551] we got output : tensor([[0.3892]])\n",
      "for data: [0.8528464] we got output : tensor([[0.0332]])\n",
      "for data: [0.52088388] we got output : tensor([[0.4356]])\n",
      "for data: [0.46007229] we got output : tensor([[0.6039]])\n",
      "for data: [0.196447] we got output : tensor([[0.9672]])\n",
      "for data: [0.74277133] we got output : tensor([[0.0771]])\n",
      "for data: [0.55228278] we got output : tensor([[0.3540]])\n",
      "for data: [0.69964411] we got output : tensor([[0.1099]])\n",
      "for data: [0.92964226] we got output : tensor([[0.0198]])\n",
      "for data: [0.0936146] we got output : tensor([[0.9881]])\n",
      "for data: [0.27406509] we got output : tensor([[0.9268]])\n",
      "for data: [0.41164621] we got output : tensor([[0.7260]])\n",
      "for data: [0.45276725] we got output : tensor([[0.6236]])\n",
      "for data: [0.55868805] we got output : tensor([[0.3383]])\n",
      "for data: [0.45540141] we got output : tensor([[0.6167]])\n",
      "for data: [0.83337826] we got output : tensor([[0.0382]])\n",
      "for data: [0.67275761] we got output : tensor([[0.1373]])\n",
      "for data: [0.58869789] we got output : tensor([[0.2709]])\n",
      "for data: [0.74912795] we got output : tensor([[0.0733]])\n",
      "for data: [0.05102028] we got output : tensor([[0.9920]])\n",
      "for data: [0.38834815] we got output : tensor([[0.7758]])\n",
      "for data: [0.20216479] we got output : tensor([[0.9652]])\n",
      "for data: [0.02374476] we got output : tensor([[0.9937]])\n",
      "for data: [0.74121666] we got output : tensor([[0.0781]])\n",
      "for data: [0.03102493] we got output : tensor([[0.9933]])\n",
      "for data: [0.72558065] we got output : tensor([[0.0887]])\n",
      "for data: [0.99285168] we got output : tensor([[0.0137]])\n",
      "for data: [0.36013284] we got output : tensor([[0.8270]])\n",
      "for data: [0.60589654] we got output : tensor([[0.2370]])\n",
      "for data: [0.22516413] we got output : tensor([[0.9559]])\n",
      "for data: [0.41392935] we got output : tensor([[0.7207]])\n",
      "for data: [0.38723149] we got output : tensor([[0.7780]])\n",
      "for data: [0.46455062] we got output : tensor([[0.5918]])\n",
      "for data: [0.6505935] we got output : tensor([[0.1650]])\n",
      "for data: [0.45757571] we got output : tensor([[0.6107]])\n",
      "for data: [0.38773708] we got output : tensor([[0.7770]])\n",
      "for data: [0.90052023] we got output : tensor([[0.0239]])\n",
      "for data: [0.57427663] we got output : tensor([[0.3021]])\n",
      "for data: [0.76093904] we got output : tensor([[0.0666]])\n",
      "for data: [0.37927732] we got output : tensor([[0.7934]])\n",
      "for data: [0.64958956] we got output : tensor([[0.1664]])\n",
      "for data: [0.65564093] we got output : tensor([[0.1583]])\n",
      "for data: [0.88576556] we got output : tensor([[0.0264]])\n",
      "for data: [0.05534608] we got output : tensor([[0.9916]])\n",
      "for data: [0.38862836] we got output : tensor([[0.7753]])\n",
      "for data: [0.05910368] we got output : tensor([[0.9914]])\n",
      "for data: [0.23690711] we got output : tensor([[0.9502]])\n",
      "for data: [0.8486816] we got output : tensor([[0.0342]])\n",
      "for data: [0.32094502] we got output : tensor([[0.8820]])\n",
      "for data: [0.0336666] we got output : tensor([[0.9931]])\n",
      "for data: [0.35374516] we got output : tensor([[0.8372]])\n",
      "for data: [0.87663124] we got output : tensor([[0.0281]])\n",
      "for data: [0.2584433] we got output : tensor([[0.9377]])\n",
      "for data: [0.86193082] we got output : tensor([[0.0311]])\n",
      "for data: [0.47907719] we got output : tensor([[0.5516]])\n",
      "for data: [0.47450295] we got output : tensor([[0.5644]])\n",
      "for data: [0.10804958] we got output : tensor([[0.9864]])\n",
      "for data: [0.38927868] we got output : tensor([[0.7739]])\n",
      "for data: [0.67941417] we got output : tensor([[0.1300]])\n",
      "for data: [0.63892005] we got output : tensor([[0.1817]])\n",
      "for data: [0.93212063] we got output : tensor([[0.0195]])\n",
      "for data: [0.28641788] we got output : tensor([[0.9169]])\n",
      "for data: [0.1153947] we got output : tensor([[0.9854]])\n",
      "for data: [0.00752276] we got output : tensor([[0.9945]])\n",
      "for data: [0.41808006] we got output : tensor([[0.7111]])\n",
      "for data: [0.03260387] we got output : tensor([[0.9932]])\n",
      "for data: [0.80749137] we got output : tensor([[0.0463]])\n",
      "for data: [0.2330089] we got output : tensor([[0.9521]])\n",
      "for data: [0.63576456] we got output : tensor([[0.1865]])\n",
      "for data: [0.68194188] we got output : tensor([[0.1273]])\n",
      "for data: [0.4466944] we got output : tensor([[0.6396]])\n",
      "for data: [0.58643031] we got output : tensor([[0.2756]])\n",
      "for data: [0.98224923] we got output : tensor([[0.0145]])\n",
      "for data: [0.74316059] we got output : tensor([[0.0768]])\n",
      "for data: [0.4752028] we got output : tensor([[0.5624]])\n",
      "for data: [0.50231658] we got output : tensor([[0.4867]])\n",
      "for data: [0.11469841] we got output : tensor([[0.9855]])\n",
      "for data: [0.09512961] we got output : tensor([[0.9879]])\n",
      "for data: [0.33594085] we got output : tensor([[0.8630]])\n",
      "for data: [0.84822684] we got output : tensor([[0.0343]])\n",
      "for data: [0.66951949] we got output : tensor([[0.1411]])\n",
      "for data: [0.11395606] we got output : tensor([[0.9856]])\n",
      "for data: [0.4521109] we got output : tensor([[0.6254]])\n",
      "for data: [0.22794932] we got output : tensor([[0.9546]])\n",
      "for data: [0.72391079] we got output : tensor([[0.0899]])\n",
      "for data: [0.73391162] we got output : tensor([[0.0828]])\n",
      "for data: [0.34807578] we got output : tensor([[0.8458]])\n",
      "for data: [0.30574136] we got output : tensor([[0.8987]])\n",
      "for data: [0.00706519] we got output : tensor([[0.9945]])\n",
      "for data: [0.33537876] we got output : tensor([[0.8638]])\n",
      "for data: [0.3824022] we got output : tensor([[0.7875]])\n",
      "for data: [0.15421429] we got output : tensor([[0.9786]])\n",
      "for data: [0.03268521] we got output : tensor([[0.9932]])\n",
      "for data: [0.79155569] we got output : tensor([[0.0524]])\n",
      "for data: [0.01101274] we got output : tensor([[0.9943]])\n",
      "for data: [0.44804916] we got output : tensor([[0.6362]])\n",
      "for data: [0.18292898] we got output : tensor([[0.9714]])\n",
      "for data: [0.42475288] we got output : tensor([[0.6952]])\n",
      "for data: [0.21818371] we got output : tensor([[0.9590]])\n",
      "for data: [0.94909924] we got output : tensor([[0.0176]])\n",
      "for data: [0.09077982] we got output : tensor([[0.9884]])\n",
      "for data: [0.59327394] we got output : tensor([[0.2616]])\n",
      "for data: [0.79671801] we got output : tensor([[0.0503]])\n",
      "for data: [0.42972105] we got output : tensor([[0.6829]])\n",
      "for data: [0.61338063] we got output : tensor([[0.2234]])\n",
      "for data: [0.91821933] we got output : tensor([[0.0213]])\n",
      "for data: [0.53450069] we got output : tensor([[0.3993]])\n",
      "for data: [0.19174292] we got output : tensor([[0.9687]])\n",
      "for data: [0.50660617] we got output : tensor([[0.4747]])\n",
      "for data: [0.61766589] we got output : tensor([[0.2158]])\n",
      "for data: [0.86742] we got output : tensor([[0.0300]])\n",
      "for data: [0.05352817] we got output : tensor([[0.9918]])\n",
      "for data: [0.83826073] we got output : tensor([[0.0369]])\n",
      "for data: [0.91491426] we got output : tensor([[0.0218]])\n",
      "for data: [0.57763947] we got output : tensor([[0.2946]])\n",
      "for data: [0.84815689] we got output : tensor([[0.0343]])\n",
      "for data: [0.34716635] we got output : tensor([[0.8472]])\n",
      "for data: [0.25290641] we got output : tensor([[0.9412]])\n",
      "for data: [0.89481408] we got output : tensor([[0.0249]])\n",
      "for data: [0.37026343] we got output : tensor([[0.8098]])\n",
      "for data: [0.74032905] we got output : tensor([[0.0786]])\n",
      "for data: [0.63033422] we got output : tensor([[0.1948]])\n",
      "for data: [0.13017473] we got output : tensor([[0.9831]])\n",
      "for data: [0.32135887] we got output : tensor([[0.8815]])\n",
      "for data: [0.34639166] we got output : tensor([[0.8484]])\n",
      "for data: [0.96723599] we got output : tensor([[0.0158]])\n",
      "for data: [0.81238586] we got output : tensor([[0.0447]])\n",
      "for data: [0.57222216] we got output : tensor([[0.3066]])\n",
      "for data: [0.85084314] we got output : tensor([[0.0337]])\n",
      "for data: [0.27476137] we got output : tensor([[0.9263]])\n",
      "for data: [0.73810736] we got output : tensor([[0.0801]])\n",
      "for data: [0.6762691] we got output : tensor([[0.1334]])\n",
      "for data: [0.25627833] we got output : tensor([[0.9391]])\n",
      "for data: [0.14406755] we got output : tensor([[0.9806]])\n",
      "for data: [0.78763697] we got output : tensor([[0.0540]])\n",
      "for data: [0.66929978] we got output : tensor([[0.1414]])\n",
      "for data: [0.28507335] we got output : tensor([[0.9180]])\n",
      "for data: [0.05815607] we got output : tensor([[0.9914]])\n",
      "for data: [0.98091014] we got output : tensor([[0.0146]])\n",
      "for data: [0.40617908] we got output : tensor([[0.7381]])\n",
      "for data: [0.32320515] we got output : tensor([[0.8793]])\n",
      "for data: [0.20239274] we got output : tensor([[0.9651]])\n",
      "for data: [0.2469927] we got output : tensor([[0.9447]])\n",
      "for data: [0.72204144] we got output : tensor([[0.0913]])\n",
      "for data: [0.35753685] we got output : tensor([[0.8312]])\n",
      "for data: [0.48211157] we got output : tensor([[0.5432]])\n",
      "for data: [0.76369226] we got output : tensor([[0.0652]])\n",
      "for data: [0.55706405] we got output : tensor([[0.3423]])\n",
      "for data: [0.81719062] we got output : tensor([[0.0431]])\n",
      "for data: [0.46821843] we got output : tensor([[0.5817]])\n",
      "for data: [0.09827254] we got output : tensor([[0.9876]])\n",
      "for data: [0.98324941] we got output : tensor([[0.0144]])\n",
      "for data: [0.27745748] we got output : tensor([[0.9242]])\n",
      "for data: [0.69766678] we got output : tensor([[0.1117]])\n",
      "for data: [0.34175393] we got output : tensor([[0.8550]])\n",
      "for data: [0.34111346] we got output : tensor([[0.8560]])\n",
      "for data: [0.30988062] we got output : tensor([[0.8944]])\n",
      "for data: [0.50569082] we got output : tensor([[0.4773]])\n",
      "for data: [0.57402095] we got output : tensor([[0.3026]])\n",
      "for data: [0.90339033] we got output : tensor([[0.0235]])\n",
      "for data: [0.7884485] we got output : tensor([[0.0536]])\n",
      "for data: [0.89911056] we got output : tensor([[0.0242]])\n",
      "for data: [0.39118972] we got output : tensor([[0.7701]])\n",
      "for data: [0.6224925] we got output : tensor([[0.2076]])\n",
      "for data: [0.06608388] we got output : tensor([[0.9908]])\n",
      "for data: [0.10885682] we got output : tensor([[0.9862]])\n",
      "for data: [0.83847728] we got output : tensor([[0.0368]])\n",
      "for data: [0.05408654] we got output : tensor([[0.9917]])\n",
      "for data: [0.32154732] we got output : tensor([[0.8812]])\n",
      "for data: [0.90813869] we got output : tensor([[0.0228]])\n",
      "for data: [0.485136] we got output : tensor([[0.5347]])\n",
      "for data: [0.28207035] we got output : tensor([[0.9205]])\n",
      "for data: [0.50029412] we got output : tensor([[0.4924]])\n",
      "for data: [0.66627138] we got output : tensor([[0.1449]])\n",
      "for data: [0.0515196] we got output : tensor([[0.9919]])\n",
      "for data: [0.22703099] we got output : tensor([[0.9550]])\n",
      "for data: [0.60655656] we got output : tensor([[0.2357]])\n",
      "for data: [0.95722721] we got output : tensor([[0.0168]])\n",
      "for data: [0.64354992] we got output : tensor([[0.1749]])\n",
      "for data: [0.01633129] we got output : tensor([[0.9941]])\n",
      "for data: [0.89982372] we got output : tensor([[0.0240]])\n",
      "for data: [0.32460135] we got output : tensor([[0.8776]])\n",
      "for data: [0.07735767] we got output : tensor([[0.9898]])\n",
      "for data: [0.26831164] we got output : tensor([[0.9310]])\n",
      "for data: [0.00345598] we got output : tensor([[0.9947]])\n",
      "for data: [0.32526466] we got output : tensor([[0.8768]])\n",
      "for data: [0.22915046] we got output : tensor([[0.9540]])\n",
      "for data: [0.58101418] we got output : tensor([[0.2872]])\n",
      "for data: [0.95920252] we got output : tensor([[0.0166]])\n",
      "for data: [0.43933637] we got output : tensor([[0.6587]])\n",
      "for data: [0.93743346] we got output : tensor([[0.0189]])\n",
      "for data: [0.11503005] we got output : tensor([[0.9854]])\n",
      "for data: [0.68987695] we got output : tensor([[0.1191]])\n",
      "for data: [0.39917834] we got output : tensor([[0.7534]])\n",
      "for data: [0.60699483] we got output : tensor([[0.2349]])\n",
      "for data: [0.72267094] we got output : tensor([[0.0908]])\n",
      "for data: [0.67979899] we got output : tensor([[0.1295]])\n",
      "for data: [0.92336928] we got output : tensor([[0.0206]])\n",
      "for data: [0.82365519] we got output : tensor([[0.0411]])\n",
      "for data: [0.47088032] we got output : tensor([[0.5744]])\n",
      "for data: [0.13355808] we got output : tensor([[0.9825]])\n",
      "for data: [0.63424429] we got output : tensor([[0.1887]])\n",
      "for data: [0.05593867] we got output : tensor([[0.9916]])\n",
      "for data: [0.44260953] we got output : tensor([[0.6504]])\n",
      "for data: [0.98276378] we got output : tensor([[0.0145]])\n",
      "for data: [0.79303405] we got output : tensor([[0.0517]])\n",
      "for data: [0.64739469] we got output : tensor([[0.1695]])\n",
      "for data: [0.96877559] we got output : tensor([[0.0157]])\n",
      "for data: [0.00057492] we got output : tensor([[0.9948]])\n",
      "for data: [0.28256762] we got output : tensor([[0.9201]])\n",
      "for data: [0.06713363] we got output : tensor([[0.9907]])\n",
      "for data: [0.93745279] we got output : tensor([[0.0189]])\n",
      "for data: [0.05855619] we got output : tensor([[0.9914]])\n",
      "for data: [0.12762847] we got output : tensor([[0.9835]])\n",
      "for data: [0.20302854] we got output : tensor([[0.9649]])\n",
      "for data: [0.15621561] we got output : tensor([[0.9782]])\n",
      "for data: [0.59389389] we got output : tensor([[0.2603]])\n",
      "for data: [0.70061659] we got output : tensor([[0.1090]])\n",
      "for data: [0.42950393] we got output : tensor([[0.6836]])\n",
      "for data: [0.74372494] we got output : tensor([[0.0765]])\n",
      "for data: [0.49591657] we got output : tensor([[0.5047]])\n",
      "for data: [0.2830963] we got output : tensor([[0.9197]])\n",
      "for data: [0.06183463] we got output : tensor([[0.9911]])\n",
      "for data: [0.20295781] we got output : tensor([[0.9649]])\n",
      "for data: [0.34172238] we got output : tensor([[0.8550]])\n",
      "for data: [0.5303885] we got output : tensor([[0.4103]])\n",
      "for data: [0.30530714] we got output : tensor([[0.8992]])\n",
      "for data: [0.86923594] we got output : tensor([[0.0296]])\n",
      "for data: [0.88028715] we got output : tensor([[0.0274]])\n",
      "for data: [0.80852819] we got output : tensor([[0.0460]])\n",
      "for data: [0.45876337] we got output : tensor([[0.6075]])\n",
      "for data: [0.27880783] we got output : tensor([[0.9231]])\n",
      "for data: [0.92491213] we got output : tensor([[0.0204]])\n",
      "for data: [0.32514701] we got output : tensor([[0.8769]])\n",
      "for data: [0.04660392] we got output : tensor([[0.9923]])\n",
      "for data: [0.99751146] we got output : tensor([[0.0133]])\n",
      "for data: [0.28964065] we got output : tensor([[0.9141]])\n",
      "for data: [0.25380394] we got output : tensor([[0.9406]])\n",
      "for data: [0.84028769] we got output : tensor([[0.0363]])\n",
      "for data: [0.41147452] we got output : tensor([[0.7263]])\n",
      "for data: [0.02309604] we got output : tensor([[0.9937]])\n",
      "for data: [0.49855459] we got output : tensor([[0.4972]])\n",
      "for data: [0.88826208] we got output : tensor([[0.0260]])\n",
      "for data: [0.52307927] we got output : tensor([[0.4297]])\n",
      "for data: [0.38881858] we got output : tensor([[0.7748]])\n",
      "for data: [0.30135659] we got output : tensor([[0.9032]])\n",
      "for data: [0.64790222] we got output : tensor([[0.1687]])\n",
      "for data: [0.35022007] we got output : tensor([[0.8426]])\n",
      "for data: [0.10075485] we got output : tensor([[0.9873]])\n",
      "for data: [0.69960721] we got output : tensor([[0.1099]])\n",
      "for data: [0.84055811] we got output : tensor([[0.0363]])\n",
      "for data: [0.83455705] we got output : tensor([[0.0379]])\n"
     ]
    }
   ],
   "source": [
    "layer1_weight = sample_model.layer1.weight.T.data.tolist()\n",
    "layer1_bias = sample_model.layer1.bias.data.tolist()\n",
    "layer2_weight = sample_model.layer2.weight.T.data.tolist()\n",
    "layer2_bias = sample_model.layer2.bias.data.tolist()\n",
    "\n",
    "data = np.random.rand(1000).reshape((-1, 1))\n",
    "y = data < 0.5\n",
    "y = y.reshape((-1, 1))\n",
    "\n",
    "data_test = np.random.rand(10).reshape((-1, 1))\n",
    "y_test = data_test < 0.5\n",
    "y_test = y_test.reshape((-1, 1))\n",
    "\n",
    "# Decryption of result\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "loss = []\n",
    "for data, target in zip(data, y):\n",
    "    # Encoding and encryption\n",
    "    x_enc = prepare_input(context, data)\n",
    "    # Encrypted evaluation\n",
    "    # conv layer\n",
    "    # pack all channels into a single flattened vector\n",
    "    # fc1 layer\n",
    "    enc_x = x_enc.mm(layer1_weight) + layer1_bias\n",
    "    enc_x = nn.Sigmoid()(torch.tensor(enc_x.decrypt()))\n",
    "    enc_x = prepare_input(context, enc_x)\n",
    "    # enc_x = -0.004*enc_x*enc_x*enc_x+ 0.197*enc_x + 0.5\n",
    "    enc_output = enc_x.mm(layer2_weight) + layer2_bias\n",
    "    # enc_output = -0.004*enc_x*enc_x*enc_x+ 0.197*enc_x + 0.5\n",
    "\n",
    "    # Decryption of result\n",
    "    output = enc_output.decrypt()\n",
    "    output = nn.Sigmoid()(torch.tensor(output))\n",
    "\n",
    "    output = torch.tensor(output).view(1, -1)\n",
    "    print(f\"for data: {data} we got output : {output}\")\n",
    "    # compute loss\n",
    "    if (sample_model(torch.tensor(data).type(torch.float)) > 0.5) == (output > 0.5):\n",
    "        loss.append(1)\n",
    "    else:\n",
    "        loss.append(0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "1.0"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(loss) / len(loss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Trainable encrypted model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "layer1_weight = sample_model.layer1.weight.T.data.tolist()\n",
    "layer1_bias = sample_model.layer1.bias.data.tolist()\n",
    "layer2_weight = sample_model.layer2.weight.T.data.tolist()\n",
    "layer2_bias = sample_model.layer2.bias.data.tolist()\n",
    "\n",
    "\n",
    "class EncryptedLR:\n",
    "\n",
    "    def __init__(self, torch_lr):\n",
    "        self.layer1_weight = torch_lr.layer1.weight.T.data.tolist()[0]\n",
    "        self.layer1_bias = torch_lr.layer1.bias.data.tolist()\n",
    "        self.layer2_weight = torch_lr.layer2.weight.T.data.tolist()[0]\n",
    "        self.layer2_bias = torch_lr.layer2.bias.data.tolist()\n",
    "\n",
    "        # we accumulate gradients and counts the number of iterations\n",
    "\n",
    "        self._delta_w_1 = 0\n",
    "        self._delta_b_1 = 0\n",
    "        self._delta_w_2 = 0\n",
    "        self._delta_b_2 = 0\n",
    "\n",
    "        self._count_1 = 0\n",
    "        self._count_2 = 0\n",
    "\n",
    "    def forward(self, enc_x):\n",
    "        enc_out = enc_x.dot(self.layer1_weight) + self.layer1_bias\n",
    "        enc_out = EncryptedLR.sigmoid(enc_out)\n",
    "        enc_out = enc_out.dot(self.layer2_weight) + self.layer2_bias\n",
    "        enc_out = EncryptedLR.sigmoid(enc_out)\n",
    "        return enc_out\n",
    "\n",
    "    def backward(self, enc_x, enc_out, enc_y, layer_num):  # noqa\n",
    "        out_minus_y = (enc_out - enc_y)  # noqa\n",
    "        eval(f\"self._delta_w_{layer_num} += enc_x * out_minus_y\")\n",
    "        eval(f\"self._delta_b_{layer_num} += out_minus_y\")\n",
    "        eval(f\"self._count_{layer_num} += 1\")\n",
    "\n",
    "    def update_parameters(self):\n",
    "        if self._count == 0:\n",
    "            raise RuntimeError(\"You should at least run one forward iteration\")\n",
    "        # update weights\n",
    "        # We use a small regularization term to keep the output\n",
    "        # of the linear layer in the range of the sigmoid approximation\n",
    "        self.layer1_weight -= self._delta_w_1 * (1 / self._count_1) + self.weight * 0.05\n",
    "        self.layer1_bias -= self._delta_b_1 * (1 / self._count_1)\n",
    "\n",
    "        self.layer2_weight -= self._delta_w_2 * (1 / self._count_2) + self.weight * 0.05\n",
    "        self.layer2_bias -= self._delta_b_2 * (1 / self._count_2)\n",
    "\n",
    "        # reset gradient accumulators and iterations count\n",
    "\n",
    "        self._delta_w_1 = 0\n",
    "        self._delta_b_1 = 0\n",
    "        self._delta_w_2 = 0\n",
    "        self._delta_b_2 = 0\n",
    "\n",
    "        self._count_1 = 0\n",
    "        self._count_2 = 0\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(enc_x):\n",
    "        # We use the polynomial approximation of degree 3\n",
    "        # sigmoid(x) = 0.5 + 0.197 * x - 0.004 * x^3\n",
    "        # from https://eprint.iacr.org/2018/462.pdf\n",
    "        # which fits the function pretty well in the range [-5,5]\n",
    "        return enc_x.polyval([0.5, 0.197, 0, -0.004])\n",
    "\n",
    "    def plain_accuracy(self, x_test, y_test):\n",
    "        # evaluate accuracy of the model on\n",
    "        # the plain (x_test, y_test) dataset\n",
    "        w_1 = torch.tensor(self.layer1_weight)\n",
    "        b_1 = torch.tensor(self.layer1_bias)\n",
    "        w_2 = torch.tensor(self.layer2_weight)\n",
    "        b_2 = torch.tensor(self.layer2_bias)\n",
    "        out = torch.sigmoid(x_test.matmul(w_1) + b_1).reshape(-1, 1)\n",
    "        out = torch.sigmoid(out.matmul(w_2)+b_2).reshape(-1,1)\n",
    "        correct = torch.abs(y_test - out) < 0.5\n",
    "        return correct.float().mean()\n",
    "\n",
    "    def encrypt(self, context):\n",
    "        self.layer1_weight = ts.ckks_vector(context, self.layer1_weight)\n",
    "        self.layer1_bias = ts.ckks_vector(context, self.layer1_bias)\n",
    "        self.layer2_weight = ts.ckks_vector(context, self.layer2_weight)\n",
    "        self.layer2_bias = ts.ckks_vector(context, self.layer2_bias)\n",
    "\n",
    "\n",
    "    def decrypt(self):\n",
    "        self.layer1_weight = self.layer1_weight.decrypt()\n",
    "        self.layer1_bias = self.layer1_bias.decrypt()\n",
    "        self.layer2_weight = self.layer2_weight.decrypt()\n",
    "        self.layer2_bias = self.layer2_bias.decrypt()\n",
    "\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Data summary #############\n",
      "x_train has shape: torch.Size([1024, 1])\n",
      "y_train has shape: torch.Size([1024, 1])\n",
      "x_test has shape: torch.Size([512, 1])\n",
      "y_test has shape: torch.Size([512, 1])\n",
      "#######################################\n",
      "Encryption of the training_set took 41 seconds\n",
      "Distribution on plain data - Layer 1:\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution on encrypted data - Layer 1:\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "def random_data(m=1024, n=1):\n",
    "    # data separable by the line `y = x`\n",
    "    x_train = torch.randn(m, n)\n",
    "    x_test = torch.randn(m // 2, n)\n",
    "    y_train = (x_train[:, 0] >= x_train[:, 0]).float().unsqueeze(0).t()\n",
    "    y_test = (x_test[:, 0] >= x_test[:, 0]).float().unsqueeze(0).t()\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "# You can use whatever data you want without modification to the tutorial\n",
    "# x_train, y_train, x_test, y_test = random_data()\n",
    "x_train, y_train, x_test, y_test = random_data()\n",
    "\n",
    "print(\"############# Data summary #############\")\n",
    "print(f\"x_train has shape: {x_train.shape}\")\n",
    "print(f\"y_train has shape: {y_train.shape}\")\n",
    "print(f\"x_test has shape: {x_test.shape}\")\n",
    "print(f\"y_test has shape: {y_test.shape}\")\n",
    "print(\"#######################################\")\n",
    "# parameters\n",
    "poly_mod_degree = 8192\n",
    "coeff_mod_bit_sizes = [40, 21, 21, 21, 21, 21, 21, 40]\n",
    "# create TenSEALContext\n",
    "t_start = time()\n",
    "enc_x_train = [ts.ckks_vector(context, x.tolist()) for x in x_train]\n",
    "enc_y_train = [ts.ckks_vector(context, y.tolist()) for y in y_train]\n",
    "t_end = time()\n",
    "print(f\"Encryption of the training_set took {int(t_end - t_start)} seconds\")\n",
    "\n",
    "\n",
    "normal_dist = lambda x, mean, var: np.exp(- np.square(x - mean) / (2 * var)) / np.sqrt(2 * np.pi * var)\n",
    "\n",
    "def plot_normal_dist(mean, var, rmin=-10, rmax=10):\n",
    "    x = np.arange(rmin, rmax, 0.01)\n",
    "    y = normal_dist(x, mean, var)\n",
    "    fig = plt.plot(x, y)\n",
    "\n",
    "# plain distribution\n",
    "lr = SampleModel()\n",
    "data = lr.layer1(x_test)\n",
    "mean, var = map(float, [data.mean(), data.std() ** 2])\n",
    "plot_normal_dist(mean, var)\n",
    "print(\"Distribution on plain data - Layer 1:\")\n",
    "plt.show()\n",
    "# encrypted distribution\n",
    "# print(lr.layer2.weight.T.data.tolist()[0])\n",
    "# data = lr.layer2.weight.T.data.tolist()[0]*(x_test) + lr.layer2.bias.data.tolist()\n",
    "# mean, var = map(float, [data.mean(), data.std() ** 2])\n",
    "# plot_normal_dist(mean, var)\n",
    "# print(\"Distribution on plain data - Layer 2:\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "eelr = EncryptedLR(lr)\n",
    "eelr.encrypt(context)\n",
    "\n",
    "w_1 = eelr.layer1_weight\n",
    "b_1 = eelr.layer1_bias\n",
    "data = []\n",
    "for enc_x in enc_x_train:\n",
    "    enc_out = enc_x.dot(w_1) + b_1\n",
    "    data.append(enc_out.decrypt())\n",
    "data = torch.tensor(data)\n",
    "mean, var = map(float, [data.mean(), data.std() ** 2])\n",
    "plot_normal_dist(mean, var)\n",
    "print(\"Distribution on encrypted data - Layer 1:\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# w_2 = eelr.layer2_weight\n",
    "# b_2 = eelr.layer2_bias\n",
    "# data = []\n",
    "# for enc_x in enc_x_train:\n",
    "#     enc_out = enc_x.dot(w_2) + b_2\n",
    "#     data.append(enc_out.decrypt())\n",
    "# data = torch.tensor(data)\n",
    "# mean, var = map(float, [data.mean(), data.std() ** 2])\n",
    "# plot_normal_dist(mean, var)\n",
    "# print(\"Distribution on encrypted data - Layer 2:\")\n",
    "# plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# in-house developed remez for sigmoid"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from numpy.polynomial import Polynomial\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def approximate_sigmoid(x, precision):\n",
    "    a = -5\n",
    "    b = 5\n",
    "    while (b - a) > precision:\n",
    "        c = (a + b) / 2\n",
    "        d = (b - a) / 2\n",
    "        x_points = np.linspace(c - d, c + d, 10)\n",
    "        y_points = sigmoid(x_points)\n",
    "        p = Polynomial.fit(x_points, y_points, 6)\n",
    "        y_c_poly = p(c)\n",
    "        y_d_poly = p(c + d)\n",
    "        if y_c_poly < y_d_poly:\n",
    "            b = c + d\n",
    "        else:\n",
    "            a = c\n",
    "    return p\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize\n",
    "\n",
    "\n",
    "def remez(f, a, b, n):\n",
    "    # Initialize an arbitrary set of (n+2) points in the interval [a, b]\n",
    "    x = np.linspace(a, b, n + 2)\n",
    "\n",
    "    # Set the threshold for convergence\n",
    "    eps = 1e-6\n",
    "\n",
    "    while True:\n",
    "        # Compute the coefficients of the polynomial using the current set of points\n",
    "        A = np.vander(x - a, n + 1)\n",
    "        error_coefs = np.array([[(-1) ** i] for i in range(len(x))])\n",
    "        A = np.append(A, error_coefs, axis=1)\n",
    "\n",
    "        c = np.linalg.solve(A, f(x))\n",
    "\n",
    "        E = (error_coefs * c[-1]).flatten()\n",
    "        c = c[:-1]\n",
    "        # Compute the error function at the current set of points\n",
    "        print(\"Error:\")\n",
    "        print(E)\n",
    "\n",
    "        x_new = np.array([])\n",
    "        calculated_pol = np.polynomial.Polynomial(np.flip(c))\n",
    "        for i in range(n + 1):\n",
    "            print(f\" The x[{i}] is {x[i] - a} and ans for x[{i}]: {f(x[i]) - calculated_pol(x[i] - a)}\")\n",
    "\n",
    "            print(f\"The x[{i}+1] is {x[i + 1] - a} and ans for x[{i}+1]: {f(x[i + 1]) - calculated_pol(x[i + 1] - a)}\")\n",
    "\n",
    "            # Compute the point at which the error function attains its maximum or minimum value\n",
    "            x_star = optimize.bisect(lambda y: (f(y) - calculated_pol(y - a)), x[i], x[i + 1])\n",
    "            x_new = np.append(x_new, x_star)\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.allclose(x, x_new, atol=eps):\n",
    "            break\n",
    "        else:\n",
    "            x = x_new\n",
    "\n",
    "    # Return the final set of (n+2) points and the coefficients of the polynomial\n",
    "    return x, c\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:\n",
      "[ 4.83012068e-17 -4.83012068e-17  4.83012068e-17 -4.83012068e-17\n",
      "  4.83012068e-17]\n",
      " The x[0] is 0.0 and ans for x[0]: 4.85722573273506e-17\n",
      "The x[0+1] is 2.5 and ans for x[0+1]: -5.551115123125783e-17\n",
      " The x[1] is 2.5 and ans for x[1]: -5.551115123125783e-17\n",
      "The x[1+1] is 5.0 and ans for x[1+1]: -1.1102230246251565e-16\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "f(a) and f(b) must have different signs",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[719], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mremez\u001B[49m\u001B[43m(\u001B[49m\u001B[43msigmoid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[718], line 34\u001B[0m, in \u001B[0;36mremez\u001B[0;34m(f, a, b, n)\u001B[0m\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe x[\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m+1] is \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mx[i\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m-\u001B[39ma\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m and ans for x[\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m+1]: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mf(x[i\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m])\u001B[38;5;241m-\u001B[39mcalculated_pol(x[i\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m-\u001B[39ma)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     33\u001B[0m     \u001B[38;5;66;03m# Compute the point at which the error function attains its maximum or minimum value\u001B[39;00m\n\u001B[0;32m---> 34\u001B[0m     x_star \u001B[38;5;241m=\u001B[39m \u001B[43moptimize\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbisect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43mcalculated_pol\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     35\u001B[0m     x_new \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mappend(x_new, x_star)\n\u001B[1;32m     37\u001B[0m \u001B[38;5;66;03m# Check for convergence\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/capstone_project/lib/python3.10/site-packages/scipy/optimize/_zeros_py.py:557\u001B[0m, in \u001B[0;36mbisect\u001B[0;34m(f, a, b, args, xtol, rtol, maxiter, full_output, disp)\u001B[0m\n\u001B[1;32m    555\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m rtol \u001B[38;5;241m<\u001B[39m _rtol:\n\u001B[1;32m    556\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrtol too small (\u001B[39m\u001B[38;5;132;01m%g\u001B[39;00m\u001B[38;5;124m < \u001B[39m\u001B[38;5;132;01m%g\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (rtol, _rtol))\n\u001B[0;32m--> 557\u001B[0m r \u001B[38;5;241m=\u001B[39m \u001B[43m_zeros\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_bisect\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mxtol\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrtol\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmaxiter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfull_output\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdisp\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    558\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m results_c(full_output, r)\n",
      "\u001B[0;31mValueError\u001B[0m: f(a) and f(b) must have different signs"
     ]
    }
   ],
   "source": [
    "remez(sigmoid, -5, 5, 3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
