{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6ed45bf-37f1-4189-b696-5707062d2ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/john/.local/lib/python3.10/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 1.16.0-unknown is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/home/john/.local/lib/python3.10/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 0.1.43ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/home/john/.local/lib/python3.10/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 2.22.1ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.augmentations.dropout.grid_dropout import GridDropout\n",
    "import pickle\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b28b366-738f-41db-8047-3de5ca425f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c2db926-014f-4233-8ea8-18de6828d24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RetinaTransform(image):\n",
    "    # resizing image because original does not fit in memory.\n",
    "\n",
    "    image = resize(image, (64,64),anti_aliasing=False)\n",
    "    # Bringing image to 0-1 range, #ATTENTION: CHECK IF ALL IMAGES min max is 0 and 255\n",
    "    image = (image / 255)\n",
    "    \n",
    "    label = image.copy()\n",
    "    # grid_drop = GridDropout(ratio=0.5, unit_size_min=10, unit_size_max=50, fill_value=1, random_offset=True, always_apply=True)\n",
    "    # image = grid_drop(image=image)[\"image\"]\n",
    "    # Bringing data to CHW format\n",
    "    # N is a batch size, C denotes a number of channels, \n",
    "    # H is a height of input planes in pixels, and W is width in pixels.\n",
    "    image = image.transpose([2,0,1])\n",
    "    label = label.transpose([2,0,1]) #No need to transpose lables\n",
    "    \n",
    "    #Fixing dtype to avoid runtime error and save memory\n",
    "    image = torch.tensor(image ,dtype=torch.float32)\n",
    "    label = torch.tensor(label ,dtype=torch.float32)\n",
    "    \n",
    "    return image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a6109b-def2-433d-ba14-ac74117cdee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset into memory\n",
    "if 'image_array' not in globals():\n",
    "    image_array = []\n",
    "    data_paths= glob(\"../data/ODIR-5K/ODIR-5K/Testing Images/*\")+glob(\"../data/ODIR-5K/ODIR-5K/Training Images/*\")+glob(\"../data/REFUGE/Images_Square/*\")\n",
    "    data_paths = data_paths\n",
    "    local_transform = RetinaTransform\n",
    "\n",
    "\n",
    "    for image_name in tqdm(data_paths):\n",
    "        image = cv2.imread(image_name)\n",
    "        image,label = local_transform(image)\n",
    "        image_array.append((image,label))\n",
    "        \n",
    "    with open(\"image_array.pkl\", \"wb\") as f:\n",
    "        pickle.dump(image_array,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "386defcf-7fdf-4cba-9043-33375b017197",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"image_array.pkl\", \"rb\") as f:\n",
    "    image_array = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14c3d066-da1e-4bab-8a27-3ad1cceeae1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_array = [(i*2-1,j*2-1) for i,j in image_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6643d2af-0dff-4aea-ae01-c7b27427866d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "812f409f-4597-4766-93bc-e8dfd875f244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow((image_array[0][0].numpy().transpose(1,2,0)+1)*255/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "076ec0d4-f4be-4610-bc6c-e24ec30fc161",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# normalize the images between -1 and 1\n",
    "# Tanh as the last layer of the generator output\n",
    "\n",
    "# In GAN papers, the loss function to optimize G is min (log 1-D), but in practice folks practically use max log D\n",
    "\n",
    "#     because the first formulation has vanishing gradients early on\n",
    "#     Goodfellow et. al (2014)\n",
    "\n",
    "# In practice, works well:\n",
    "\n",
    "#     Flip labels when training generator: real = fake, fake = real\n",
    "\n",
    "# the stability of the GAN game suffers if you have sparse gradients\n",
    "# LeakyReLU = good (in both G and D)\n",
    "\n",
    "\n",
    "# optim.Adam rules!\n",
    "#     See Radford et. al. 2015\n",
    "# Use SGD for discriminator and ADAM for generator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9ae0636-04e7-4dfd-af3e-acc6a0e5b2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SuperConv2d(x, supr_conv_arch):\n",
    "    return torch.concat([nn.Conv2d(*conf).to(device)(x) for conf in supr_conv_arch], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3b5bba0-8987-4800-b363-42e48177a494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EMSNETGenerator(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         # self.super_conv = nn.Conv2d(in_channels=3, out_channels=10, kernel_size=3, stride=1,padding=1)\n",
    "#         self.linear_encoder = nn.Linear(30*64*64, 128)\n",
    "#         self.batch_norm2d = nn.BatchNorm2d(30)\n",
    "#         self.batch_norm1d = nn.BatchNorm1d(128)\n",
    "#         self.lrelu = nn.LeakyReLU(0.2)\n",
    "#         self.linear_decoder = nn.Sequential(\n",
    "#             nn.Linear(128,256),\n",
    "#             self.lrelu,\n",
    "#             nn.Linear(256,512),\n",
    "#             self.lrelu,\n",
    "#             nn.Linear(512,1024),\n",
    "#             self.lrelu,\n",
    "#             nn.Linear(1024,3*64*64),\n",
    "#         )\n",
    "#         self.tanh = nn.Tanh()\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "#         self.dropout= nn.Dropout(p=0.8)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = SuperConv2d(x,[[3,10,3,1,1],[3,10,5,1,2],[3,10,7,1,3]])\n",
    "#         x = self.lrelu(x)\n",
    "#         x = self.batch_norm2d(x)\n",
    "#         x = torch.flatten(x,start_dim=1)\n",
    "#         x = self.linear_encoder(x)\n",
    "#         x = self.batch_norm1d(x)\n",
    "#         x = self.lrelu(x)\n",
    "#         x = self.linear_decoder(x)\n",
    "#         # x = self.batch_norm1d(x)\n",
    "#         x = self.tanh(x)\n",
    "#         x = x.reshape(-1,3,64,64)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9f2d483-d12d-4798-924d-ddb909c52ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "resnet_model = resnet50(weights=\"IMAGENET1K_V1\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c05644b5-9dd8-4832-ad7d-fd48deda8741",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMSNETGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.resnet_model = resnet50(weights=\"IMAGENET1K_V1\")\n",
    "        self.batch_norm1d = nn.BatchNorm1d(1000)\n",
    "        self.batch_norm2d = nn.BatchNorm2d(3)\n",
    "        self.lrelu = nn.LeakyReLU(0.2)\n",
    "        self.linear_decoder = nn.Linear(1000,3*64*64)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.resnet_model(x)\n",
    "        x = self.batch_norm1d(x)\n",
    "        x = self.lrelu(x)\n",
    "        x = torch.flatten(x,start_dim=1)\n",
    "        x = self.linear_decoder(x)\n",
    "        x = x.reshape(-1,3,64,64)\n",
    "        x = self.batch_norm2d(x)   \n",
    "        x = self.tanh(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c3dbf0a-aab3-4d9f-9428-79525cd88933",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMSNETDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EMSNETDiscriminator, self).__init__()\n",
    "        self.ndf = 20\n",
    "        self.predict = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(3, self.ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(self.ndf, self.ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(self.ndf * 2, self.ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(self.ndf * 4, self.ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(self.ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.predict(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "450f4dcb-767a-4a02-83ad-201c1b035e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMSNETGAN(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generator = EMSNETGenerator()\n",
    "        self.discriminator = EMSNETDiscriminator()\n",
    "        self.automatic_optimization = False\n",
    "        self.cntr = 0 \n",
    "    def forward(self, img):\n",
    "        return self.generator(img)\n",
    "\n",
    "    def adversarial_loss(self, y_hat, y):\n",
    "        return F.binary_cross_entropy(y_hat, y)\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        imgs, labels = batch\n",
    "        \n",
    "        optimizer_g, optimizer_d = self.optimizers()\n",
    "        \n",
    "        self.toggle_optimizer(optimizer_d)\n",
    "        \n",
    "        valid = torch.ones(imgs.size(0))\n",
    "        valid = valid.type_as(imgs)\n",
    "\n",
    "        real_loss = self.adversarial_loss(self.discriminator(labels).view(-1), valid)\n",
    "\n",
    "        fake = torch.zeros(imgs.size(0))\n",
    "        fake = fake.type_as(imgs)\n",
    "\n",
    "        fake_loss = self.adversarial_loss(self.discriminator(self.generator(imgs)).view(-1), fake)\n",
    "\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "        self.log(\"d_loss\", d_loss, prog_bar=True)\n",
    "        self.manual_backward(d_loss)\n",
    "        optimizer_d.step()\n",
    "        optimizer_d.zero_grad()\n",
    "\n",
    "        self.untoggle_optimizer(optimizer_d)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.toggle_optimizer(optimizer_g)\n",
    "        \n",
    "        self.generated_imgs = self.generator(imgs)\n",
    "        sample_imgs = self.generated_imgs[:6]\n",
    "        grid = torchvision.utils.make_grid((sample_imgs+1)*255/2)\n",
    "        self.logger.experiment.add_image(\"generated_images\", grid, self.cntr)\n",
    "        self.cntr+=1\n",
    "        \n",
    "        \n",
    "        valid = torch.ones(imgs.size(0))\n",
    "        valid = valid.type_as(imgs)\n",
    "        \n",
    "        #pixel-wise loss\n",
    "        pixel_loss = self.adversarial_loss(torch.flatten((self.generator(imgs)+1)/2,start_dim=1),torch.flatten((imgs+1)/2,start_dim=1))\n",
    "        # gd_loss = self.adversarial_loss(self.discriminator(self.generator(imgs)).view(-1), valid)\n",
    "        gd_loss=0\n",
    "        g_loss = (pixel_loss+gd_loss)/2\n",
    "        self.log(\"g_loss\", g_loss, prog_bar=True)\n",
    "        self.manual_backward(g_loss)\n",
    "        optimizer_g.step()\n",
    "        optimizer_g.zero_grad()\n",
    "        self.untoggle_optimizer(optimizer_g)\n",
    "        \n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        opt_g = torch.optim.Adam(self.generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "        return [opt_g, opt_d], []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3d4b4fe-28cc-4d06-a94a-b7093510b642",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# discriminant = EMSNETDiscriminator()\n",
    "# generant = EMSNETGenerator()\n",
    "# loss_ = nn.BCELoss()\n",
    "# with torch.no_grad():\n",
    "#     for idx,i in enumerate(val_dataloader):\n",
    "#         # print(i[0].shape, generant(i[0]).shape)\n",
    "#         print(i[0].shape, discriminant(i[0]))\n",
    "#         real_label = torch.ones(i[0].size(0))\n",
    "#         output = generant(i[1])\n",
    "#         errD_real = loss_(output, real_label)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c33fea2d-5297-43bd-956d-7d31e9485df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetinaDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data_indices, transform=None):\n",
    "        \n",
    "        self.data = [image_array[idx] for idx in data_indices]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "296d4f22-d256-46ad-9b39-20b6b8cfd302",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RetinaDataset(data_indices=np.arange(0,len(image_array)-50))\n",
    "# train_dataset = RetinaDataset(data_indices=np.arange(0,1200))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=False, num_workers=8)\n",
    "\n",
    "val_dataset = RetinaDataset(data_indices=np.arange(len(image_array)-50,len(image_array)))\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05d528c1-74e4-401d-8592-ca927cf17dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(train_dataset[0][0].numpy().transpose(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b3fc827-4e05-43e8-9ab8-da9050366e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "emsnet = EMSNETGAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f64a5a55-1635-4f9f-8291-b57b1908f74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "# class LitProgressBar(TQDMProgressBar):\n",
    "#     def init_validation_tqdm(self):\n",
    "#         bar = super().init_validation_tqdm()\n",
    "#         bar.set_description(\"running validation...\")\n",
    "#         return bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9647aea-b48d-4bfc-a3b2-42818204483d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/john/.local/lib/python3.10/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 1.16.0-unknown is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/home/john/.local/lib/python3.10/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 0.1.43ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/home/john/.local/lib/python3.10/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 2.22.1ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=10, accelerator='gpu')\n",
    "# tuner = Tuner(trainer)\n",
    "# tuner.scale_batch_size(emsnet, mode=\"binsearch\")\n",
    "# trainer.fit(model=emsnet, train_dataloaders=train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f019005-cf3a-4806-8b3b-0f11ff4c8e19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type                | Params\n",
      "------------------------------------------------------\n",
      "0 | generator     | EMSNETGenerator     | 37.9 M\n",
      "1 | discriminator | EMSNETDiscriminator | 272 K \n",
      "------------------------------------------------------\n",
      "38.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "38.1 M    Total params\n",
      "152.529   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ed553244c94f2390779ba56c0bb444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/john/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model=emsnet, train_dataloaders=train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "35ec7fc4-f9e5-48a7-afd6-d92e21835937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# emsnet.generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a206c511-e0e9-455f-b0e3-ff1bbb268f09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "# Using pretrained weights:\n",
    "# resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "resnet_model = resnet50(weights=\"IMAGENET1K_V1\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "25f368d0-3ec4-44f9-bb77-942d3a8c7073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1000])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for idx,i in enumerate(train_dataloader):\n",
    "        print(resnet_model(i[0].to(device)).shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e647d748-c541-474c-8236-0687be1a2b5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 64, 64]) torch.Size([128, 3, 64, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument weight in method wrapper__cudnn_batch_norm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [63]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx,i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape,i[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 5\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43memsnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# print(emsnet.discriminator(output))\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# print(emsnet.discriminator(i[1]))\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     plt\u001b[38;5;241m.\u001b[39mimshow(output[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m255\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m255\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [51]\u001b[0m, in \u001b[0;36mEMSNETGenerator.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m x \u001b[38;5;241m=\u001b[39m SuperConv2d(x,[[\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m],[\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m],[\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m7\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m]])\n\u001b[1;32m     24\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlrelu(x)\n\u001b[0;32m---> 25\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x,start_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     27\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_encoder(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2448\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2451\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2452\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument weight in method wrapper__cudnn_batch_norm)"
     ]
    }
   ],
   "source": [
    "emsnet.eval()\n",
    "with torch.no_grad():\n",
    "    for idx,i in enumerate(train_dataloader):\n",
    "        print(i[0].shape,i[1].shape)\n",
    "        output = emsnet.generator(i[0].to(device))\n",
    "        # print(emsnet.discriminator(output))\n",
    "        # print(emsnet.discriminator(i[1]))\n",
    "        plt.imshow(output[0].cpu().detach().numpy().transpose(2,1,0)*255/2+255/2)\n",
    "        plt.show()\n",
    "        plt.imshow(i[1][0].permute(1,2,0)*255/2+255/2)\n",
    "        plt.show()\n",
    "        plt.imshow(i[0][0].permute(1,2,0)*255/2+255/2)\n",
    "        plt.show()\n",
    "        if idx == 4: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "efcfbb38-67a7-4cd3-bc5d-46ea9ac4b1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308341ab-51e0-45e3-9f0b-f61c6d00ac87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
